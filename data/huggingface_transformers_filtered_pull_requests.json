[
    {
        "merged": false,
        "additions": 43,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-10-18T06:26:47Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Hi @gante \r\n\r\nDo you think that we should also add `assistant_attention_mask` and `assistant_position_ids` in `assisted_decoding`? I see that the original model has `attention_mask` and `position_ids`(in most models) in the model inputs but the assistant model has no these kinds of input.\r\n\r\nIf you think it is okay to align the original model and the assistant model, maybe we can find a more elegant way to integrate it. Thx!",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-18T01:17:33Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.17 to 1.26.18.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/releases\">urllib3's releases</a>.</em></p>\n<blockquote>\n<h2>1.26.18</h2>\n<ul>\n<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses. (GHSA-g4mx-q9vg-27p4)</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/blob/main/CHANGES.rst\">urllib3's changelog</a>.</em></p>\n<blockquote>\n<h1>1.26.18 (2023-10-17)</h1>\n<ul>\n<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses.</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/9c2c2307dd1d6af504e09aac0326d86ee3597a0b\"><code>9c2c230</code></a> Release 1.26.18 (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3159\">#3159</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/b594c5ceaca38e1ac215f916538fb128e3526a36\"><code>b594c5c</code></a> Merge pull request from GHSA-g4mx-q9vg-27p4</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/944f0eb134485f41bc531be52de12ba5a37bca73\"><code>944f0eb</code></a> [1.26] Use vendored six in urllib3.contrib.securetransport</li>\n<li>See full diff in <a href=\"https://github.com/urllib3/urllib3/compare/1.26.17...1.26.18\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.17&new-version=1.26.18)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).\n\n</details>",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-18T01:17:13Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.17 to 1.26.18.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/releases\">urllib3's releases</a>.</em></p>\n<blockquote>\n<h2>1.26.18</h2>\n<ul>\n<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses. (GHSA-g4mx-q9vg-27p4)</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/blob/main/CHANGES.rst\">urllib3's changelog</a>.</em></p>\n<blockquote>\n<h1>1.26.18 (2023-10-17)</h1>\n<ul>\n<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses.</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/9c2c2307dd1d6af504e09aac0326d86ee3597a0b\"><code>9c2c230</code></a> Release 1.26.18 (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3159\">#3159</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/b594c5ceaca38e1ac215f916538fb128e3526a36\"><code>b594c5c</code></a> Merge pull request from GHSA-g4mx-q9vg-27p4</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/944f0eb134485f41bc531be52de12ba5a37bca73\"><code>944f0eb</code></a> [1.26] Use vendored six in urllib3.contrib.securetransport</li>\n<li>See full diff in <a href=\"https://github.com/urllib3/urllib3/compare/1.26.17...1.26.18\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.17&new-version=1.26.18)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).\n\n</details>",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-18T01:16:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.17 to 1.26.18.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/releases\">urllib3's releases</a>.</em></p>\n<blockquote>\n<h2>1.26.18</h2>\n<ul>\n<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses. (GHSA-g4mx-q9vg-27p4)</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/blob/main/CHANGES.rst\">urllib3's changelog</a>.</em></p>\n<blockquote>\n<h1>1.26.18 (2023-10-17)</h1>\n<ul>\n<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses.</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/9c2c2307dd1d6af504e09aac0326d86ee3597a0b\"><code>9c2c230</code></a> Release 1.26.18 (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3159\">#3159</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/b594c5ceaca38e1ac215f916538fb128e3526a36\"><code>b594c5c</code></a> Merge pull request from GHSA-g4mx-q9vg-27p4</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/944f0eb134485f41bc531be52de12ba5a37bca73\"><code>944f0eb</code></a> [1.26] Use vendored six in urllib3.contrib.securetransport</li>\n<li>See full diff in <a href=\"https://github.com/urllib3/urllib3/compare/1.26.17...1.26.18\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.17&new-version=1.26.18)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).\n\n</details>",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-10-17T22:38:41Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR attempts to support InstructBlip as an acceptable model in the VQA pipeline. Currently, only Blip2 is supported.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review?\r\n\r\n@NielsRogge @amyeroberts @Narsil \r\n\r\n# Description\r\n\r\nThis PR is not complete, but I'd like some guidance so that I can finalize the contribution. One issue is that InstructBlip handles the tokenization a bit. I have a bit of an ugly check; maybe this can be improved.\r\n\r\nThe bigger issue is that the pipeline loads a `BlipImageProcessor` instead of `InstructBlipProcessor`. I think this comes from this [config](https://huggingface.co/Salesforce/instructblip-flan-t5-xl/blob/main/preprocessor_config.json). The wrinkle here is that the Q-former (the `InstructBlipProcessor` class) is a layer above the image processor (the `BlipImageProcessor` class). We need a handle to an instantiated `InstructBlipProcessor`, but the `pipeline` function doesn't seem to load it. I don't see a hook for subclasses to load additional processors either. Maybe add the `InstructBlipProcessor` as a kwarg to `__init__` in the `VisualQuestionAnsweringPipeline` class? Would love feedback from the core devs.\r\n\r\n# Testing\r\n\r\nThe following code works with my changes:\r\n\r\n```python\r\nfrom transformers import pipeline\r\n\r\ncheckpoint = \"Salesforce/instructblip-flan-t5-xl\"\r\npipe = transformers.pipeline(\"vqa\", model=checkpoint)\r\n\r\n# This shouldn't be necessary, but I'm not sure why AutoImageProcessor loads the wrong processor:\r\nimage_processor = InstructBlipProcessor.from_pretrained(checkpoint)\r\npipe.image_processor = image_processor\r\n\r\nimage_url = \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/lena.png\"\r\npipe(question=\"What is she wearing ?\", image=image_url)\r\n# [{'answer': 'hat'}]\r\n```\r\n\r\n\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2023-10-17T21:25:54Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26638 \r\n\r\nFix doc string for speech-to-text configuration.\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 151,
        "deletions": 396,
        "changed_files": 7,
        "created_at": "2023-10-17T20:19:34Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Implements the proposal suggested in #26723, namely:\r\n\r\n- consolidating the inference on one/many GPUs docs into one page\r\n- removing the inference on specialized hardware page\r\n- cleanups and updates to the inference docs to provide more context about the how/what of the optimization techniques, code examples so users don't have to skip around to other pages, etc.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2023-10-17T19:30:19Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes https://github.com/huggingface/transformers/issues/26638\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-17T18:59:10Z",
        "closed_at": "2023-10-17T21:06:37Z",
        "merged_at": "2023-10-17T21:06:37Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 402,
        "deletions": 166,
        "changed_files": 35,
        "created_at": "2023-10-17T18:08:47Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This is a draft and it is NOT ready for a review. ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 171,
        "deletions": 39,
        "changed_files": 4,
        "created_at": "2023-10-17T16:41:42Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR enables speculative decoding when batch size is > 1. The models works for Whisper.\r\n\r\n# TODO:\r\n\r\n- [ ] Clean code\r\n- [ ] Test on Llama\r\n- [ ] Tests\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-17T14:38:42Z",
        "closed_at": "2023-10-17T16:15:51Z",
        "merged_at": null,
        "body": "As per title, please refer to https://github.com/huggingface/transformers/blob/db611aabee863cc5b1fdc22dcec5ce8e6c3e3b36/src/transformers/models/falcon/modeling_falcon.py#L1110",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 15,
        "changed_files": 1,
        "created_at": "2023-10-17T14:29:38Z",
        "closed_at": null,
        "merged_at": null,
        "body": "-  Fixed broken links.\r\n\r\nI hope this small contribution adds value to this project.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-17T13:17:42Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nThis PR ports IDEFICS to tensorflow \r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-17T11:16:25Z",
        "closed_at": null,
        "merged_at": null,
        "body": "* Fix `resize_token_embeddings` about `requires_grad`\r\n\r\nThe method `resize_token_embeddings` should keep `requires_grad` unchanged for all parameters in embeddings.\r\n\r\nPreviously, `resize_token_embeddings` always set `requires_grad` to `True`. After fixed, `resize_token_embeddings` copy the `requires_grad` attribute in the old embeddings.\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26861\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. \r\n  - issue: #26861 \r\n  - forum: [forum discussion](https://discuss.huggingface.co/t/how-to-freez-a-model/12819/2)\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-17T10:12:11Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAddresses https://github.com/huggingface/transformers/issues/18068, and a few other GH issues with the same problem\r\n\r\nWhen a user defines a custom stopping criteria that makes use of the `scores` input, if `return_dict_in_generate=True, output_scores=True` are not passed, an exception will be thrown (`scores` is `None`). This is because we only keep track of `scores` when those two flags are passed.\r\n\r\nI've added this requirement in several places in the docstrings, to reduce the odds of being missed.\r\n\r\nFor completeness: I've also considered always keeping track of scores. However, this adds a non-negligible overhead -- in BF16 for the 7B Mistral model, we would need an extra ~50MB per 1000 generated tokens (and batching would act as a multiplier here). Ditto for 4-bit BNB. Since custom stopping criteria are seldom used, this didn't look like a good tradeoff :)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 113,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-17T06:31:08Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes https://github.com/huggingface/transformers/issues/26817\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@Arthur\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-17T03:54:07Z",
        "closed_at": "2023-10-17T17:30:47Z",
        "merged_at": "2023-10-17T17:30:46Z",
        "body": "Part of Docstring Sprint #26638 \r\n\r\nOne small bug I noticed, which can be seen in my first commit, was that check_docstrings.py added a duplicate default specification for use_entity_aware_attention, which I deleted in my second commit.\r\n\r\n## Before submitting\r\n- [Y] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [Y] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [Y] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [N] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [N] Did you write any new necessary tests?",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-17T02:59:05Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR simply fix a link to set_transform documentation. \r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\nBroken link to set_transform docs\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@stevhliu  @MKhalusova \r\n\r\n\r\n\r\n\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-17T01:33:43Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nAdded hindi language code in language list in .github/workflows/build_documentation.yml and .github/workflows/build_pr_documentation.yml.\r\nFixes # 26837\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n\r\n\r\n## Who can review?\r\n\r\n@stevhliu and @MKhalusova please review this PR.\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 216,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-16T23:52:27Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR introduces a new script named `convert_hf_to_openai.py` that allows for the conversion of Hugging Face Whisper models back to the original OpenAI Whisper format. This just does the opposite of the [`convert_openai_to_hf.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/convert_openai_to_hf.py) script.\r\n\r\nWhile Hugging Face is easier to use, for example, [for fine-tuning](https://huggingface.co/blog/fine-tune-whisper) and has many integrations, the original OpenAI Whisper library provides more fine-grained control over this specific model, facilitating the testing of new approaches and certain algorithms (at least in our case).\r\n\r\n## Doctests\r\n\r\nI added a doctest at the beginning that passes, but it requires the `openai-whisper` package to be installed, so I left it disabled with the double `>>`. Not sure how do you prefer to handle this case: leave it like that, adding the Whisper package somewhere in the CI (like [`.github/workflows/doctests.yml`](https://github.com/huggingface/transformers/blob/main/.github/workflows/doctests.yml)), or in any other way.\r\n\r\nBesides, even though the original `convert_openai_to_hf.py` script did not have them, let me know if you want me to add some tests to this. I have tested it myself to work with all the Whisper model sizes, even the Large V2.\r\n\r\n## Before submitting\r\n\r\n- [ ] This PR fixes a typo or improves the docs.\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review?\r\n\r\nPossible candidates:\r\n- `convert_openai_to_hf.py` script creator: @ArthurZucker\r\n- Speech models: @sanchit-gandhi\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-16T21:23:19Z",
        "closed_at": "2023-10-17T13:38:03Z",
        "merged_at": "2023-10-17T13:38:03Z",
        "body": "# What does this PR do?\r\n\r\nFixes https://github.com/huggingface/trl/issues/832 \r\nFixes https://github.com/huggingface/trl/issues/875\r\nand all issues related with FA-2 + Falcon fine-tuning\r\n\r\nBefore this PR we were passing a `nn.Dropout` to the flash attention forward method, leading to an error since the dropout argument is expected to be a float. \r\n\r\nAlso modified the test a bit to cover that usecase for future models\r\n\r\ncc @ArthurZucker @lewtun \r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-16T20:00:29Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nFix docstring for bit image processor.\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ydshieh",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-16T19:40:13Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR brings back the `set_epoch` logic and solves the last remaining regression from the accelerate integration. Resuming training should now be exactly identical. (Introduced due to an incomplete implementation in https://github.com/huggingface/transformers/pull/24028)\r\n\r\nLinked with https://github.com/huggingface/accelerate/pull/2057\r\n\r\nFixes # (issue)\r\n\r\nfixes https://github.com/huggingface/transformers/issues/26541\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@LysandreJik @ArthurZucker ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 56,
        "deletions": 24,
        "changed_files": 8,
        "created_at": "2023-10-16T19:33:51Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFixes #26700 #23145\r\n\r\n\r\n## Who can review?\r\n\r\n@LysandreJik @SunMarc ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-16T19:25:16Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFix docstring for bert japanese tokenizer.\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ydshieh",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-16T18:59:59Z",
        "closed_at": "2023-10-16T20:47:20Z",
        "merged_at": "2023-10-16T20:47:20Z",
        "body": "# What does this PR do?\r\n\r\nmissing `torch.no_grad` \ud83d\ude2d ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 78,
        "deletions": 20,
        "changed_files": 4,
        "created_at": "2023-10-16T18:48:14Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nReplaces https://github.com/huggingface/transformers/pull/26560 \r\nFixes https://github.com/huggingface/transformers/issues/26451\r\n\r\nProposes a simpler fix for dealing with FA-2 + PEFT + quantization fine-tuning where users usually cast all other modules (e.g. LayerNorms) in fp32 for training stability. \r\n\r\nWith https://github.com/huggingface/transformers/pull/26761 being introduced, it is now much simpler to retrieve model's original dtype, note also that `self.config._pre_quantization_dtype` remains the single source of truth as `to` is not supported for quantized models\r\n\r\ncc @ArthurZucker @pacman100 \r\n\r\nAdded also a nice test",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 26,
        "deletions": 15,
        "changed_files": 2,
        "created_at": "2023-10-16T17:34:42Z",
        "closed_at": "2023-10-17T09:32:50Z",
        "merged_at": "2023-10-17T09:32:50Z",
        "body": "# What does this PR do?\r\n\r\nFixes https://github.com/huggingface/transformers/issues/26332 -- see [this](https://github.com/huggingface/transformers/issues/26332#issuecomment-1764736970) comment for a full explanation.\r\n\r\nTL;DR: in `beam_sample`, `logits_warper` are now applied BEFORE adding the beam scores. We have been postponing this change to avoid introducing output differences, but the truth is that the order of operations causes issues (e.g. [1](https://github.com/huggingface/transformers/issues/26332) [2](https://github.com/huggingface/transformers/issues/22914))\r\n\r\nThis is technically a bug fix (`beam_sample` is unusable for long generations with `temperature < 1.0` before this change), hence the lack of a deprecation cycle. However, it may alter some generated outputs (short generations with low temperature), hence the \ud83d\udea8\ud83d\udea8.\r\n\r\nPlease note that other common operators, like `top_p` and `top_k`, are unaltered by this change.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-16T17:02:38Z",
        "closed_at": "2023-10-17T22:15:51Z",
        "merged_at": "2023-10-17T22:15:50Z",
        "body": "# What does this PR do?\r\n\r\nThis fixes the `_is_package_available` check for tensorflow variants (like tensorflow-rocm) where their meta-data differs from the package name. There is an list of available candidate names there, but they are never used since `_is_package_available` will return false preventing the candidate list from being checked.\r\nThis also adds `tf-nightly-rocm` to the list of candidates.\r\n\r\nFixes # (issue)\r\nN/A\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\nThis fixes a small functional error, no documentation updates are required.\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n- tensorflow review: @gante and @Rocketknight1\r\n",
        "comments": 10
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-16T16:36:07Z",
        "closed_at": null,
        "merged_at": null,
        "body": "The Seq2SeqTrainer drops `decoder_input_ids` during the generation step for metrics that expect text generation (like `rouge`) when `labels` is present. However, it doesn't drop `decoder_attention_mask` when it does this, which means that in some cases, we pass `decoder_attention_mask` with no `decoder_input_ids`, resulting in the model getting very confused and throwing a shape error.\r\n\r\nThis PR fixes the issue.\r\n\r\nFixes #24567",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2023-10-16T14:26:33Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n\r\nFixes #26428 reported by @momergul\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 320,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-16T12:16:12Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nThis PR translates the pipeline_tutorial.md in Hindi language and mentions it in _toctree.yml in doc/source/hi/ folder. Thus  Fixes # 26787\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n\r\n\r\n## Who can review?\r\n\r\n@stevhliu and @MKhalusova please review this PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2023-10-16T12:08:34Z",
        "closed_at": null,
        "merged_at": null,
        "body": "As per title.\r\n\r\nThe issue is that \r\n\r\n```python\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nimport torch\r\n\r\nmodel_id = \"hf-internal-testing/tiny-random-GPTJForCausalLM\"\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\n\r\ninp = tokenizer(\"this is me\", return_tensors=\"pt\").to(\"cuda\")\r\nmodel = model.to(\"cuda\")\r\n\r\nmodel = model.eval()\r\nwith torch.no_grad():\r\n    res = model(**inp, use_cache=True)\r\n\r\nprint(\"res.past_key_values[0][0].dtype\", res.past_key_values[0][0].dtype)\r\nprint(\"res.past_key_values[0][1].dtype\", res.past_key_values[0][1].dtype)\r\n```\r\ngives\r\n```\r\nres.past_key_values[0][0].dtype torch.float32\r\nres.past_key_values[0][1].dtype torch.float16\r\n```\r\n\r\nthe reason being that `sin` and `cos` are not properly casted to the correct dtype contrarily to the implementation in e.g. llama.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-16T11:53:52Z",
        "closed_at": "2023-10-16T20:47:24Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/427\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 63,
        "deletions": 15,
        "changed_files": 2,
        "created_at": "2023-10-16T11:42:08Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR addresses two issues in the `convert_openai_whisper_to_hf.py` script for it to work correctly.\r\n\r\n1. It corrects the `decoder_attention_heads` value. This did not produce any error, but the models converted did not transcribe correctly.\r\n\r\n2. It also fixes the `_download()` function:\r\n\r\n* Adds the `root` parameter, previously gave the following error:\r\n```\r\n$ python convert_openai_to_hf.py \\\r\n  --checkpoint_path tiny \\\r\n  --pytorch_dump_folder_path pytorch_model_hf.bin\r\n\r\nTraceback (most recent call last):\r\n  File \"convert_openai_to_hf.py\", line 184, in <module>\r\n    convert_openai_whisper_to_tfms(args.checkpoint_path, args.pytorch_dump_folder_path)\r\n  File \"convert_openai_to_hf.py\", line 133, in convert_openai_whisper_to_tfms\r\n    original_checkpoint = _download(_MODELS[checkpoint_path])\r\nTypeError: _download() missing 1 required positional argument: 'root'\r\n```\r\n*  Returns the download path instead of the model bytes, it produced the following error before:\r\n```\r\n$ python convert_openai_to_hf.py \\\r\n  --checkpoint_path tiny \\\r\n  --pytorch_dump_folder_path pytorch_model_hf.bin\r\n\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 72.1M/72.1M [00:01<00:00, 41.8MiB/s]\r\nTraceback (most recent call last):\r\n  File \"convert_openai_to_hf.py\", line 185, in <module>\r\n    convert_openai_whisper_to_tfms(args.checkpoint_path, args.pytorch_dump_folder_path)\r\n  File \"convert_openai_to_hf.py\", line 137, in convert_openai_whisper_to_tfms\r\n    dimensions = original_checkpoint[\"dims\"]\r\nTypeError: byte indices must be integers or slices, not str\r\n```\r\n\r\n## Before submitting\r\n- [x] I've read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests).\r\n- [ ] This was discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/). Please add a link to it if that's the case.\r\n- [x] I have updated the documentation with my changes where necessary. \r\n- [x] I have written any new necessary tests.\r\n\r\n## Who can review?\r\n\r\n- Library:\r\n  - tokenizers: @ArthurZucker (based on issue #20600).",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 6,
        "created_at": "2023-10-16T11:04:14Z",
        "closed_at": "2023-10-17T05:10:08Z",
        "merged_at": "2023-10-17T05:10:08Z",
        "body": "# What does this PR do?\r\n\r\nThis PR fix the typo present in the files inside `docs/source/en/model_doc` folder\r\n\r\n```pseudo\r\noptimizaton > optimization\r\nSimiliar > Similar\r\nexaclty > exactly\r\nconditionning > conditioning\r\noutupt > output\r\navalable > avaliable\r\n```\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n\r\n\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-10-16T10:12:06Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nPR https://github.com/huggingface/transformers/pull/25863 adds support for using HF built-in scheduler with deepspeed optimizer. This PR goes a step further by enabling users to use custom schedulers with deepspeed optimizer. It achieves this by utilizing the `trainer.create_scheduler` function in the deepspeed setup, allowing users to override it and create their own custom scheduler\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@muellerzr and @pacman100\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 864,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-10-16T06:50:03Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-15T19:15:34Z",
        "closed_at": null,
        "merged_at": null,
        "body": "according to description : \"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` has to be\"\r\n                f\" divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\r\n\r\n\r\n\r\n\r\n\r\n\r\n## Before submitting\r\n- [ X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ -] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ X] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [- ] Did you write any new necessary tests?\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-15T17:06:07Z",
        "closed_at": "2023-10-16T13:47:45Z",
        "merged_at": "2023-10-16T13:47:45Z",
        "body": "# What does this PR do?\r\n\r\nThis PR improves the docs of OWL-ViT and OWLv2 by including a figure as well as a link to demo notebooks.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 21,
        "changed_files": 4,
        "created_at": "2023-10-15T15:28:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFixes the docstrings for CodeGen following #26638 \r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-15T14:44:00Z",
        "closed_at": "2023-10-16T16:26:56Z",
        "merged_at": "2023-10-16T16:26:56Z",
        "body": "# What does this PR do?\r\n\r\nFix docstring for bert generation tokenizer.\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ydshieh",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-10-15T14:05:40Z",
        "closed_at": "2023-10-15T14:36:23Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nIt fixes docstring for bert generation tokenizer.\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ydshieh\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2245,
        "deletions": 2,
        "changed_files": 25,
        "created_at": "2023-10-15T01:33:51Z",
        "closed_at": null,
        "merged_at": null,
        "body": "## Description\r\n- **Motivation** - The PvT is a useful backbone for computer vision tasks, but only the outdated v1 is available in Hugging Face.\r\n- **What this PR Does** - Full integration of PvT-v2 model (works with AutoModel).\r\n- **Notes** - Like the original implementation, the config allows for using either Spatial Reduction \"SR\" or average pooling \"AP\" to reduce complexity in the attention layer, default is using SRA (as in the original code).\r\n\r\n@amyeroberts \r\n\r\n## Resources\r\n**Model paper**\r\n- [PVT v2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/abs/2106.13797)\r\n\r\n**Open Source Implementations**\r\n- [Original](https://github.com/whai362/PVT/blob/v2/classification/pvt_v2.py)\r\n- [Panoptic Segformer](https://github.com/zhiqi-li/Panoptic-SegFormer/blob/master/easymd/models/backbones/pvt_v2.py)\r\n\r\n## Checks\r\n- Add PvT-v2 to model docs \u2714\r\n- Build pytests and have them pass \u2714\r\n- Formatting (make fix-copies, make fixup) \u2714\r\n- Convert open-source weights and test expected logits, uploaded to hub \u2714\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-10-14T20:35:12Z",
        "closed_at": null,
        "merged_at": null,
        "body": "A useful feature has been added:\r\nA function can retrieve models with a matching prefix in a particular module\r\n\r\nReduced cyclomatic complexity of the existing function: get_all_auto_configured_models",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-10-14T18:37:39Z",
        "closed_at": "2023-10-16T07:52:30Z",
        "merged_at": "2023-10-16T07:52:30Z",
        "body": "e.g. > e.g.,\r\nimage image > image\r\nNumpy > NumPy\r\ncropping image image files > cropping image files\r\nLog-Mel Spectrogram features, feature extraction from > generate Log-Mel Spectrogram features, feature extraction from",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 164,
        "changed_files": 1,
        "created_at": "2023-10-14T16:52:52Z",
        "closed_at": "2023-10-16T12:27:11Z",
        "merged_at": null,
        "body": "\r\n\r\n# What does this PR do?\r\nCode Simplification by removing redundant checks and using more concise syntax.\r\n\r\nAvoiding Redundant Instantiations  by directly using the classes rather than instantiating them in separate variables.\r\n\r\nConsistent Variable Naming\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 489,
        "deletions": 454,
        "changed_files": 90,
        "created_at": "2023-10-14T12:04:28Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n- Added Typing\r\n- Fixes #26745 \r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@LysandreJik \r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T22:37:03Z",
        "closed_at": "2023-10-17T07:26:03Z",
        "merged_at": "2023-10-17T07:26:03Z",
        "body": "# What does this PR do?\r\n\r\nThe window_size should have been implemented to allow separate values for height and width, but it was not working. This fixes that problem.\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@amyeroberts @nandwalritik @SatyaJandhyalaAtMS \r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T21:29:54Z",
        "closed_at": "2023-10-16T08:10:24Z",
        "merged_at": "2023-10-16T08:10:24Z",
        "body": "# What does this PR do?\r\nHello!\r\n\r\nI found a description in `README_ja.md` that is not translated into Japanese.\r\nTherefore, I am only translated the relevant sections.\r\n\r\nThank you!\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-10-13T21:00:08Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR passes `split_batches` in the `TrainingArguments` to the `Accelerator()`, the same way that `dispatch_batches` is currently done.\r\n\r\nIn the future we should consider whether we want to make a dataclass with the args for `Accelerator` that a user can pass instead. (Or a raw Accelerator potentially with guards)\r\n\r\nFixes # (issue)\r\n\r\nSolves https://github.com/huggingface/accelerate/issues/2023\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker @LysandreJik @pacman100 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 13,
        "changed_files": 5,
        "created_at": "2023-10-13T15:49:43Z",
        "closed_at": "2023-10-16T16:27:45Z",
        "merged_at": "2023-10-16T16:27:45Z",
        "body": "This PR makes a couple of fixes to `ConversationalPipeline` to make it a lot easier to use:\r\n\r\n- Inputs can now just be conversations in standard list-of-dicts format. I think the `Conversation` class is quite hard for users to discover, and this is a lot more intuitive.\r\n- We no longer read `max_length` because very few models set this parameter, and so it's almost always the default `PretrainedConfig` value of 20, which is very low. Before this change, most calls to `ConversationalPipeline` produced no output or unnecessarily truncated the input because this limit was hit. We change the pipeline to use `max_new_tokens` instead, which is more modern.\r\n\r\ncc @arthurzucker for pipeline review and @gante if he has any comments about setting the generation parameters properly!",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 176,
        "deletions": 102,
        "changed_files": 1,
        "created_at": "2023-10-13T13:23:19Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFor models that have Flash Attention 2 (FA2) implemented we currently pass both `padding_mask` and `attention_mask` to the respective vanilla attention class, *e.g.* `LlamaAttention` and to the FA2 class, *e.g.* `LlamaFlashAttention2`.\r\n\r\n**However**, `padding_mask` is **not** used for `LlamaAttention` and `attention_mask` is not used for `LlamaFlashAttention2`. Conceptually the two masks are the same, only that `attention_mask` is a 4D mask while `padding_mask` is a 2D mask.\r\n\r\nPassing around both masks and having both masks as concepts in our codebase is ambiguous and hurts readability. In this PR, I propose to remove the concept of `padding_mask` completely and instead keep `attention_mask` always in the 2D format. Only when we need the 4D mask in `LlamaAttention` we convert it on-the-fly.\r\nWe can cache previous 4D mask in a cache by using the 2D as the key.\r\n**Note:** In the `__hash__` of PyTorch tensors (used to create the dict key) corresponds to the memory location (see [here](https://github.com/pytorch/pytorch/issues/2569)) which means that we hit the cache only when the exact same 2D attention tensor is passed. This means we cannot cache attention_masks according to their value, but only to their ID. This is however exactly what we want to keep the previous behavior and also the way that makes sure we don't loose any performance.\r\n\r\nThis PR implements the attention cache for Llama as an example. If the PR is approved it will be applied to all classes that have FA2 implemented.\r\n\r\n**Note**: An additional benefit of this PR is that it will improve the performance when using FA2 as we will not create a 4D attention mask anymore.\r\n\r\n**Benchmarks**:\r\n\r\nThe following script was used to benchmark the effect this mask implementation has on forward and generate.\r\n\r\n```py\r\n#!/usr/bin/env python3\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\nimport time\r\nimport torch\r\n\r\nDEVICE = \"cuda:1\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\r\nmodel.to(DEVICE)\r\n\r\n\r\n# forward\r\nprint(\"Forward benchmarks\")\r\nprint(50 * \"=\")\r\n\r\nfor batch_size in (1, 4, 16):\r\n    for input_seq in (4, 16, 256):\r\n        input_ids = torch.ones((batch_size, input_seq), dtype=torch.long, device=DEVICE)\r\n        attention_mask = torch.ones_like(input_ids)\r\n        attention_mask[0, 3] = 0\r\n\r\n        times = []\r\n        for _ in range(3):\r\n            start_time = time.time()\r\n            with torch.no_grad():\r\n                logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\r\n            times.append(time.time() - start_time)\r\n\r\n        result = min(times)\r\n\r\n        print(f\"Forward bsz={batch_size}, input_seq={input_seq}: {result}\")\r\n\r\n\r\n# generate\r\nprint(\"Generate benchmarks\")\r\nprint(50 * \"=\")\r\n\r\nfor batch_size in (1, 16):\r\n    for input_seq in (4, 256):\r\n        input_ids = torch.ones((batch_size, input_seq), dtype=torch.long, device=DEVICE)\r\n        attention_mask = torch.ones_like(input_ids)\r\n        attention_mask[0, 3] = 0\r\n\r\n        times = []\r\n        for _ in range(3):\r\n            start_time = time.time()\r\n            out = model.generate(input_ids=input_ids, max_new_tokens=256, do_sample=False)\r\n            times.append(time.time() - start_time)\r\n\r\n        result = min(times)\r\n\r\n        print(f\"Generate bsz={batch_size}, input_seq={input_seq}: {result}\")\r\n```\r\n\r\n**This PR:**\r\n```\r\nForward benchmarks\r\n==================================================\r\nForward bsz=1, input_seq=4: 0.012479066848754883\r\nForward bsz=1, input_seq=16: 0.011297464370727539\r\nForward bsz=1, input_seq=256: 0.01240849494934082\r\nForward bsz=4, input_seq=4: 0.011190414428710938\r\nForward bsz=4, input_seq=16: 0.013025283813476562\r\nForward bsz=4, input_seq=256: 0.03526663780212402\r\nForward bsz=16, input_seq=4: 0.01126551628112793\r\nForward bsz=16, input_seq=16: 0.012389421463012695\r\nForward bsz=16, input_seq=256: 0.1560053825378418\r\nGenerate benchmarks\r\n==================================================\r\nGenerate bsz=1, input_seq=4: 4.527426719665527\r\nGenerate bsz=1, input_seq=256: 4.667049169540405\r\nGenerate bsz=16, input_seq=4: 5.524803400039673\r\nGenerate bsz=16, input_seq=256: 7.931211709976196\r\n```\r\n\r\n**Current main:**\r\n```\r\nForward benchmarks\r\n==================================================\r\nForward bsz=1, input_seq=4: 0.017528295516967773\r\nForward bsz=1, input_seq=16: 0.012105464935302734\r\nForward bsz=1, input_seq=256: 0.01315617561340332\r\nForward bsz=4, input_seq=4: 0.011912107467651367\r\nForward bsz=4, input_seq=16: 0.013910531997680664\r\nForward bsz=4, input_seq=256: 0.035504817962646484\r\nForward bsz=16, input_seq=4: 0.012083053588867188\r\nForward bsz=16, input_seq=16: 0.012537956237792969\r\nForward bsz=16, input_seq=256: 0.15653300285339355\r\nGenerate benchmarks\r\n==================================================\r\nGenerate bsz=1, input_seq=4: 4.554980516433716\r\nGenerate bsz=1, input_seq=256: 4.695344686508179\r\nGenerate bsz=16, input_seq=4: 5.55778431892395\r\nGenerate bsz=16, input_seq=256: 7.969247102737427\r\n```\r\n\r\n=> We don't see any drop in performance at all.\r\n\r\nI've verified that the following tests all pass on a single GPU (RTX4090):\r\nFA2:\r\n```\r\nRUN_SLOW=1 pytest -m flash_attn_test tests/models/llama/test_modeling_llama.py\r\n```\r\nand all Llama fast tests:\r\n```\r\nCUDA_VISIBLE_DEVICES=\"0\" RUN_SLOW=1 pytest tests/models/llama/test_modeling_llama.py\r\n```",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 297,
        "deletions": 242,
        "changed_files": 1,
        "created_at": "2023-10-13T12:30:06Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR continues performance docs refactor in the transformers docs. It focuses mainly on the \"Efficient Training on Multiple GPUs\" doc and contains the following changes:\r\n* Improves clarity and readability\r\n* Adds links to Accelerate where relevant\r\n* Removes a duplicated chunk of content\r\n* Resolves some formatting issues",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-13T10:22:45Z",
        "closed_at": "2023-10-13T16:38:37Z",
        "merged_at": "2023-10-13T16:38:37Z",
        "body": "# What does this PR do?\r\n\r\nFixes: https://github.com/huggingface/transformers/issues/26078\r\n\r\nIn fact the current flava docstring fails, running:\r\n\r\n```bash\r\npytest --doctest-modules src/transformers/models/flava/modeling_flava.py::transformers.models.flava.modeling_flava.FlavaModel.forward\r\n```\r\n\r\nLeads to a failure. This PR fixes it\r\n\r\n`contrastive_logits_per_image` cannot be retrieved from `FlavaModel` as detailed here: https://github.com/huggingface/transformers/issues/26078#issuecomment-1713369568 , therefore I have decided to modify the docstring to reflect the correct way of retrieving outputs from `FlavaModel`\r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2023-10-13T09:27:43Z",
        "closed_at": "2023-10-16T14:16:04Z",
        "merged_at": "2023-10-16T14:16:04Z",
        "body": "I think the space between the eos and bos tokens is not present in the actual template output. I'm using this documentation as a reference for everyone asking about prompting, so would like to clarify whether there's a space or not :)\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ Rocketknight1, @ArthurZucker \r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 14,
        "changed_files": 7,
        "created_at": "2023-10-13T09:04:59Z",
        "closed_at": "2023-10-13T10:56:50Z",
        "merged_at": "2023-10-13T10:56:50Z",
        "body": "# What does this PR do?\r\n\r\nFixes: https://github.com/huggingface/transformers/issues/26778 \r\n\r\nFor users that have FA-1 installed in their environment importing some modules will lead to errors, making `transformers` unusable. This PR fixes this issue by changing `is_flash_attn_available()` to `is_flash_attn_2_available()`.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-10-13T08:33:02Z",
        "closed_at": "2023-10-17T08:13:37Z",
        "merged_at": "2023-10-17T08:13:37Z",
        "body": "This PR fixes point 3 of https://github.com/huggingface/transformers/issues/25970 by clarifying the penalty and reward cases for RepetitionPenaltyLogitsProcessor and EncoderRepetitionPenaltyLogitsProcessor within the docstrings.\r\n\r\nPR https://github.com/huggingface/transformers/pull/26129 was the original copy, but I have accidentally deleted my repo that submitted the PR, so I cannot reopen that PR \ud83d\ude47 \r\n\r\n@gante, for your review please.",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-13T06:50:44Z",
        "closed_at": "2023-10-13T08:20:31Z",
        "merged_at": "2023-10-13T08:20:31Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26638\r\n\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-13T03:38:07Z",
        "closed_at": "2023-10-13T07:52:11Z",
        "merged_at": "2023-10-13T07:52:11Z",
        "body": "# What does this PR do?\r\n\r\nThere is a change on this Hub repo. \r\n\r\nhttps://huggingface.co/HuggingFaceM4/tiny-random-idefics/commit/cb18d7776a2e725eb20a2b9f8addf0991b9194b6\r\n\r\nthat is used for testing.\r\n\r\ncc @leot13: you and @ArthurZucker  knows better than me on this",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2023-10-13T02:56:19Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nAs title.\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-12T21:04:01Z",
        "closed_at": "2023-10-16T08:08:44Z",
        "merged_at": "2023-10-16T08:08:44Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes https://github.com/huggingface/transformers/issues/26638\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-12T19:22:34Z",
        "closed_at": "2023-10-13T14:10:27Z",
        "merged_at": "2023-10-13T14:10:27Z",
        "body": "One of the Falcon tests was broken by the shift to using in-library checkpoints. The reason is that the Falcon tokenizer now correctly doesn't return `token_type_ids`, since Falcon doesn't use that input. The test was discarding that key, but this is no longer necessary.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 577,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-12T18:46:48Z",
        "closed_at": "2023-10-13T18:13:48Z",
        "merged_at": "2023-10-13T18:13:48Z",
        "body": "Hello, I would like to add the Brazilian Portuguese translation to the README.md. I translated it as faithfully and comprehensibly as possible to the original. I only did the translation, without adding anything else besides that. I kindly ask you to consider this contribution and review it for approval.\r\n@stevhliu and @MKhalusova ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-10-12T16:33:02Z",
        "closed_at": "2023-10-13T13:48:38Z",
        "merged_at": "2023-10-13T13:48:38Z",
        "body": "Disable the system prompt by default in LLaMA, as requested by Meta.\r\n\r\nNote that I've already pushed chat templates to the repos, so this change should mostly already be in effect! I'm just changing it in the library too for consistency.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-12T15:38:57Z",
        "closed_at": "2023-10-12T16:22:09Z",
        "merged_at": "2023-10-12T16:22:09Z",
        "body": "# What does this PR do?\r\n\r\nSkip `TrainerIntegrationFSDP::test_basic_run_with_cpu_offload` if `torch < 2.1` as this takes 4 hours on `torch 2.0`.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-10-12T14:12:10Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This is to fix https://github.com/huggingface/transformers/issues/26762.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 67,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2023-10-12T14:03:31Z",
        "closed_at": "2023-10-16T17:56:53Z",
        "merged_at": "2023-10-16T17:56:53Z",
        "body": "# What does this PR do?\r\n\r\nFirst step of an alternative design of https://github.com/huggingface/transformers/pull/26560 \r\n\r\nFor quantized models, instead of introducing a complex logic of retrieving the original weights dtype, I propose to simply add a private attribute `_quantization_original_dtype` in the config object. \r\n\r\n`to` method does not need to be touched here as `to` cannot be called on quantized models (but for GPTQ models you can call `to` to perform device placement only - **not** for dtype casting)\r\n\r\nthat way we could adapt #26560 to simply check if the config has the attribute `_quantization_original_dtype` which is the case only for quantized models, else retrieve the dtype by retrieving the dtype of the linear layer weights in a classic manner.\r\n\r\ncc @LysandreJik ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-12T13:26:15Z",
        "closed_at": "2023-10-12T15:43:06Z",
        "merged_at": "2023-10-12T15:43:06Z",
        "body": "# What does this PR do?\r\n\r\nThe PR #23909 changed the result of `vocab_size` of \r\n\r\n```\r\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\r\n```\r\nfrom `262` to `256`, but the logit has shape `[1, 2048, 262]`.\r\n\r\nLet's use `len` here.\r\n\r\n\r\n## To reproduce:\r\n```\r\nfrom transformers import PerceiverTokenizer\r\n\r\ntokenizer = PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\r\n# 256 on `2da88537` but `262` on one commit before (`835b0a05`)\r\nprint(tokenizer.vocab_size)\r\n```",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 8,
        "changed_files": 7,
        "created_at": "2023-10-12T10:16:41Z",
        "closed_at": "2023-10-12T16:00:27Z",
        "merged_at": "2023-10-12T16:00:27Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\nHi, this is my first PR to the project. It fixes some minor typos.\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-10-12T09:48:09Z",
        "closed_at": "2023-10-12T10:31:12Z",
        "merged_at": "2023-10-12T10:31:12Z",
        "body": "# What does this PR do?\r\n\r\nFix `MistralIntegrationTest` OOM",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-12T08:47:26Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 54,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-12T08:20:35Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAdd a default decoder_attention_mask for EncoderDecoderModel during training. Since we are already creating the default decoder_input_ids from the labels, we should also create a default decoder_attention_mask to go with it.\r\n\r\nBefore this fix, the user was shown a warning message (\"we strongly recommend passing an attention mask\") when following the [new suggested](https://huggingface.co/docs/transformers/model_doc/encoder-decoder#training) [method of training](https://github.com/huggingface/transformers/blob/bef02fd6b9cde975c51607fb936050ef706ff6d8/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L42-L47) for EncoderDecoderModel. Although there has been no report of the bug in the wild yet as it would be a silent bug, I suspect it will likely cause [this particular issue](https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked) if the pad tokens in the default decoder_input_ids are not taken into account.\r\n\r\nFixes #25271 \r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker @StevenSong \r\n\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 331,
        "deletions": 331,
        "changed_files": 154,
        "created_at": "2023-10-12T08:03:29Z",
        "closed_at": "2023-10-12T08:28:41Z",
        "merged_at": "2023-10-12T08:28:41Z",
        "body": "Hello!\r\n\r\n# What does this PR do?\r\nThis PR resolves numerous occurrences of the following:\r\n```python\r\n        raise ValueError(\r\n            \"`training_args.block_size` needs to be a multiple of the global train/eval batch size.\"\r\n            f\"Got {training_args.block_size}, {train_batch_size} and {eval_batch_size} respectively instead.\"\r\n        )\r\n```\r\nWhich results in errors like:\r\n```\r\n`training_args.block_size` needs to be a multiple of the global train/eval batch size.Got 4, 6 and 12 respectively instead.\"\r\n                                                                                     ^^\r\n```\r\n\r\n## How did I go about it?\r\nI created a simple regex: `([\"'][^\"'\\n]*[^ n])([\"']\\n *f?r?[\"'][^ ])`, which matches full strings that don't end with spaces or `n` (due to `\\n`), followed by a newline, some spaces, and then another string that also doesn't start with a space. I manually went over all cases where this pattern matched through the codebase, and replaced the pattern with `$1 $2` if it was indeed a real mistake.\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs \r\n\r\n## Who can review?\r\n\r\n@ArthurZucker \r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-10-12T07:55:50Z",
        "closed_at": "2023-10-12T09:24:18Z",
        "merged_at": "2023-10-12T09:24:18Z",
        "body": "# What does this PR do?\r\n\r\nFix `PersimmonIntegrationTest` OOM: just use 8-bit",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 52,
        "deletions": 54,
        "changed_files": 5,
        "created_at": "2023-10-12T07:41:53Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nWIP addition to #26499 (I don't have write rights there and @justinvyu is currently out of office).\r\n\r\nOpening here as draft PR to run CI - we can either switch to this PR if CI succeeds or merge my updates into #26499.\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 46,
        "deletions": 47,
        "changed_files": 33,
        "created_at": "2023-10-12T07:36:10Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFix for E721 errors which are annoying when doing `make style` with a newer `ruff`",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 773,
        "deletions": 0,
        "changed_files": 10,
        "created_at": "2023-10-12T06:51:42Z",
        "closed_at": "2023-10-17T22:01:21Z",
        "merged_at": "2023-10-17T22:01:21Z",
        "body": "# What does this PR do?\r\n\r\nAdd japanese translation to `ja/internal` \r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26746\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-11T18:03:01Z",
        "closed_at": "2023-10-12T11:19:24Z",
        "merged_at": "2023-10-12T11:19:24Z",
        "body": "\r\n\r\n# What does this PR do?\r\nI ran into a case where an external library was depending on the `new_user_input` field of Conversation. https://github.com/SeldonIO/MLServer/blob/release/1.4.x/runtimes/huggingface/mlserver_huggingface/codecs/utils.py#L37 \r\n\r\nThis field was deprecated as part of the refactor, but if `transformers` wants to maintain backwards compatibility for now (which is mentioned in a few comments) then there's a good argument for supporting it. Some comments referred to it as an \"internal\" property, but it didn't start with `_` as is Python convention, so I think it's reasonable that other libraries were referencing it directly.\r\n\r\nIt's not difficult to add it to the other supported backwards-compatible properties. In addition, the implementation of `past_user_inputs` didn't actually match the past behavior (it would contain the most recent message as well) so I updated that as well.\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@Rocketknight1\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-11T16:59:36Z",
        "closed_at": "2023-10-16T13:29:47Z",
        "merged_at": "2023-10-16T13:29:47Z",
        "body": "# What does this PR do?\r\n\r\nFixes bug: \r\nWhen `resume_from_checkpoint` is set to True, the `state` is imported, which includes `self.state.best_model_checkpoint`. If the best model is still the one corresponding to the model in the resume_from_checkpoint directory when saving the model during training, an exception will be raised at `best_model_index = checkpoints_sorted.index(str(Path(self.state.best_model_checkpoint)))`: \r\nValueError: '/path/to/checkpoint-100' is not in list.\r\n\r\nFixes issue(https://github.com/huggingface/transformers/issues/22172)\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n- trainer: @muellerzr and @pacman100",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 47,
        "changed_files": 2,
        "created_at": "2023-10-11T16:30:58Z",
        "closed_at": "2023-10-12T08:01:08Z",
        "merged_at": "2023-10-12T08:01:08Z",
        "body": "# What does this PR do?\r\n\r\n`Blip2ForConditionalGeneration` doctest fails due to OOM. This PR restructures its docstring to make it pass doctesting.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-11T13:18:39Z",
        "closed_at": "2023-10-11T14:23:37Z",
        "merged_at": "2023-10-11T14:23:36Z",
        "body": "# What does this PR do?\r\n\r\nMake daily CI use torch 2.1.0 \ud83e\udd1e ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 37,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-11T12:57:37Z",
        "closed_at": "2023-10-11T14:46:42Z",
        "merged_at": "2023-10-11T14:46:42Z",
        "body": "# What does this PR do?\r\n\r\nFix #25948",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 12,
        "changed_files": 12,
        "created_at": "2023-10-11T12:22:16Z",
        "closed_at": "2023-10-11T14:16:27Z",
        "merged_at": "2023-10-11T14:16:27Z",
        "body": "# What does this PR do?\r\n\r\nThis PR changes the print statement to print the right path in the `no_trainer` scripts when loading in a checkpoint\r\n\r\nFixes # (issue)\r\n\r\nSolves https://github.com/huggingface/transformers/issues/25998\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker \r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-11T10:48:34Z",
        "closed_at": "2023-10-11T13:58:23Z",
        "merged_at": "2023-10-11T13:58:23Z",
        "body": "# What does this PR do?\r\n1. More than 70 tests are currently failing because the function `extract_hyperparameters_from_trainer` tries to access `use_cuda_amp` attribute which is no more available. ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 608,
        "deletions": 42,
        "changed_files": 4,
        "created_at": "2023-10-10T17:44:33Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAdd FA2 to all Bart-like models",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 54,
        "deletions": 52,
        "changed_files": 13,
        "created_at": "2023-10-10T17:27:11Z",
        "closed_at": "2023-10-11T17:45:39Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ydshieh \r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-10T12:17:11Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Right now, we can only load the GPTQ Quantized model on the CUDA device. The flag `load_gptq_on_cpu` adds the support to load the GPTQ models on the CPU. The larger variants of the model are hard to load/run/trace on the GPU and that's the rationale behind adding this flag.\r\n\r\nSigned-Off By: Vivek Khandelwal <vivek@nod-labs.com>",
        "comments": 15
    },
    {
        "merged": true,
        "additions": 127,
        "deletions": 45,
        "changed_files": 14,
        "created_at": "2023-10-10T09:11:40Z",
        "closed_at": "2023-10-11T12:12:09Z",
        "merged_at": "2023-10-11T12:12:09Z",
        "body": "# What does this PR do?\r\n\r\n`Copied from` for test files.\r\n\r\nRun `make fix-copies` will show \r\n\r\n```bash\r\npython utils/check_copies.py --fix_and_overwrite\r\nDetected changes, rewriting tests/models\\longformer\\test_tokenization_longformer.py.\r\n```\r\n\r\nI will need to update `test_tokenization_longformer.py` before merge.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-10T08:48:24Z",
        "closed_at": "2023-10-10T09:20:31Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nThis PR makes batch inference for blip 2 processing with one text instead of passing a list of text. \r\nIssue #26633 \r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-10-10T08:41:14Z",
        "closed_at": "2023-10-11T14:08:55Z",
        "merged_at": "2023-10-11T14:08:55Z",
        "body": "The stalebot has crashed once again due to the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"scripts/stale.py\", line 67, in <module>\r\n    main()\r\n  File \"scripts/stale.py\", line 57, in main\r\n    issue.create_comment(\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/github/Issue.py\", line 290, in create_comment\r\n    headers, data = self._requester.requestJsonAndCheck(\"POST\", f\"{self.url}/comments\", input=post_parameters)\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/github/Requester.py\", line 494, in requestJsonAndCheck\r\n    return self.__check(*self.requestJson(verb, url, parameters, headers, input, self.__customConnection(url)))\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/github/Requester.py\", line 629, in requestJson\r\n    return self.__requestEncode(cnx, verb, url, parameters, headers, input, encode)\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/github/Requester.py\", line 726, in __requestEncode\r\n    status, responseHeaders, output = self.__requestRaw(cnx, verb, url, requestHeaders, encoded_input)\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/github/Requester.py\", line 760, in __requestRaw\r\n    response = cnx.getresponse()\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/github/Requester.py\", line 174, in getresponse\r\n    r = verb(\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/requests/sessions.py\", line 637, in post\r\n    return self.request(\"POST\", url, data=data, json=json, **kwargs)\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/requests/adapters.py\", line 486, in send\r\n    resp = conn.urlopen(\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 931, in urlopen\r\n    retries = retries.increment(method, url, response=response, _pool=self)\r\n  File \"/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/github/GithubRetry.py\", line 179, in increment\r\n    raise Requester.createException(response.status, response.headers, content)  # type: ignore\r\ngithub.GithubException.GithubException: 403 {\"message\": \"Unable to create comment because issue is locked.\", \"documentation_url\": \"https://docs.github.com/articles/locking-conversations/\"}\r\n```\r\n\r\nI believe it's the first time it's encountering a locked issue, hence the failure. I tested locally that it ran fine on this specific issue.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2023-10-10T06:54:58Z",
        "closed_at": "2023-10-11T16:01:23Z",
        "merged_at": "2023-10-11T16:01:23Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26638\r\n\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-10-10T06:29:56Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes  https://github.com/huggingface/transformers/pull/25870#issuecomment-1754523487\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n\r\ncc @ydshieh \r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-10T05:57:23Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes and part of  #26638\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-10T00:30:54Z",
        "closed_at": "2023-10-10T09:50:10Z",
        "merged_at": "2023-10-10T09:50:10Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\nThe table in this doc has a syntactical mistake thus not rendering.\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-09T20:27:46Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\nLet's make our lifes easier.\nHistory is messed will fix",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 28,
        "changed_files": 25,
        "created_at": "2023-10-09T19:50:13Z",
        "closed_at": "2023-10-10T19:35:17Z",
        "merged_at": "2023-10-10T19:35:17Z",
        "body": "`jnp.array` is a function, not a type:\r\nhttps://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html so it never makes sense to use `jnp.array` in a type annotation. Presumably the intent was to write `jnp.ndarray` aka `jax.Array`.\r\n\r\nFor a similar PR in `diffusers`, please see https://github.com/huggingface/diffusers/pull/4719.\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\ncc @pcuenca @patrickvonplaten\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 59,
        "deletions": 18,
        "changed_files": 4,
        "created_at": "2023-10-09T17:52:17Z",
        "closed_at": "2023-10-11T13:52:21Z",
        "merged_at": "2023-10-11T13:52:21Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nThis PR speeds up assistant generation / speculative decoding for encoder-decoder models such as Distill-Whisper by ~20-30%.\r\n\r\nImprovements:\r\n- If assistant and model share same encoder, let's allow the user to pass `assistant_encoder_outputs` so that the inputs are not encoded twice (gives ~20% speed-up)\r\n- In the small loop I don't think we have to allocate tensors for the attention mask all the time. This is done automatically by the model if necessary (gives ~3,4% speed-up)\r\n- The heuristic to increase / decrease the number of \"look-ahead\" tokens doesn't work well for whisper, can we maybe allow the user to somehow disable it? Maybe via a config attribute?",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 53,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-09T17:06:48Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nHandles the case when the pipeline is batched and we want to return word-level timestamps. Here, we have a batch (list) of strides, which we slice to get the first stride value.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 408,
        "deletions": 0,
        "changed_files": 204,
        "created_at": "2023-10-09T14:37:11Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Adds a few additional links to the Hub. If there are ideas to do this even deeper, would love to incorporate them in this PR.\r\nI would love loved to share initial collections as well but I have to check if I can embed collection visualisation in the doc page + it's not relevant for quite a few architecures.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-09T13:43:58Z",
        "closed_at": "2023-10-09T18:04:25Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26638\r\n\r\n## Who can review?\r\n\r\n@ydshieh \r\n",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-10-09T13:16:07Z",
        "closed_at": "2023-10-09T14:39:58Z",
        "merged_at": "2023-10-09T14:39:58Z",
        "body": "Fixes the stale bot",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 13,
        "changed_files": 4,
        "created_at": "2023-10-09T13:11:41Z",
        "closed_at": "2023-10-09T15:39:06Z",
        "merged_at": "2023-10-09T15:39:06Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue) https://github.com/huggingface/transformers/issues/26638 for `CLIPTokenizer`, `CLIPTokenizerFast`, `CLIPVisionConfig`\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-10-09T12:30:29Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-09T12:26:10Z",
        "closed_at": null,
        "merged_at": null,
        "body": "\u2026lass and created a 'LogitsProcessorList' containing both a custom logits processor and a 'MinLengthLogitsProcessor'.\r\n\r\n# What does this PR do?\r\n1. It adds a custom logits processor (CustomLogitsProcessor) to the generate method.\r\n2. Threshold for EOS Probability:\r\n3. In this case the LPL list, it includes the original suppress_tokens_logits_processor, the custom custom_logits_processor, and a MinLengthLogitsProcessor to ensure that the generated output has a minimum length of 1 token.\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n#26672\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-10-09T09:11:54Z",
        "closed_at": "2023-10-10T15:05:49Z",
        "merged_at": "2023-10-10T15:05:49Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26638  by fixing a typo in docstring of `LlamaConfig`\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ydshieh @abzdel\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 147,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-10-09T08:29:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Adding a EncT5 model/framework for non-autoregressive tasks, as described in https://arxiv.org/abs/2110.08426 (Algorithm 1 for now, but I plan to follow up with support for Algorithm 2).  There is already a T5ForSequenceClassification variant, but EncT5 uses less params (just a single decoder layer instead) and, according to the paper, achieves similar results in the benchmarks as BERT and T5.\r\n\r\nContext in #14097 \r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker @amyeroberts @sgugger @sjrl \r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-09T08:21:06Z",
        "closed_at": "2023-10-13T15:20:27Z",
        "merged_at": "2023-10-13T15:20:27Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes KeyError for Mistral. convert_mistral_weights_to_hf.py checks for the key \"ragged_attention\", but the HF release of mistral uses \"sliding_window\" instead.\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 206,
        "deletions": 39,
        "changed_files": 2,
        "created_at": "2023-10-09T07:22:02Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Closes #26553\r\n\r\nHello!\r\n\r\n# What does this PR do?\r\nI had a few hours on Saturday to work up a draft version of the updated KV caching mechanism as discussed in #26553. Ideally, this should allow Attention Sinks (https://github.com/tomaarsen/attention_sinks) / StreamingLLM (https://arxiv.org/abs/2309.17453) to be easily implemented in a third-party or in transformers directly. \r\n\r\nThe implementation doesn't work well yet, as the VRAM usage quickly shoots up after generating even just 8 tokens. This is probably some bug that I haven't had time for yet. There's a few other comments that I have on specific sections of code, so I'll write some comments below.\r\n\r\n## Goal for this draft\r\nThe intention for this draft is to continue discussion about whether this is moving in the right direction, and to determine the scope (e.g. do we want to include this updated `Cache` for all architectures that use KV caching?). \r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review?\r\n\r\n@patrickvonplaten \r\n@gante\r\n@LysandreJik \r\n@Guangxuan-Xiao\r\n\r\n- Tom Aarsen\r\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-09T03:49:43Z",
        "closed_at": "2023-10-11T13:53:32Z",
        "merged_at": "2023-10-11T13:53:32Z",
        "body": "# What does this PR do?\r\nDocstring for `SwinModel` is added.\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. https://github.com/huggingface/transformers/issues/26638\r\n- [ x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ydshieh \r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 128,
        "deletions": 75,
        "changed_files": 5,
        "created_at": "2023-10-08T20:51:17Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nWill need the next release of tokenizers.\r\nThis is what it will allows us to do:\r\n```python \r\nfrom transformers import AutoTokenizer\r\n\r\ntok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast = True, from_slow = True, legacy = True)\r\nprint(tok.tokenize(\"Hey <s>. how are you\"))\r\ntok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast = True, from_slow = True, legacy = False)\r\nprint(tok.tokenize(\"Hey <s>. how are you\"))\r\n```\r\n```python \r\n['\u2581Hey', '\u2581', '<s>', '\u2581.', '\u2581how', '\u2581are', '\u2581you']\r\n['\u2581Hey', '\u2581', '<s>', '.', '\u2581how', '\u2581are', '\u2581you']\r\n```\r\nThe extra space that was always added to the eos is now gone. \r\nThis is fully backward compatible and can be saved / push to the hub. The metaspace rust object was not broken, and the argument can also be easily set `tok._tokenizer.pre_tokenizer.legacy = legacy`. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 9,
        "changed_files": 3,
        "created_at": "2023-10-08T19:12:46Z",
        "closed_at": "2023-10-09T10:34:02Z",
        "merged_at": "2023-10-09T10:34:02Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue) https://github.com/huggingface/transformers/issues/26638 for `CLIPSegTextConfig`, `CLIPSegVisionConfig`, `CLIPTextConfig`\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n@ydshieh\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-08T18:59:51Z",
        "closed_at": "2023-10-09T12:22:44Z",
        "merged_at": "2023-10-09T12:22:44Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue) https://github.com/huggingface/transformers/issues/26638 for `CLIPImageProcessor`\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n@ydshieh \r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 89,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2023-10-08T18:35:09Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue) https://github.com/huggingface/transformers/issues/26672\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 12
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-08T16:29:52Z",
        "closed_at": "2023-10-13T10:13:43Z",
        "merged_at": "2023-10-13T10:13:43Z",
        "body": "# What does this PR do?\r\n\r\nFixes #26638 only for `DPRConfig`\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ydshieh \r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 58,
        "deletions": 22,
        "changed_files": 4,
        "created_at": "2023-10-08T10:59:40Z",
        "closed_at": "2023-10-11T15:03:32Z",
        "merged_at": "2023-10-11T15:03:32Z",
        "body": "# What does this PR do?\r\n\r\nFixes #26638 only for `LlamaTokenizer` and `LlamaTokenizerFast`.\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker and @younesbelkada\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 4645,
        "deletions": 12,
        "changed_files": 32,
        "created_at": "2023-10-08T09:48:53Z",
        "closed_at": "2023-10-13T14:41:24Z",
        "merged_at": "2023-10-13T14:41:24Z",
        "body": "# What does this PR do?\r\n\r\nThis PR adds OWLv2 in a way that is more compliant to the Transformers-philosophy, meaning one paper = one model = one file. Rather than modifying the existing OWL-ViT (v1), this PR adds a new standalone Owlv2ForObjectDetection model. It copies 99% of the v1 model, only modifying the object detection head.\r\n\r\nFollow-up of #26379 \r\n\r\nTo do:\r\n\r\n- [x] fix image processor\r\n- [x] add doc tests\r\n- [x] verify objectness logits shape",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 310,
        "deletions": 26,
        "changed_files": 2,
        "created_at": "2023-10-08T07:10:00Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nIn this PR, we implement [LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models](https://browse.arxiv.org/abs/2308.16137) proposed in August 2023 on Llama models, which removes length limits of large language models, and enables them to generate to infinite lengths with intact performance similar to training time, without any parameter updates.  Results show that LM-Infinite can encode as long as 128k tokens on a single A100 GPU, and allows generating to infinite tokens, thanks to its $O(n)$ time and space complexity for encoding and $O(1)$ complexity for decoding.  Interestingly, later [StreamingLLM](https://github.com/mit-han-lab/streaming-llm) recently also observed alike results on a similar technique.\r\n\r\nThis implementation is related to and in response to [an issue](https://github.com/huggingface/transformers/issues/26553) discussing about integrating LM-Infinite into Huggingface Transformers.\r\n\r\nThis implementation defaults back to exactly the original behaviors if the sequence length is less than 4k, and only differ in performance when sequence is longer. In this way, normal users will feel minimal differences. All usages of functionalities remain the same. Moreover, this implementation encodes sequence one at a time, similar to other normal Transformer models, compared with StreamingLLM which encodes token by token (i.e., 2k `forward` passes for encoding context of length 2k).\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. It is related to [this issue](https://github.com/huggingface/transformers/issues/26553).\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n- @ArthurZucker \r\n- @Rocketknight1 \r\n- @LysandreJik\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2023-10-08T04:36:01Z",
        "closed_at": "2023-10-16T08:11:46Z",
        "merged_at": "2023-10-16T08:11:46Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26638\r\n\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 13
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 27,
        "changed_files": 6,
        "created_at": "2023-10-08T00:03:34Z",
        "closed_at": "2023-10-12T14:51:35Z",
        "merged_at": "2023-10-12T14:51:35Z",
        "body": "Fixes #26638 \r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ydshieh\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-07T17:32:44Z",
        "closed_at": "2023-10-10T18:36:33Z",
        "merged_at": "2023-10-10T18:36:33Z",
        "body": "# What does this PR do?\r\n\r\nFixes a typo in the Flax code for T5 model.\r\n\r\nThere is a typo in the Attention module of the Flax version of T5, where the attention_mask updated by the `_concatenate_to_cache` method should override the previous attention_mask but does not because of a misnamed variable.\r\n\r\nFixes #26564 \r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n\r\n## Who can review?\r\n\r\n@sanchit-gandhi ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 618,
        "deletions": 8,
        "changed_files": 3,
        "created_at": "2023-10-07T16:11:21Z",
        "closed_at": "2023-10-10T14:42:38Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\nfixes #26638 issue\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@ydshieh \r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-10-07T15:34:32Z",
        "closed_at": "2023-10-12T15:01:14Z",
        "merged_at": "2023-10-12T15:01:14Z",
        "body": "Fixes #26638\r\n## Before Submitting\r\n\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x]  Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests), Pull Request section?\r\n- [x]  Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link to it if that's the case.\r\n- [x]  Did you make sure to update the documentation with your changes? Here are the [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can Review?\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag members/contributors who may be interested in your PR.\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-07T15:30:19Z",
        "closed_at": "2023-10-09T09:04:57Z",
        "merged_at": "2023-10-09T09:04:57Z",
        "body": "# What does this PR do?\r\n\r\nThis PR fixes a minor typo in the code comment. It replaces the word \"clone\" with \"copy\" in the comment to improve clarity.\r\n\r\n## Before submitting\r\n- [X ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag members/contributors who may be interested in your PR.\r\n\r\n\r\n\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-07T15:18:03Z",
        "closed_at": "2023-10-09T09:07:12Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-10-07T14:33:15Z",
        "closed_at": "2023-10-09T08:43:00Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26638  by fixing a typo in docstring of `LlamaConfig`\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ydshieh \r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1838,
        "deletions": 0,
        "changed_files": 7,
        "created_at": "2023-10-07T14:16:03Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Add Fast model \r\nFix issue #26501",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-07T12:02:50Z",
        "closed_at": "2023-10-08T13:41:34Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAddress #26638 \r\n\r\nHELP NEEDED!\r\n\r\n## Description\r\nI have followed the guide to contributing for #26638 , but couldn't generate anything using\r\n\r\n`python3 utils/check_docstrings.py --fix_and_overwrite`. \r\n\r\nThe output,\r\n```\r\n2023-10-07 12:47:24.603397: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2023-10-07 12:47:24.603448: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2023-10-07 12:47:24.603485: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-10-07 12:47:25.616995: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n```\r\n\r\nA similar issue: [Tensorflow](https://github.com/tensorflow/tensorflow/issues/62002)\r\n\r\nThe docstring did not miss anything thus i have removed `DPTModel` from\r\n `utils/check_docstrings.py` > `OBJECTS_TO_IGNORE`.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@ydshieh \r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-07T10:44:49Z",
        "closed_at": "2023-10-16T13:28:28Z",
        "merged_at": null,
        "body": "Added Output example for diffusion image generation model\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-07T09:39:43Z",
        "closed_at": "2023-10-10T18:49:10Z",
        "merged_at": "2023-10-10T18:49:10Z",
        "body": "# What does this PR do?\r\n\r\nThis pull request updates the `run_summarization.py` script in the PyTorch implementation to fix the default value of `source_prefix`. Currently, the default value is set to `\"\"`, which can be misleading when fine-tuning T5 models. This PR sets the default value to `None`, aligning it with other implementations and ensuring that users are prompted with a warning when not providing a source prefix.\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26653\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n- @ArthurZucker \r\n- @younesbelkada  \r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2023-10-07T08:59:13Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAs the title says, this PR introduces two modifications:\r\n1. remove the gradient clipping code related to fairscale FSDP, as fairscale FSDP has been removed. See: https://github.com/huggingface/transformers/pull/25702 \r\n2. updated the error message when mixing `--bf16` and `--half_precision_backend apex` as the `cuda_amp` option is no longer available. I'm not sure how to write it more appropriately, so I just use `auto` instead.\r\n\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 94,
        "deletions": 91,
        "changed_files": 37,
        "created_at": "2023-10-07T08:50:58Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nupdated docstring for the following :\r\n\r\n`BartTokenizerFast`,` BarthezTokenizerFast`, `BertTokenizerFast`, `AlbertTokenizerFast`, `BigBirdTokenizerFast`, `BlenderbotSmallTokenizerFast`, `BlenderbotTokenizerFast`",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-07T07:24:12Z",
        "closed_at": "2023-10-07T08:10:11Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\nfixed doc string for GPT2Config\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n  @ydshieh\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-07T06:26:56Z",
        "closed_at": "2023-10-09T10:18:03Z",
        "merged_at": "2023-10-09T10:18:03Z",
        "body": "Two typos found in reviewing this documentation.\r\n\r\n1) max_new_tokens=***6*** as ***4*** is not sufficient to generate \"Vegetables\" as indicated - you will get only \"Veget\". (incidentally - some mention of how to select this value might be useful as it seems to change in each example)\r\n\r\n2) inputs = processor(prompts, return_tensors=\"pt\"))***.to(device)*** as inputs need to be on the same device (as they are in all other examples on the page)\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 3383,
        "deletions": 3,
        "changed_files": 29,
        "created_at": "2023-10-07T01:21:28Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nIn this PR, we implement [LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models](https://browse.arxiv.org/abs/2308.16137) on Llama model proposed in August 2023, which removes length limits of large language models, and enables them to generate to infinite lengths with intact performance similar to training time, without any parameter updates.  Results show that LM-Infinite can encode as long as 128k tokens on a single A100 GPU, and allows generating to infinite tokens, thanks to its $O(n)$ time and space complexity for encoding and $O(1)$ complexity for decoding.  Interestingly, later [StreamingLLM](https://github.com/mit-han-lab/streaming-llm) recently also observed alike results on a similar technique.\r\n\r\nThis implementation is related to and in response to [an issue](https://github.com/huggingface/transformers/issues/26553) discussing about integrating LM-Infinite into Huggingface Transformers.\r\n\r\nThis LlamaInfinite model allows for seamless adaptation from usage of original Llama models, simply by substituting `LlamaForCausalModel.from_pretrained()` with `LlamaInfiniteForCausalLM.from_pretrained()`. All other usages remain the same. This implementation is compatible with all previous Llama model checkpoints without any modifications, so new model checkpoints are needed.\r\n\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. It is related to [this issue](https://github.com/huggingface/transformers/issues/26553).\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n- @tomaarsen\r\n- @patrickvonplaten\r\n- @LysandreJik\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-06T21:39:01Z",
        "closed_at": "2023-10-12T21:20:32Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26638 \r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 33,
        "deletions": 31,
        "changed_files": 6,
        "created_at": "2023-10-06T21:28:24Z",
        "closed_at": "2023-10-12T15:01:00Z",
        "merged_at": "2023-10-12T15:01:00Z",
        "body": "# What does this PR do?\r\n\r\n- [x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n\r\n\r\n\r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-06T21:11:47Z",
        "closed_at": "2023-10-09T14:32:13Z",
        "merged_at": "2023-10-09T14:32:13Z",
        "body": "fixed DonutImageProcessor docstring and removed from OBJECTS_TO_IGNORE in check_docstrings.py",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-06T16:29:26Z",
        "closed_at": "2023-10-09T09:34:44Z",
        "merged_at": "2023-10-09T09:34:44Z",
        "body": "# What does this PR do?\r\n\r\nFix broken links as follows:\r\n\r\n[-] https://blog.openai.com/language-unsupervised/\r\n[+] https://openai.com/research/language-unsupervised/\r\n\r\n[-] https://blog.openai.com/better-language-models/\r\n[+] https://openai.com/research/better-language-models/\r\n\r\n[-] https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX\r\n[+] https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-06T16:08:12Z",
        "closed_at": "2023-10-09T09:42:09Z",
        "merged_at": "2023-10-09T09:42:09Z",
        "body": "# What does this PR do?\r\n\r\nThe torch and torch pipeline job again (almost) reach the limit of CircleCI runner's RAM (16 G).\r\n\r\ntorch pipeline job crashed [nightly run here](https://app.circleci.com/pipelines/github/huggingface/transformers/74561/workflows/5c4e2b07-0688-4d86-bac6-85250ebbd741/jobs/944985)\r\n\r\nA screenshot (of [another run](https://app.circleci.com/pipelines/github/huggingface/transformers/74427/workflows/d5130b1e-0bec-4b41-867c-7dd61722434e/jobs/942978/resources))\r\n\r\n<img width=\"1061\" alt=\"Screenshot 2023-10-06 175412\" src=\"https://github.com/huggingface/transformers/assets/2521628/2f9dc0b6-3a0d-46a6-88c6-bd1e787f06b6\">\r\n\r\nLet's use 6 workers for now: I will try to find time to investigate what happens recently.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 108,
        "deletions": 4,
        "changed_files": 18,
        "created_at": "2023-10-06T15:25:35Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Lots of user repos use common tokenizers like `LlamaTokenizer` even for non-LLaMA models. Unfortunately, these tokenizers come with default chat templates, which may result in user confusion if they use `apply_chat_template`. We want to avoid a scenario where users think they're getting the right template but actually aren't!\r\n\r\nThis PR raises a short warning the first time a `default_chat_template` is read, which happens when `apply_chat_template` or `ConversationalPipeline` is called for a model without a `chat_template` set. The warning tells the user what's happening, and suggests adding an explicit chat template.\r\n\r\nOver time, the goal is to eventually deprecate and remove `default_chat_template`, because we want to avoid using class-level templates entirely, and move to explicit repo level `chat_template` attributes. This PR starts that process, while still retaining the feature for backward compatibility.",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-06T15:12:11Z",
        "closed_at": "2023-10-06T15:36:22Z",
        "merged_at": "2023-10-06T15:36:22Z",
        "body": "# What does this PR do?\r\n\r\nAn example demonstrating how to fix docstring. I will write a step-by-step guide on a new issue page. But this PR servers as an final output of such fix.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 64,
        "deletions": 24,
        "changed_files": 4,
        "created_at": "2023-10-06T13:58:51Z",
        "closed_at": "2023-10-16T14:24:31Z",
        "merged_at": "2023-10-16T14:24:31Z",
        "body": "# What does this PR do?\r\n\r\nJust showing an approach which duplicate files but in a minimal way.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-06T10:03:43Z",
        "closed_at": "2023-10-16T13:29:01Z",
        "merged_at": "2023-10-16T13:29:01Z",
        "body": "# What does this PR do?\r\n1. Make fsdp ram efficient loading optional. Certain models are having issues when handling meta devices during pre-trained model loading. Fixes https://github.com/huggingface/accelerate/issues/1948 and https://github.com/huggingface/accelerate/issues/2031 by making the ram efficient loading optional.\r\n2. This PR should be merged after PR https://github.com/huggingface/accelerate/pull/2037 is merged.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 142,
        "deletions": 143,
        "changed_files": 4,
        "created_at": "2023-10-06T04:41:55Z",
        "closed_at": "2023-10-06T16:24:29Z",
        "merged_at": "2023-10-06T16:24:29Z",
        "body": "# What does this PR do?\r\n\r\nBriefly reviewed the Simplified Chinese documentation.\r\n\r\n- Fix punctuation and spacing issues.\r\n- Fixes #26603 \r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@stevhliu\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 66,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-10-05T17:22:44Z",
        "closed_at": "2023-10-06T11:04:40Z",
        "merged_at": "2023-10-06T11:04:40Z",
        "body": "This PR updates the chat templates doc with more tips on writing your own templates.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-05T15:31:24Z",
        "closed_at": "2023-10-11T12:03:52Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nEnables prompt tuning for BigCode models.\r\nCurrently, prompt tuning for bigcode models fail due to this error AttributeError (no GPTBigCodeForCausalLM has no attribute named `word_embeddings`).\r\n\r\n@ArthurZucker, @younesbelkada, @pacman100 ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-05T14:10:24Z",
        "closed_at": "2023-10-05T14:57:51Z",
        "merged_at": "2023-10-05T14:57:51Z",
        "body": "# What does this PR do?\r\n\r\nDoc builder docker image build starts to fail with\r\n\r\n```\r\nThe package you are trying to install is only a placeholder project on PyPI.org repository.\r\nThis package is hosted on NVIDIA Python Package Index.\r\n  \r\nThis package can be installed as:\r\n\r\n$ pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com/ pytorch-quantization\r\n```\r\n\r\nI tried to install it with the suggested command, but the doc build step will fail with some cuda libary issue.\r\n(when building `transformers/docs/source/en/model_doc/qdqbert.md`)\r\n\r\n**I removed the line that installs `pytorch-quantization` and doc build can pass (and docker image built).**",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-10-05T14:01:55Z",
        "closed_at": "2023-10-05T17:41:09Z",
        "merged_at": null,
        "body": "# What does this PR do?\n\n<!--\nCongratulations! You've made it this far! You're not quite done yet though.\n\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\n\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\n-->\n\n<!-- Remove if not applicable -->\n\nFixes # (issue)\n\n\n## Before submitting\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\n      Pull Request section?\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\n      to it if that's the case.\n- [ ] Did you make sure to update the documentation with your changes? Here are the\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\n- [ ] Did you write any new necessary tests?\n\n\n## Who can review?\n\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\nmembers/contributors who may be interested in your PR.\n\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\n\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\n Please tag fewer than 3 people.\n\nModels:\n\n- text models: @ArthurZucker and @younesbelkada\n- vision models: @amyeroberts\n- speech models: @sanchit-gandhi\n- graph models: @clefourrier\n\nLibrary:\n\n- flax: @sanchit-gandhi\n- generate: @gante\n- pipelines: @Narsil\n- tensorflow: @gante and @Rocketknight1\n- tokenizers: @ArthurZucker\n- trainer: @muellerzr and @pacman100\n\nIntegrations:\n\n- deepspeed: HF Trainer/Accelerate: @pacman100\n- ray/raytune: @richardliaw, @amogkam\n- Big Model Inference: @SunMarc\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\n\nDocumentation: @stevhliu and @MKhalusova\n\nHF projects:\n\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\n- datasets: [different repo](https://github.com/huggingface/datasets)\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\n\nMaintained examples (not research project or legacy):\n\n- Flax: @sanchit-gandhi\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\n- TensorFlow: @Rocketknight1\n\n -->",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2023-10-05T12:53:47Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nThis registers `ModelOutput` as supported `torch` pytree nodes. Currently all subclasses of `ModelOutput` are already registered by `ModelOutput.__init_subclass__()`. This PR additionally registers `ModelOutput` itself as supported `torch` pytree nodes.\r\n\r\nSee also:\r\n\r\n- #25357\r\n- #25358\r\n- https://github.com/pytorch/pytorch/pull/109684#discussion_r1347384106\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [X] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [X] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@sgugger\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 5746,
        "deletions": 0,
        "changed_files": 20,
        "created_at": "2023-10-05T12:44:55Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nAdds support for the CharacterBERT model: https://github.com/helboukkouri/character-bert\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-05T10:54:36Z",
        "closed_at": "2023-10-05T13:33:35Z",
        "merged_at": "2023-10-05T13:33:35Z",
        "body": "# What does this PR do?\r\n\r\nCurrently failed, see [here](https://github.com/huggingface/transformers/actions/runs/6413071912/job/17411419865)",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-05T09:42:59Z",
        "closed_at": "2023-10-05T13:33:05Z",
        "merged_at": "2023-10-05T13:33:05Z",
        "body": "One may want to create the ClearML task manually (for example by calling `Task.init`) to be used after the training ends. This PR handles that case.",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 835,
        "deletions": 356,
        "changed_files": 5,
        "created_at": "2023-10-05T07:45:10Z",
        "closed_at": null,
        "merged_at": null,
        "body": "### What does this PR do?\r\nRefactoring `modeling_utils.py` to move all quantization-related logic into new `HFQuantizer` class\r\n\r\n### Reasons and benefits:\r\n- easier to understand how quantization works during loading.\r\n- easier to add new quantization methods like SPQR.\r\n- much easier to implement 4-bit serialization (once BnB supports it)\r\n- it was a rainy day outside )\r\n\r\n### Things to be done:\r\n- extend `HFQuantizer` functionality to cover `_load_pretrained_model()` and `_load_state_dict_into_model()`\r\n- extend `HFQuantizer` functionality in BnB to cover `set_module_quantized_tensor_to_device()` from `integrations/bitsandbytes.py`\r\n- review multiple TODOs left in the comments.\r\n- undo temporary changes in the tests\r\n- check for backward compatibility issues\r\n- eliminate repeats and redundancies, if any\r\n- consider fully absorbing `integrations/bitsandbytes.py` into new class\r\n- move new classes / functions into proper project file. possibly separate folder.\r\n\r\n### Current state:\r\n- reworked code up to `_load_state_dict_into_model()`\r\n- all tests in BnB and GPTQ still pass with `RUN_SLOW=1`.\r\n\r\nsummoning  @SunMarc and @younesbelkada to comment on the idea and current state.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-05T07:26:05Z",
        "closed_at": "2023-10-05T07:38:09Z",
        "merged_at": "2023-10-05T07:38:09Z",
        "body": "# What does this PR do?\r\nFixes #26597 by updating the default data format",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-10-05T07:11:14Z",
        "closed_at": "2023-10-05T08:27:05Z",
        "merged_at": "2023-10-05T08:27:05Z",
        "body": "# What does this PR do?\r\n\r\nFix failing tests on `main` due to torch 2.1",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 30,
        "changed_files": 4,
        "created_at": "2023-10-05T07:06:35Z",
        "closed_at": "2023-10-06T14:40:55Z",
        "merged_at": "2023-10-06T14:40:55Z",
        "body": "# What does this PR do?\r\nFixes #26605 by making sure the LlamaTokenizerFast handles the cases when bos or eos is None and `add_bos_token` is set to `True` by raising an error. ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 109,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-05T02:21:32Z",
        "closed_at": null,
        "merged_at": null,
        "body": "@LysandreJik done i will upload the code according to your request if you like feel free to ask anything",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-04T23:41:38Z",
        "closed_at": "2023-10-06T11:04:54Z",
        "merged_at": "2023-10-06T11:04:54Z",
        "body": "#### Issue\r\nSometimes training with `fp16`, the `dtype` of `self.inv_freq` will be changed from `fp32` to `fp16`. This scenario will cause the position `t` to use dtype of `fp16`, like\r\n```\r\nt = torch.arange(seq_len, device=device, dtype=torch.float16)\r\n```\r\n\r\nAfter converting to onnx graph, however, Range Ops  in `onnx` do not support `fp16` as [here](https://github.com/onnx/onnx/blob/e11dacfa9930eafd3b34391ef5422d09ba9896dc/onnx/defs/generator/defs.cc#L488-L557)\r\n\r\n#### Update\r\n\r\nUse the below to avoid this scenario\r\n\r\n```\r\nt = torch.arange(seq_len, device=device).to(dtype)\r\n```\r\n\r\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-04T15:03:25Z",
        "closed_at": "2023-10-04T15:28:54Z",
        "merged_at": "2023-10-04T15:28:54Z",
        "body": "Please ignore the extra unwanted bracket",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 134,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-04T14:55:24Z",
        "closed_at": "2023-10-16T13:12:04Z",
        "merged_at": "2023-10-16T13:12:04Z",
        "body": "This PR contributes task guide for image-to-image. cc @NielsRogge @rafaelpadilla @amyeroberts ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-04T14:52:24Z",
        "closed_at": "2023-10-04T15:47:56Z",
        "merged_at": "2023-10-04T15:47:55Z",
        "body": "# What does this PR do?\r\ncc @LysandreJik, @ydshieh main is red quite often because of these two tests (custom pr-ci as well) let's skip for now. Marked flaky by test insight",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-10-04T14:38:30Z",
        "closed_at": "2023-10-04T15:02:35Z",
        "merged_at": "2023-10-04T15:02:35Z",
        "body": "# What does this PR do?\r\n\r\nRuns `make fix-copies` to correct the Mistral config after the PR #26052, and subsequently fills out the missing docstring args.\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-10-04T14:33:32Z",
        "closed_at": "2023-10-04T15:02:49Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\nFixes the doc of mistral",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-04T14:22:05Z",
        "closed_at": "2023-10-04T14:22:59Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-04T13:09:02Z",
        "closed_at": "2023-10-04T15:48:12Z",
        "merged_at": "2023-10-04T15:48:12Z",
        "body": "url changed, previous returns 404",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-10-04T12:50:07Z",
        "closed_at": "2023-10-05T12:44:31Z",
        "merged_at": "2023-10-05T12:44:31Z",
        "body": "# What does this PR do?\r\n\r\nSame PR as https://github.com/huggingface/transformers/pull/26484 but without any extra diff \r\n\r\nBefore this PR we were performing a simple check if module_name in key but that lead to some modules silently converted in fp32.\r\n\r\nFor example instructblip models got their word_embedding layers converted in fp32 because _keep_in_fp32_modules includes \"wo\" which is contained in the string word_embedding. The fix is to check if module_name in key.split(\".\")\r\n\r\nI can confirm with this PR the failing instructblip tests now pass\r\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-04T11:11:17Z",
        "closed_at": "2023-10-04T15:34:33Z",
        "merged_at": "2023-10-04T15:34:33Z",
        "body": "# What does this PR do?\r\nThe convert_t5x_checkpoint_to_pytorch is used to convert t5x models into pytorch models.\r\nHowever, it contains a typo at [line 142](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/convert_t5x_checkpoint_to_pytorch.py#L142): The wi weights in **decoder** is converted to weights in **encoder**, and it makes the following errors when we run the script **for t5 v1.0 models** (where Split MLP layers is false and uses T5DenseActDense(wi) instead of T5DenseGatedActDense(wi_0, wi_1) is run:\r\n\r\n```\r\n File \"/opt/conda/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1497, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for T5ForConditionalGeneration:\r\n        Missing key(s) in state_dict: \"decoder.block.0.layer.2.DenseReluDense.wi.weight\", \"decoder.block.1.layer.2.DenseReluDense.wi.weight\", \"decoder.block.2.layer.2.DenseReluDense.wi.weight\", \"decoder.block.3.layer.2.DenseReluDense.wi.weight\", \"decoder.block.4.layer.2.DenseReluDense.wi.weight\", \"decoder.block.5.layer.2.DenseReluDense.wi.weight\", \"decoder.block.6.layer.2.DenseReluDense.wi.weight\", \"decoder.block.7.layer.2.DenseReluDense.wi.weight\", \"decoder.block.8.layer.2.DenseReluDense.wi.weight\", \"decoder.block.9.layer.2.DenseReluDense.wi.weight\", \"decoder.block.10.layer.2.DenseReluDense.wi.weight\", \"decoder.block.11.layer.2.DenseReluDense.wi.weight\".\r\n        Unexpected key(s) in state_dict: \"encoder.block.0.layer.2.DenseReluDense.wi.weight\", \"encoder.block.1.layer.2.DenseReluDense.wi.weight\", \"encoder.block.2.layer.2.DenseReluDense.wi.weight\", \"encoder.block.3.layer.2.DenseReluDense.wi.weight\", \"encoder.block.4.layer.2.DenseReluDense.wi.weight\", \"encoder.block.5.layer.2.DenseReluDense.wi.weight\", \"encoder.block.6.layer.2.DenseReluDense.wi.weight\", \"encoder.block.7.layer.2.DenseReluDense.wi.weight\", \"encoder.block.8.layer.2.DenseReluDense.wi.weight\", \"encoder.block.9.layer.2.DenseReluDense.wi.weight\", \"encoder.block.10.layer.2.DenseReluDense.wi.weight\", \"encoder.block.11.layer.2.DenseReluDense.wi.weight\".\r\n```\r\n\r\nThe following is the changed part:\r\n```\r\n            if split_mlp_wi:\r\n\r\n                new[f\"decoder.block.{i}.layer.2.DenseReluDense.wi_0.weight\"] = wi[0].T\r\n\r\n                new[f\"decoder.block.{i}.layer.2.DenseReluDense.wi_1.weight\"] = wi[1].T\r\n\r\n            else:\r\n\r\n                new[f\"encoder.block.{i}.layer.2.DenseReluDense.wi.weight\"] = wi.T\r\n```\r\n\r\nWhen changing the following line from \r\n```\r\nnew[f\"encoder.block.{i}.layer.2.DenseReluDense.wi.weight\"] = wi.T\r\n```\r\nto \r\n```\r\nnew[f\"decoder.block.{i}.layer.2.DenseReluDense.wi.weight\"] = wi.T\r\n``` \r\nthe code works fine.\r\n\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\nRelated pull request seems to be [this one](https://github.com/huggingface/transformers/pull/20801), \r\nso tagging the original author @basting and the one mentioned in that PR:\r\n@patrickvonplaten\r\n@sanchit-gandhi\r\n@ArthurZucker\r\n@younesbelkada\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\n\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-10-04T09:32:29Z",
        "closed_at": "2023-10-06T13:53:59Z",
        "merged_at": "2023-10-06T13:53:59Z",
        "body": "# What does this PR do?\r\n\r\nFix failing `MusicgenTest .test_pipeline_text_to_audio`",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 341,
        "deletions": 9,
        "changed_files": 3,
        "created_at": "2023-10-04T08:29:00Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis introduces flash attention 2 for bert as discussed in  https://github.com/huggingface/transformers/issues/26350\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-04T03:07:44Z",
        "closed_at": null,
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/423\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 0,
        "changed_files": 8,
        "created_at": "2023-10-04T01:22:26Z",
        "closed_at": "2023-10-04T15:09:48Z",
        "merged_at": "2023-10-04T15:09:48Z",
        "body": "# What does this PR do?\r\n\r\nThis PR adds `# Copied from` statements to audio feature extractors (and other related data processing modules) that use the `floats_list` function. The [Whisper version](https://github.com/huggingface/transformers/blob/2f3ea08a077ba3133fa8a604b22436cad250b055/tests/models/whisper/test_feature_extraction_whisper.py#L42-L53) of `floats_list` is considered the \"canonical\" version of the function, since the CLAP model has an existing `# Copied from tests.models.whisper.test_feature_extraction_whisper.floats_list` statement for its [`floats_list`](https://github.com/huggingface/transformers/blob/2f3ea08a077ba3133fa8a604b22436cad250b055/tests/models/clap/test_feature_extraction_clap.py#L37-L38) function.\r\n\r\nThis issue was brought up in https://github.com/huggingface/transformers/pull/24799#discussion_r1325148003, https://github.com/huggingface/transformers/pull/24799#discussion_r1326652124, and the following thread.\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@sanchit-gandhi\r\n@ArthurZucker\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-04T00:53:56Z",
        "closed_at": "2023-10-04T09:52:46Z",
        "merged_at": "2023-10-04T09:52:46Z",
        "body": "Bumps [pillow](https://github.com/python-pillow/Pillow) from 9.3.0 to 10.0.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/python-pillow/Pillow/releases\">pillow's releases</a>.</em></p>\n<blockquote>\n<h2>10.0.1</h2>\n<p><a href=\"https://pillow.readthedocs.io/en/stable/releasenotes/10.0.1.html\">https://pillow.readthedocs.io/en/stable/releasenotes/10.0.1.html</a></p>\n<h2>Changes</h2>\n<ul>\n<li>Updated libwebp to 1.3.2 <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7395\">#7395</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Updated zlib to 1.3 <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7344\">#7344</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n</ul>\n<h2>10.0.0</h2>\n<p><a href=\"https://pillow.readthedocs.io/en/stable/releasenotes/10.0.0.html\">https://pillow.readthedocs.io/en/stable/releasenotes/10.0.0.html</a></p>\n<h2>Changes</h2>\n<ul>\n<li>Fixed deallocating mask images <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7246\">#7246</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Added ImageFont.MAX_STRING_LENGTH <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7244\">#7244</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Fix Windows build with pyproject.toml <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7230\">#7230</a> [<a href=\"https://github.com/nulano\"><code>@\u200bnulano</code></a>]</li>\n<li>Do not close provided file handles with libtiff <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7199\">#7199</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Convert to HSV if mode is HSV in getcolor() <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7226\">#7226</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Added alpha_only argument to getbbox() <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7123\">#7123</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Prioritise speed in <em>repr_png</em> <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7242\">#7242</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Limit size even if one dimension is zero in decompression bomb check <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7235\">#7235</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Restored 32-bit support <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7234\">#7234</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Removed deleted file from codecov.yml and increased coverage threshold <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7232\">#7232</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Removed support for 32-bit <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7228\">#7228</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Use --config-settings instead of deprecated --global-option <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7171\">#7171</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Better C integer definitions <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/6645\">#6645</a> [<a href=\"https://github.com/Yay295\"><code>@\u200bYay295</code></a>]</li>\n<li>Fixed finding dependencies on Cygwin <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7175\">#7175</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Improved checks in font_render <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7218\">#7218</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Change <code>grabclipboard()</code> to use PNG compression on macOS <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7219\">#7219</a> [<a href=\"https://github.com/abey79\"><code>@\u200babey79</code></a>]</li>\n<li>Added PyPy 3.10 and removed PyPy 3.8 <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7216\">#7216</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Added in_place argument to ImageOps.exif_transpose() <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7092\">#7092</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Corrected error code <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7177\">#7177</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Use &quot;not in&quot; <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7174\">#7174</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Only call text_layout once in getmask2 <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7206\">#7206</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Fixed calling putpalette() on L and LA images before load() <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7187\">#7187</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Removed unused INT64 definition <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7180\">#7180</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Updated xz to 5.4.3 <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7136\">#7136</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Fixed saving TIFF multiframe images with LONG8 tag types <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7078\">#7078</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Do not set size unnecessarily if image fails to open <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7056\">#7056</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Removed unused code <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7210\">#7210</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Removed unused variables <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7205\">#7205</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Fixed signedness comparison warning <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7203\">#7203</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Fixed combining single duration across duplicate APNG frames <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7146\">#7146</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Remove temporary file when error is raised <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7148\">#7148</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Do not use temporary file when grabbing clipboard on Linux <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7200\">#7200</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>If the clipboard fails to open on Windows, wait and try again <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7141\">#7141</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Fixed saving multiple 1 mode frames to GIF <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7181\">#7181</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Replaced absolute PIL import with relative import <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7173\">#7173</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n<li>Removed files and types override <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7194\">#7194</a> [<a href=\"https://github.com/radarhere\"><code>@\u200bradarhere</code></a>]</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/python-pillow/Pillow/blob/main/CHANGES.rst\">pillow's changelog</a>.</em></p>\n<blockquote>\n<h2>10.0.1 (2023-09-15)</h2>\n<ul>\n<li>\n<p>Updated libwebp to 1.3.2 <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7395\">#7395</a>\n[radarhere]</p>\n</li>\n<li>\n<p>Updated zlib to 1.3 <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7344\">#7344</a>\n[radarhere]</p>\n</li>\n</ul>\n<h2>10.0.0 (2023-07-01)</h2>\n<ul>\n<li>\n<p>Fixed deallocating mask images <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7246\">#7246</a>\n[radarhere]</p>\n</li>\n<li>\n<p>Added ImageFont.MAX_STRING_LENGTH <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7244\">#7244</a>\n[radarhere, hugovk]</p>\n</li>\n<li>\n<p>Fix Windows build with pyproject.toml <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7230\">#7230</a>\n[hugovk, nulano, radarhere]</p>\n</li>\n<li>\n<p>Do not close provided file handles with libtiff <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7199\">#7199</a>\n[radarhere]</p>\n</li>\n<li>\n<p>Convert to HSV if mode is HSV in getcolor() <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7226\">#7226</a>\n[radarhere]</p>\n</li>\n<li>\n<p>Added alpha_only argument to getbbox() <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7123\">#7123</a>\n[radarhere. hugovk]</p>\n</li>\n<li>\n<p>Prioritise speed in <em>repr_png</em> <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7242\">#7242</a>\n[radarhere]</p>\n</li>\n<li>\n<p>Do not use CFFI access by default on PyPy <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7236\">#7236</a>\n[radarhere]</p>\n</li>\n<li>\n<p>Limit size even if one dimension is zero in decompression bomb check <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7235\">#7235</a>\n[radarhere]</p>\n</li>\n<li>\n<p>Use --config-settings instead of deprecated --global-option <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7171\">#7171</a>\n[radarhere]</p>\n</li>\n<li>\n<p>Better C integer definitions <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/6645\">#6645</a>\n[Yay295, hugovk]</p>\n</li>\n<li>\n<p>Fixed finding dependencies on Cygwin <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7175\">#7175</a>\n[radarhere]</p>\n</li>\n<li>\n<p>Changed grabclipboard() to use PNG instead of JPG compression on macOS <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7219\">#7219</a>\n[abey79, radarhere]</p>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/e34d346f10c0b1c814661e662a3e0c1ef084cf1c\"><code>e34d346</code></a> Updated order</li>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/a62f2402a6bcf11a0a1670542216725a3f9190e0\"><code>a62f240</code></a> 10.0.1 version bump</li>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/d50250d9eab741ae3ddd592d8910cfd7973b9d35\"><code>d50250d</code></a> Added release notes for 10.0.1</li>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/b4c7d4b8b2710b7af6cc944a804902eb75fd9056\"><code>b4c7d4b</code></a> Update CHANGES.rst [ci skip]</li>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/730f74600e8215ab510f71bb1fbb49d906c4356b\"><code>730f746</code></a> Updated libwebp to 1.3.2</li>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/b0e28048d692effadfe7a4268a03e1d20e0198bb\"><code>b0e2804</code></a> Updated zlib to 1.3</li>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/6e28ed1f36d0eb74053af54e1eddc9c29db698cd\"><code>6e28ed1</code></a> 10.0.0 version bump</li>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/c827f3b30f50bf04fd65daeeba6bbfd56fc7b50e\"><code>c827f3b</code></a> Merge pull request <a href=\"https://redirect.github.com/python-pillow/Pillow/issues/7246\">#7246</a> from radarhere/deallocate</li>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/39a3b1d83edcf826c3864e26bedff5b4e4dd331b\"><code>39a3b1d</code></a> Fixed deallocating mask images</li>\n<li><a href=\"https://github.com/python-pillow/Pillow/commit/8c1dc819fd91471825da01976ac0e0bc8789590f\"><code>8c1dc81</code></a> Update CHANGES.rst [ci skip]</li>\n<li>Additional commits viewable in <a href=\"https://github.com/python-pillow/Pillow/compare/9.3.0...10.0.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pillow&package-manager=pip&previous-version=9.3.0&new-version=10.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).\n\n</details>",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-04T00:16:44Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This can be triggered fairly quickly with low precision e.g. `bfloat16` and `typical_p=0.99`.\r\n\r\n@gante ",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-03T18:50:32Z",
        "closed_at": "2023-10-06T08:49:03Z",
        "merged_at": "2023-10-06T08:49:03Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes the issue: https://github.com/huggingface/transformers/issues/26575\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. https://github.com/huggingface/transformers/issues/26575\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ArthurZucker \r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-10-03T17:29:20Z",
        "closed_at": "2023-10-03T17:43:42Z",
        "merged_at": "2023-10-03T17:43:42Z",
        "body": "Skip tests temporarily so that `main` remains green.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 69,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2023-10-03T17:24:40Z",
        "closed_at": "2023-10-04T14:15:30Z",
        "merged_at": "2023-10-04T14:15:30Z",
        "body": "This PR adds a new `add_generation_prompt` argument to `apply_chat_template`. We need this when we want to chat with a chat model - if the model has special tokens that indicate the start of a bot message, then we need to append these to the end of a generation prompt to indicate to the model that it should write a bot reply, and not continue the user message or something like that.\r\n\r\nNote that many prompts (e.g. LLaMA) don't include special tokens at the start of bot messages - this makes them very easy to generate for. This argument would have no effect for them, so I don't need to update their chat templates to support it.\r\n\r\nFixes #26539",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 762,
        "deletions": 74,
        "changed_files": 6,
        "created_at": "2023-10-03T17:11:01Z",
        "closed_at": null,
        "merged_at": null,
        "body": "As per title, this PR proposes to support natively `torch.nn.functional.scaled_dot_product_attention` in transformers. I propose to enable SDPA by default if `torch>=2.1` (release in a few days), for the reasons written in the PR. The support could then be extended using https://github.com/huggingface/optimum/blob/main/optimum/bettertransformer/models/attention.py.\r\n\r\n WDYT @younesbelkada?\r\n\r\nNote that the tests for BetterTransformer are very comprehensive: notably testing the logits for forward, backward. I am not sure whether there is a need to add any new test in transformers following this.\r\n\r\n---\r\n\r\nThe introduced `_inplace_unmask_padding` is a workaround for https://github.com/pytorch/pytorch/issues/110213.\r\n\r\nIt behaves as follow:\r\n\r\nIf attention_mask is\r\n ```\r\n    [[0, 0, 1]\r\n     [1, 1, 1]\r\n     [0, 1, 1]]\r\n```\r\nand expanded_mask is (e.g. here left-padding case)\r\n```\r\n    [[[[0, 0, 0],\r\n       [0, 0, 0],\r\n       [0, 0, 1]]],\r\n     [[[1, 0, 0],\r\n       [1, 1, 0],\r\n       [1, 1, 1]]],\r\n     [[[0, 0, 0],\r\n       [0, 1, 0],\r\n       [0, 1, 1]]]]\r\n```\r\nthen the modified expanded_mask will be\r\n```\r\n    [[[[1, 1, 1],   <-- modified\r\n       [1, 1, 1],   <-- modified\r\n       [0, 0, 1]]],\r\n     [[[1, 0, 0],\r\n       [1, 1, 0],\r\n       [1, 1, 1]]],\r\n     [[[1, 1, 1],   <-- modified\r\n       [0, 1, 0],\r\n       [0, 1, 1]]]]\r\n```\r\n\r\nThe implementation could probably be simplified though (merging `_expand_mask` and `_inplace_unmask_padding`).\r\n\r\nModifying as such the attention mask is fine given that we modify it only for pad tokens on the `-2` dimension. Softmax is computed on the `-1` dimension, and thus there is no change for the relevant non-padding tokens.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 501,
        "deletions": 247,
        "changed_files": 47,
        "created_at": "2023-10-03T17:00:06Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n- sets the defaults for AddedToken instances where needed to match what is pushed to the hub\r\n- sets the default for AddedToken to not strip left and right to match the fast tokenizers\r\n- fixes the `added_tokens.json` file: a recent push made it save all the added tokens encoder, but it should only save the indexes greater than the vocab size for forward compatibility. \r\n- fixes the list of `additionnal_special_tokens` that were added twice / overwritten\r\n- fixes `add_tokens` : if the added tokens is a string we check if it's not already in the added vocab instead of always defaulting to strip left or right.\r\n- fixes saving: the added_tokens_decoder should not add a `\"__type \":\"AddedToken\"` field to the added tokens otherwise the previous versions of transformers will try to load them.\r\n\r\nfixes #26732, fixes #26775, fixes #26773, fixes #26768, fixes #26859",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2023-10-03T16:27:03Z",
        "closed_at": "2023-10-05T13:20:39Z",
        "merged_at": "2023-10-05T13:20:39Z",
        "body": "# What does this PR do?\r\n\r\nThis PR adds the feature of accepting arbitary number of input and output channels when using the Swin2SR model. This allows to perform super resolution from greyscale (1 channel) to color (rgb), or from low resolution multi band satellite to high resolution rgb satellite.\r\n\r\nAll examples and pretrained models are running as expected based on my tests. No new dependencies have been added.\r\n\r\nJust use it like\r\n\r\n```python\r\nfrom transformers import Swin2SRForImageSuperResolution, Swin2SRConfig\r\nimport torch\r\n\r\nSwin2SRConfig = (\r\n     num_channels_in=1,\r\n     num_channels_out=3\r\n)\r\nmodel = Swin2SRForImageSuperResolution(Swin2SRConfig)\r\n\r\nwith torch.no_grad():\r\n    # or use the image preprocessor per default\r\n    out = model({\"pixel_values\":torch.randn((1,1,264,264))})\r\n\r\n```\r\n\r\nFixes #26566.\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Yes [here](https://github.com/huggingface/transformers/issues/26566)\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests? No, test where there already.\r\n\r\n\r\n## Tagging the reviewers\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2023-10-03T14:19:59Z",
        "closed_at": "2023-10-03T14:32:13Z",
        "merged_at": "2023-10-03T14:32:13Z",
        "body": "# What does this PR do?\n\nDoing `python -c \"from transformers import *\"` on a fresh env fails",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 31,
        "deletions": 23,
        "changed_files": 13,
        "created_at": "2023-10-03T12:14:29Z",
        "closed_at": "2023-10-06T08:40:47Z",
        "merged_at": "2023-10-06T08:40:47Z",
        "body": "Fixed case where behavior of BertTokenizer and BertTokenizerFast is different.\r\nAn empty list will be evaluated to `False` but not to `is None`.\r\n\r\n(I mistakenly closed my first merge request)\r\n\r\nFixes #26123\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 48,
        "deletions": 20,
        "changed_files": 4,
        "created_at": "2023-10-03T11:40:34Z",
        "closed_at": "2023-10-16T18:48:28Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFixes: https://github.com/huggingface/transformers/issues/26451\r\n\r\nCurrently performing bf16 fine-tuning with FA-2 leads to hidden states silently being casted in float16\r\nAs it is challenging to retrieve the original dtype of the model in case the model is quantized, I propose to store that dtype in a private attribute to be able to retrieve it conveniently without having to perform any sort of hack that gets the correct dtype if the model is quantized\r\n\r\n\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 54,
        "deletions": 16,
        "changed_files": 4,
        "created_at": "2023-10-03T10:55:51Z",
        "closed_at": "2023-10-03T12:53:09Z",
        "merged_at": "2023-10-03T12:53:09Z",
        "body": "# What does this PR do?\r\n\r\nThis PR fixes multiple bugs with PEFT and some corner cases such as:\r\n\r\n- loading an adapter model with `token` argument that currently fails\r\n- logger.warning that errors out since it does not seem to accept the argument `FutureWarning`\r\n- Saving that fails with 4-bit quantized models\r\n\r\nAdded some nice tests\r\n\r\ncc @LysandreJik \r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4170,
        "deletions": 0,
        "changed_files": 18,
        "created_at": "2023-10-03T10:45:50Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAdds BLIVA to transformers.  \r\n\r\n* Original repo: https://github.com/mlpc-ucsd/BLIVA\r\n* Paper: https://arxiv.org/abs/2308.09936\r\n\r\nFixes #26629 - issue with new model request\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-03T09:35:21Z",
        "closed_at": "2023-10-16T14:23:55Z",
        "merged_at": null,
        "body": "Currently we are testing only against mi210 AMD devices, this PR enables testing on mi250 as well \ud83e\udd17",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-03T05:58:02Z",
        "closed_at": "2023-10-03T06:55:13Z",
        "merged_at": "2023-10-03T06:55:12Z",
        "body": "[//]: # (dependabot-start)\n\u26a0\ufe0f  **Dependabot is rebasing this PR** \u26a0\ufe0f \n\nRebasing might not happen immediately, so don't worry if this takes some time.\n\nNote: if you make any changes to this PR yourself, they will take precedence over the rebase.\n\n---\n\n[//]: # (dependabot-end)\n\nBumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.9 to 1.26.17.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/releases\">urllib3's releases</a>.</em></p>\n<blockquote>\n<h2>1.26.17</h2>\n<ul>\n<li>Added the <code>Cookie</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>. (GHSA-v845-jxx5-vc9f)</li>\n</ul>\n<h2>1.26.16</h2>\n<ul>\n<li>Fixed thread-safety issue where accessing a <code>PoolManager</code> with many distinct origins would cause connection pools to be closed while requests are in progress (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2954\">#2954</a>)</li>\n</ul>\n<h2>1.26.15</h2>\n<ul>\n<li>Fix socket timeout value when HTTPConnection is reused (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2645\">urllib3/urllib3#2645</a>)</li>\n<li>Remove &quot;!&quot; character from the unreserved characters in IPv6 Zone ID parsing (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2899\">urllib3/urllib3#2899</a>)</li>\n<li>Fix IDNA handling of 'x80' byte (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2901\">urllib3/urllib3#2901</a>)</li>\n</ul>\n<h2>1.26.14</h2>\n<ul>\n<li>Fixed parsing of port 0 (zero) returning None, instead of 0 (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2850\">#2850</a>)</li>\n<li>Removed deprecated <code>HTTPResponse.getheaders()</code> calls in <code>urllib3.contrib</code> module.</li>\n</ul>\n<h2>1.26.13</h2>\n<ul>\n<li>Deprecated the <code>HTTPResponse.getheaders()</code> and <code>HTTPResponse.getheader()</code> methods.</li>\n<li>Fixed an issue where parsing a URL with leading zeroes in the port would be rejected even when the port number after removing the zeroes was valid.</li>\n<li>Fixed a deprecation warning when using cryptography v39.0.0.</li>\n<li>Removed the <code>&lt;4</code> in the <code>Requires-Python</code> packaging metadata field.</li>\n</ul>\n<h2>1.26.12</h2>\n<ul>\n<li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module. Both will be removed in v2.x. See this <a href=\"https://redirect.github.com/urllib3/urllib3/issues/2680\">GitHub issue</a> for justification and info on how to migrate.</li>\n</ul>\n<h2>1.26.11</h2>\n<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=\"https://github.com/sponsors/urllib3\">GitHub Sponsors</a>.</strong></p>\n<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=\"https://urllib3.readthedocs.io/en/latest/v2-roadmap.html\">Read more in the v2.0 Roadmap</a></p>\n<ul>\n<li>Fixed an issue where reading more than 2 GiB in a call to HTTPResponse.read would raise an OverflowError on Python 3.9 and earlier.</li>\n</ul>\n<h2>1.26.10</h2>\n<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=\"https://github.com/sponsors/urllib3\">GitHub Sponsors</a>.</strong></p>\n<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=\"https://urllib3.readthedocs.io/en/latest/v2-roadmap.html\">Read more in the v2.0 Roadmap</a></p>\n<p>:closed_lock_with_key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>\n<ul>\n<li>Removed support for Python 3.5</li>\n<li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/blob/main/CHANGES.rst\">urllib3's changelog</a>.</em></p>\n<blockquote>\n<h1>1.26.17 (2023-10-02)</h1>\n<ul>\n<li>Added the <code>Cookie</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>. (<code>[#3139](https://github.com/urllib3/urllib3/issues/3139) &lt;https://github.com/urllib3/urllib3/pull/3139&gt;</code>_)</li>\n</ul>\n<h1>1.26.16 (2023-05-23)</h1>\n<ul>\n<li>Fixed thread-safety issue where accessing a <code>PoolManager</code> with many distinct origins\nwould cause connection pools to be closed while requests are in progress (<code>[#2954](https://github.com/urllib3/urllib3/issues/2954) &lt;https://github.com/urllib3/urllib3/pull/2954&gt;</code>_)</li>\n</ul>\n<h1>1.26.15 (2023-03-10)</h1>\n<ul>\n<li>Fix socket timeout value when <code>HTTPConnection</code> is reused (<code>[#2645](https://github.com/urllib3/urllib3/issues/2645) &lt;https://github.com/urllib3/urllib3/issues/2645&gt;</code>__)</li>\n<li>Remove &quot;!&quot; character from the unreserved characters in IPv6 Zone ID parsing\n(<code>[#2899](https://github.com/urllib3/urllib3/issues/2899) &lt;https://github.com/urllib3/urllib3/issues/2899&gt;</code>__)</li>\n<li>Fix IDNA handling of '\\x80' byte (<code>[#2901](https://github.com/urllib3/urllib3/issues/2901) &lt;https://github.com/urllib3/urllib3/issues/2901&gt;</code>__)</li>\n</ul>\n<h1>1.26.14 (2023-01-11)</h1>\n<ul>\n<li>Fixed parsing of port 0 (zero) returning None, instead of 0. (<code>[#2850](https://github.com/urllib3/urllib3/issues/2850) &lt;https://github.com/urllib3/urllib3/issues/2850&gt;</code>__)</li>\n<li>Removed deprecated getheaders() calls in contrib module. Fixed the type hint of <code>PoolKey.key_retries</code> by adding <code>bool</code> to the union. (<code>[#2865](https://github.com/urllib3/urllib3/issues/2865) &lt;https://github.com/urllib3/urllib3/issues/2865&gt;</code>__)</li>\n</ul>\n<h1>1.26.13 (2022-11-23)</h1>\n<ul>\n<li>Deprecated the <code>HTTPResponse.getheaders()</code> and <code>HTTPResponse.getheader()</code> methods.</li>\n<li>Fixed an issue where parsing a URL with leading zeroes in the port would be rejected\neven when the port number after removing the zeroes was valid.</li>\n<li>Fixed a deprecation warning when using cryptography v39.0.0.</li>\n<li>Removed the <code>&lt;4</code> in the <code>Requires-Python</code> packaging metadata field.</li>\n</ul>\n<h1>1.26.12 (2022-08-22)</h1>\n<ul>\n<li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module.\nBoth will be removed in v2.x. See this <code>GitHub issue &lt;https://github.com/urllib3/urllib3/issues/2680&gt;</code>_\nfor justification and info on how to migrate.</li>\n</ul>\n<h1>1.26.11 (2022-07-25)</h1>\n<ul>\n<li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would\nraise an <code>OverflowError</code> on Python 3.9 and earlier.</li>\n</ul>\n<h1>1.26.10 (2022-07-07)</h1>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/c9016bf464751a02b7e46f8b86504f47d4238784\"><code>c9016bf</code></a> Release 1.26.17</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/01220354d389cd05474713f8c982d05c9b17aafb\"><code>0122035</code></a> Backport GHSA-v845-jxx5-vc9f (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3139\">#3139</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/e63989f97d206e839ab9170c8a76e3e097cc60e8\"><code>e63989f</code></a> Fix installing <code>brotli</code> extra on Python 2.7</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/2e7a24d08713a0131f0b3c7197889466d645cc49\"><code>2e7a24d</code></a> [1.26] Configure OS for RTD to fix building docs</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/57181d6ea910ac7cb2ff83345d9e5e0eb816a0d0\"><code>57181d6</code></a> [1.26] Improve error message when calling urllib3.request() (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3058\">#3058</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/3c0148048a523325819377b23fc67f8d46afc3aa\"><code>3c01480</code></a> [1.26] Run coverage even with failed jobs</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/d94029b7e2193ff47b627906a70e06377a09aae8\"><code>d94029b</code></a> Release 1.26.16</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/18e92145e9cddbabdf51c98f54202aa37fd5d4c8\"><code>18e9214</code></a> Use trusted publishing for PyPI</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/d25cf83bbae850a290fe34ed1610ae55c0558b36\"><code>d25cf83</code></a> [1.26] Fix invalid test_ssl_failure_midway_through_conn</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/25cca389496b86ee809c21e5b641aeaa74809263\"><code>25cca38</code></a> [1.26] Fix test_ssl_object_attributes</li>\n<li>Additional commits viewable in <a href=\"https://github.com/urllib3/urllib3/compare/1.26.9...1.26.17\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.9&new-version=1.26.17)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).\n\n</details>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-03T04:45:16Z",
        "closed_at": "2023-10-03T06:55:01Z",
        "merged_at": "2023-10-03T06:55:01Z",
        "body": "Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.5 to 1.26.17.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/releases\">urllib3's releases</a>.</em></p>\n<blockquote>\n<h2>1.26.17</h2>\n<ul>\n<li>Added the <code>Cookie</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>. (GHSA-v845-jxx5-vc9f)</li>\n</ul>\n<h2>1.26.16</h2>\n<ul>\n<li>Fixed thread-safety issue where accessing a <code>PoolManager</code> with many distinct origins would cause connection pools to be closed while requests are in progress (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2954\">#2954</a>)</li>\n</ul>\n<h2>1.26.15</h2>\n<ul>\n<li>Fix socket timeout value when HTTPConnection is reused (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2645\">urllib3/urllib3#2645</a>)</li>\n<li>Remove &quot;!&quot; character from the unreserved characters in IPv6 Zone ID parsing (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2899\">urllib3/urllib3#2899</a>)</li>\n<li>Fix IDNA handling of 'x80' byte (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2901\">urllib3/urllib3#2901</a>)</li>\n</ul>\n<h2>1.26.14</h2>\n<ul>\n<li>Fixed parsing of port 0 (zero) returning None, instead of 0 (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2850\">#2850</a>)</li>\n<li>Removed deprecated <code>HTTPResponse.getheaders()</code> calls in <code>urllib3.contrib</code> module.</li>\n</ul>\n<h2>1.26.13</h2>\n<ul>\n<li>Deprecated the <code>HTTPResponse.getheaders()</code> and <code>HTTPResponse.getheader()</code> methods.</li>\n<li>Fixed an issue where parsing a URL with leading zeroes in the port would be rejected even when the port number after removing the zeroes was valid.</li>\n<li>Fixed a deprecation warning when using cryptography v39.0.0.</li>\n<li>Removed the <code>&lt;4</code> in the <code>Requires-Python</code> packaging metadata field.</li>\n</ul>\n<h2>1.26.12</h2>\n<ul>\n<li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module. Both will be removed in v2.x. See this <a href=\"https://redirect.github.com/urllib3/urllib3/issues/2680\">GitHub issue</a> for justification and info on how to migrate.</li>\n</ul>\n<h2>1.26.11</h2>\n<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=\"https://github.com/sponsors/urllib3\">GitHub Sponsors</a>.</strong></p>\n<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=\"https://urllib3.readthedocs.io/en/latest/v2-roadmap.html\">Read more in the v2.0 Roadmap</a></p>\n<ul>\n<li>Fixed an issue where reading more than 2 GiB in a call to HTTPResponse.read would raise an OverflowError on Python 3.9 and earlier.</li>\n</ul>\n<h2>1.26.10</h2>\n<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=\"https://github.com/sponsors/urllib3\">GitHub Sponsors</a>.</strong></p>\n<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=\"https://urllib3.readthedocs.io/en/latest/v2-roadmap.html\">Read more in the v2.0 Roadmap</a></p>\n<p>:closed_lock_with_key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>\n<ul>\n<li>Removed support for Python 3.5</li>\n<li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>\n</ul>\n<h2>1.26.9</h2>\n<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=\"https://github.com/sponsors/urllib3\">GitHub Sponsors</a>.</strong></p>\n<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=\"https://urllib3.readthedocs.io/en/latest/v2-roadmap.html\">Read more in the v2.0 Roadmap</a></p>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/blob/main/CHANGES.rst\">urllib3's changelog</a>.</em></p>\n<blockquote>\n<h1>1.26.17 (2023-10-02)</h1>\n<ul>\n<li>Added the <code>Cookie</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>. (<code>[#3139](https://github.com/urllib3/urllib3/issues/3139) &lt;https://github.com/urllib3/urllib3/pull/3139&gt;</code>_)</li>\n</ul>\n<h1>1.26.16 (2023-05-23)</h1>\n<ul>\n<li>Fixed thread-safety issue where accessing a <code>PoolManager</code> with many distinct origins\nwould cause connection pools to be closed while requests are in progress (<code>[#2954](https://github.com/urllib3/urllib3/issues/2954) &lt;https://github.com/urllib3/urllib3/pull/2954&gt;</code>_)</li>\n</ul>\n<h1>1.26.15 (2023-03-10)</h1>\n<ul>\n<li>Fix socket timeout value when <code>HTTPConnection</code> is reused (<code>[#2645](https://github.com/urllib3/urllib3/issues/2645) &lt;https://github.com/urllib3/urllib3/issues/2645&gt;</code>__)</li>\n<li>Remove &quot;!&quot; character from the unreserved characters in IPv6 Zone ID parsing\n(<code>[#2899](https://github.com/urllib3/urllib3/issues/2899) &lt;https://github.com/urllib3/urllib3/issues/2899&gt;</code>__)</li>\n<li>Fix IDNA handling of '\\x80' byte (<code>[#2901](https://github.com/urllib3/urllib3/issues/2901) &lt;https://github.com/urllib3/urllib3/issues/2901&gt;</code>__)</li>\n</ul>\n<h1>1.26.14 (2023-01-11)</h1>\n<ul>\n<li>Fixed parsing of port 0 (zero) returning None, instead of 0. (<code>[#2850](https://github.com/urllib3/urllib3/issues/2850) &lt;https://github.com/urllib3/urllib3/issues/2850&gt;</code>__)</li>\n<li>Removed deprecated getheaders() calls in contrib module. Fixed the type hint of <code>PoolKey.key_retries</code> by adding <code>bool</code> to the union. (<code>[#2865](https://github.com/urllib3/urllib3/issues/2865) &lt;https://github.com/urllib3/urllib3/issues/2865&gt;</code>__)</li>\n</ul>\n<h1>1.26.13 (2022-11-23)</h1>\n<ul>\n<li>Deprecated the <code>HTTPResponse.getheaders()</code> and <code>HTTPResponse.getheader()</code> methods.</li>\n<li>Fixed an issue where parsing a URL with leading zeroes in the port would be rejected\neven when the port number after removing the zeroes was valid.</li>\n<li>Fixed a deprecation warning when using cryptography v39.0.0.</li>\n<li>Removed the <code>&lt;4</code> in the <code>Requires-Python</code> packaging metadata field.</li>\n</ul>\n<h1>1.26.12 (2022-08-22)</h1>\n<ul>\n<li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module.\nBoth will be removed in v2.x. See this <code>GitHub issue &lt;https://github.com/urllib3/urllib3/issues/2680&gt;</code>_\nfor justification and info on how to migrate.</li>\n</ul>\n<h1>1.26.11 (2022-07-25)</h1>\n<ul>\n<li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would\nraise an <code>OverflowError</code> on Python 3.9 and earlier.</li>\n</ul>\n<h1>1.26.10 (2022-07-07)</h1>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/c9016bf464751a02b7e46f8b86504f47d4238784\"><code>c9016bf</code></a> Release 1.26.17</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/01220354d389cd05474713f8c982d05c9b17aafb\"><code>0122035</code></a> Backport GHSA-v845-jxx5-vc9f (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3139\">#3139</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/e63989f97d206e839ab9170c8a76e3e097cc60e8\"><code>e63989f</code></a> Fix installing <code>brotli</code> extra on Python 2.7</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/2e7a24d08713a0131f0b3c7197889466d645cc49\"><code>2e7a24d</code></a> [1.26] Configure OS for RTD to fix building docs</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/57181d6ea910ac7cb2ff83345d9e5e0eb816a0d0\"><code>57181d6</code></a> [1.26] Improve error message when calling urllib3.request() (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3058\">#3058</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/3c0148048a523325819377b23fc67f8d46afc3aa\"><code>3c01480</code></a> [1.26] Run coverage even with failed jobs</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/d94029b7e2193ff47b627906a70e06377a09aae8\"><code>d94029b</code></a> Release 1.26.16</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/18e92145e9cddbabdf51c98f54202aa37fd5d4c8\"><code>18e9214</code></a> Use trusted publishing for PyPI</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/d25cf83bbae850a290fe34ed1610ae55c0558b36\"><code>d25cf83</code></a> [1.26] Fix invalid test_ssl_failure_midway_through_conn</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/25cca389496b86ee809c21e5b641aeaa74809263\"><code>25cca38</code></a> [1.26] Fix test_ssl_object_attributes</li>\n<li>Additional commits viewable in <a href=\"https://github.com/urllib3/urllib3/compare/1.26.5...1.26.17\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.5&new-version=1.26.17)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).\n\n</details>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-03T04:39:54Z",
        "closed_at": "2023-10-03T06:54:50Z",
        "merged_at": "2023-10-03T06:54:50Z",
        "body": "Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.26.5 to 1.26.17.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/releases\">urllib3's releases</a>.</em></p>\n<blockquote>\n<h2>1.26.17</h2>\n<ul>\n<li>Added the <code>Cookie</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>. (GHSA-v845-jxx5-vc9f)</li>\n</ul>\n<h2>1.26.16</h2>\n<ul>\n<li>Fixed thread-safety issue where accessing a <code>PoolManager</code> with many distinct origins would cause connection pools to be closed while requests are in progress (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2954\">#2954</a>)</li>\n</ul>\n<h2>1.26.15</h2>\n<ul>\n<li>Fix socket timeout value when HTTPConnection is reused (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2645\">urllib3/urllib3#2645</a>)</li>\n<li>Remove &quot;!&quot; character from the unreserved characters in IPv6 Zone ID parsing (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2899\">urllib3/urllib3#2899</a>)</li>\n<li>Fix IDNA handling of 'x80' byte (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2901\">urllib3/urllib3#2901</a>)</li>\n</ul>\n<h2>1.26.14</h2>\n<ul>\n<li>Fixed parsing of port 0 (zero) returning None, instead of 0 (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/2850\">#2850</a>)</li>\n<li>Removed deprecated <code>HTTPResponse.getheaders()</code> calls in <code>urllib3.contrib</code> module.</li>\n</ul>\n<h2>1.26.13</h2>\n<ul>\n<li>Deprecated the <code>HTTPResponse.getheaders()</code> and <code>HTTPResponse.getheader()</code> methods.</li>\n<li>Fixed an issue where parsing a URL with leading zeroes in the port would be rejected even when the port number after removing the zeroes was valid.</li>\n<li>Fixed a deprecation warning when using cryptography v39.0.0.</li>\n<li>Removed the <code>&lt;4</code> in the <code>Requires-Python</code> packaging metadata field.</li>\n</ul>\n<h2>1.26.12</h2>\n<ul>\n<li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module. Both will be removed in v2.x. See this <a href=\"https://redirect.github.com/urllib3/urllib3/issues/2680\">GitHub issue</a> for justification and info on how to migrate.</li>\n</ul>\n<h2>1.26.11</h2>\n<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=\"https://github.com/sponsors/urllib3\">GitHub Sponsors</a>.</strong></p>\n<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=\"https://urllib3.readthedocs.io/en/latest/v2-roadmap.html\">Read more in the v2.0 Roadmap</a></p>\n<ul>\n<li>Fixed an issue where reading more than 2 GiB in a call to HTTPResponse.read would raise an OverflowError on Python 3.9 and earlier.</li>\n</ul>\n<h2>1.26.10</h2>\n<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=\"https://github.com/sponsors/urllib3\">GitHub Sponsors</a>.</strong></p>\n<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=\"https://urllib3.readthedocs.io/en/latest/v2-roadmap.html\">Read more in the v2.0 Roadmap</a></p>\n<p>:closed_lock_with_key: <strong>This is the first release to be signed with Sigstore!</strong> You can verify the distributables using the <code>.sig</code> and <code>.crt</code> files included on this release.</p>\n<ul>\n<li>Removed support for Python 3.5</li>\n<li>Fixed an issue where a <code>ProxyError</code> recommending configuring the proxy as HTTP instead of HTTPS could appear even when an HTTPS proxy wasn't configured.</li>\n</ul>\n<h2>1.26.9</h2>\n<p><strong>If you or your organization rely on urllib3 consider supporting us via <a href=\"https://github.com/sponsors/urllib3\">GitHub Sponsors</a>.</strong></p>\n<p>:warning: <strong>urllib3 v2.0 will drop support for Python 2</strong>: <a href=\"https://urllib3.readthedocs.io/en/latest/v2-roadmap.html\">Read more in the v2.0 Roadmap</a></p>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/blob/main/CHANGES.rst\">urllib3's changelog</a>.</em></p>\n<blockquote>\n<h1>1.26.17 (2023-10-02)</h1>\n<ul>\n<li>Added the <code>Cookie</code> header to the list of headers to strip from requests when redirecting to a different host. As before, different headers can be set via <code>Retry.remove_headers_on_redirect</code>. (<code>[#3139](https://github.com/urllib3/urllib3/issues/3139) &lt;https://github.com/urllib3/urllib3/pull/3139&gt;</code>_)</li>\n</ul>\n<h1>1.26.16 (2023-05-23)</h1>\n<ul>\n<li>Fixed thread-safety issue where accessing a <code>PoolManager</code> with many distinct origins\nwould cause connection pools to be closed while requests are in progress (<code>[#2954](https://github.com/urllib3/urllib3/issues/2954) &lt;https://github.com/urllib3/urllib3/pull/2954&gt;</code>_)</li>\n</ul>\n<h1>1.26.15 (2023-03-10)</h1>\n<ul>\n<li>Fix socket timeout value when <code>HTTPConnection</code> is reused (<code>[#2645](https://github.com/urllib3/urllib3/issues/2645) &lt;https://github.com/urllib3/urllib3/issues/2645&gt;</code>__)</li>\n<li>Remove &quot;!&quot; character from the unreserved characters in IPv6 Zone ID parsing\n(<code>[#2899](https://github.com/urllib3/urllib3/issues/2899) &lt;https://github.com/urllib3/urllib3/issues/2899&gt;</code>__)</li>\n<li>Fix IDNA handling of '\\x80' byte (<code>[#2901](https://github.com/urllib3/urllib3/issues/2901) &lt;https://github.com/urllib3/urllib3/issues/2901&gt;</code>__)</li>\n</ul>\n<h1>1.26.14 (2023-01-11)</h1>\n<ul>\n<li>Fixed parsing of port 0 (zero) returning None, instead of 0. (<code>[#2850](https://github.com/urllib3/urllib3/issues/2850) &lt;https://github.com/urllib3/urllib3/issues/2850&gt;</code>__)</li>\n<li>Removed deprecated getheaders() calls in contrib module. Fixed the type hint of <code>PoolKey.key_retries</code> by adding <code>bool</code> to the union. (<code>[#2865](https://github.com/urllib3/urllib3/issues/2865) &lt;https://github.com/urllib3/urllib3/issues/2865&gt;</code>__)</li>\n</ul>\n<h1>1.26.13 (2022-11-23)</h1>\n<ul>\n<li>Deprecated the <code>HTTPResponse.getheaders()</code> and <code>HTTPResponse.getheader()</code> methods.</li>\n<li>Fixed an issue where parsing a URL with leading zeroes in the port would be rejected\neven when the port number after removing the zeroes was valid.</li>\n<li>Fixed a deprecation warning when using cryptography v39.0.0.</li>\n<li>Removed the <code>&lt;4</code> in the <code>Requires-Python</code> packaging metadata field.</li>\n</ul>\n<h1>1.26.12 (2022-08-22)</h1>\n<ul>\n<li>Deprecated the <code>urllib3[secure]</code> extra and the <code>urllib3.contrib.pyopenssl</code> module.\nBoth will be removed in v2.x. See this <code>GitHub issue &lt;https://github.com/urllib3/urllib3/issues/2680&gt;</code>_\nfor justification and info on how to migrate.</li>\n</ul>\n<h1>1.26.11 (2022-07-25)</h1>\n<ul>\n<li>Fixed an issue where reading more than 2 GiB in a call to <code>HTTPResponse.read</code> would\nraise an <code>OverflowError</code> on Python 3.9 and earlier.</li>\n</ul>\n<h1>1.26.10 (2022-07-07)</h1>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/c9016bf464751a02b7e46f8b86504f47d4238784\"><code>c9016bf</code></a> Release 1.26.17</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/01220354d389cd05474713f8c982d05c9b17aafb\"><code>0122035</code></a> Backport GHSA-v845-jxx5-vc9f (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3139\">#3139</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/e63989f97d206e839ab9170c8a76e3e097cc60e8\"><code>e63989f</code></a> Fix installing <code>brotli</code> extra on Python 2.7</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/2e7a24d08713a0131f0b3c7197889466d645cc49\"><code>2e7a24d</code></a> [1.26] Configure OS for RTD to fix building docs</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/57181d6ea910ac7cb2ff83345d9e5e0eb816a0d0\"><code>57181d6</code></a> [1.26] Improve error message when calling urllib3.request() (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3058\">#3058</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/3c0148048a523325819377b23fc67f8d46afc3aa\"><code>3c01480</code></a> [1.26] Run coverage even with failed jobs</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/d94029b7e2193ff47b627906a70e06377a09aae8\"><code>d94029b</code></a> Release 1.26.16</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/18e92145e9cddbabdf51c98f54202aa37fd5d4c8\"><code>18e9214</code></a> Use trusted publishing for PyPI</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/d25cf83bbae850a290fe34ed1610ae55c0558b36\"><code>d25cf83</code></a> [1.26] Fix invalid test_ssl_failure_midway_through_conn</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/25cca389496b86ee809c21e5b641aeaa74809263\"><code>25cca38</code></a> [1.26] Fix test_ssl_object_attributes</li>\n<li>Additional commits viewable in <a href=\"https://github.com/urllib3/urllib3/compare/1.26.5...1.26.17\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.26.5&new-version=1.26.17)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/huggingface/transformers/network/alerts).\n\n</details>",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 301,
        "deletions": 486,
        "changed_files": 3,
        "created_at": "2023-10-02T17:34:14Z",
        "closed_at": "2023-10-05T14:20:42Z",
        "merged_at": "2023-10-05T14:20:42Z",
        "body": "The `docs/index.md` file currently contains two auto-generated parts: the list of models (same as in README), and a table of models with supported frameworks. Due to the number of models available in transformers (200+), the list and the table have become quite large, and there have been internal discussions about removing the list of models from the `index.md`. \r\n\r\nThis PR adds the following changes: \r\n- removes the autogenerated model list from the `index.md` and updates the script so it's no longer added\r\n- modifies the script that generates the table to make model names links to corresponding model_doc. \r\n\r\nThe model lists in the main README and localized READMEs remain as is. ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-02T16:29:16Z",
        "closed_at": "2023-10-03T06:55:40Z",
        "merged_at": "2023-10-03T06:55:39Z",
        "body": "# What does this PR do?\r\n\r\nChange warning to info when adding a new text embedding. The reason is that this warning is trigger every time someone runs:\r\n\r\nwhich confuses users: https://github.com/huggingface/diffusers/issues/5212 .\r\n\r\nI don't think we should use `pad_to_multiple` here as it would mean that we add 8 new tokens every time we call `load_textual_inversion`\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-02T14:48:09Z",
        "closed_at": null,
        "merged_at": null,
        "body": "\u2026 or np.array filled with 0s or 1s, and do_rescale=False\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nThis PR fixes a bug that causes image processors like CLIPImageProcessor return NaN/Inf values when an input image is a float tensor/np.array filled with 0s or 1s, and `do_rescale=False`\r\n\r\nReproduction code:\r\n```\r\nfrom transformers import CLIPImageProcessor\r\nimport numpy as np\r\nprocessor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\r\nimage = np.random.randint(0,2,(3,3,3)).astype(np.float32)\r\nprint(processor(image, do_rescale=False))\r\n```\r\n\r\nWith such an input, `image_transform.resize` will return a uint8 image. Due to `do_rescale=False`, the input image for `image_transform.normalize` is also uint8, which makes `std=[0,0,0]` at https://github.com/huggingface/transformers/blob/1b8decb04c246ec8e1c4ba7f2749043d0876d24e/src/transformers/image_transforms.py#L391  \r\nThis leads to division-by-zero when the image is normalized\r\nThis PR fixes this bug simply by making `image_transform.resize` return a float image for this case\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nCould you please review this PR @amyeroberts?\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-02T13:25:06Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Using a tuple in this case is more optimal",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 46,
        "deletions": 27,
        "changed_files": 3,
        "created_at": "2023-10-02T12:48:57Z",
        "closed_at": "2023-10-03T10:23:47Z",
        "merged_at": "2023-10-03T10:23:47Z",
        "body": "# What does this PR do?\nFixes #26500, fixes #26536",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-02T12:09:51Z",
        "closed_at": "2023-10-02T12:59:25Z",
        "merged_at": "2023-10-02T12:59:25Z",
        "body": "# What does this PR do?\r\n\r\nProtect `adapter_kwargs` check in case it is excplicitly None\r\n\r\nAddresses: https://github.com/huggingface/transformers/pull/26488#issuecomment-1742821190\r\n\r\ncc @LysandreJik \r\n\r\nThe fix could be also to pop with an empty dict here: https://github.com/huggingface/transformers/blob/main/src/transformers/models/auto/auto_factory.py#L467",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 15,
        "changed_files": 2,
        "created_at": "2023-10-02T11:08:32Z",
        "closed_at": null,
        "merged_at": null,
        "body": "## What does this PR do?\r\nUpdating tests to (presumably) adjust for minor packages/models changes elsewhere. \r\nSee Issue #26533  for details.\r\n \r\n## Who can review?\r\n- generate: @gante\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\n## Testing\r\nI tested this PR locally with RUN_SLOW=1 and two GPUs.  Only ran quantization tests. \r\nThe generation tests now pass, nothing is broken (unless broken before, see the issue for the [remaining problem in model class mismatch](https://github.com/huggingface/transformers/issues/26533#issuecomment-1742764417)).",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 17,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-02T09:57:46Z",
        "closed_at": "2023-10-13T18:13:00Z",
        "merged_at": "2023-10-13T18:13:00Z",
        "body": "# What does this PR do?\r\n\r\nAdds resources of CLIP according to [this issue](https://github.com/huggingface/transformers/issues/20055.)\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nPart of #20055\r\n\r\n\r\n## Before submitting\r\n- [x ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@stevhliu, @jungnerd, @wonhyeongseo may you please review this PR?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-02T08:58:11Z",
        "closed_at": null,
        "merged_at": null,
        "body": "\u2026dded comment before model initialization line and Add configuration_[model_name].py to utils/documentation_tests.txt\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-02T08:07:51Z",
        "closed_at": "2023-10-02T15:19:13Z",
        "merged_at": "2023-10-02T15:19:13Z",
        "body": "Adds configuration_roformer.py to utils/documentation_tests.txt\r\n\r\nBased on https://github.com/huggingface/transformers/issues/19487\r\n\r\n@ydshieh Please have a look at this \ud83c\udf1e",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 576,
        "deletions": 531,
        "changed_files": 268,
        "created_at": "2023-10-02T07:47:22Z",
        "closed_at": "2023-10-02T07:58:06Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-02T07:06:18Z",
        "closed_at": "2023-10-12T08:48:38Z",
        "merged_at": "2023-10-12T08:48:38Z",
        "body": "This explicitely shows how to control `warnings` using the `logging` level setters in `transformers.\r\n\r\nFix https://github.com/huggingface/transformers/issues/26381",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2023-10-02T05:29:34Z",
        "closed_at": "2023-10-02T07:28:04Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-10-02T02:29:25Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR addresses the issue with the `JumanppTokenizer` that incorrectly handled half-width spaces by replacing them with full-width spaces. This problem originated from a bug in the dependent library, `rhoknp`. This PR resolves this bug by updating `rhoknp` to the latest version.\r\n\r\nBefore this PR:\r\n\r\n```python\r\n>>> tokenizer = JumanppTokenizer()\r\n>>> tokenizer.tokenize(\"foo\\u2009bar\")  # \\u2009: half-width space\r\n>>> [\"foo\", \"\\u3000\", \"bar\"]  # \\u3000: full-width space (bug)\r\n```\r\n\r\nWith this PR:\r\n\r\n```python\r\n# With this PR\r\n>>> tokenizer = JumanppTokenizer()\r\n>>> tokenizer.tokenize(\"foo\\u2009bar\")  # \\u2009: half-width space\r\n>>> [\"foo\", \"\\u2009\", \"bar\"]\r\n```\r\n\r\nThis PR also includes updates to the test cases for `JumanppTokenizer` to ensure its proper functionality.\r\n\r\nIt's important to note that this PR changes the behavior of `JumanppTokenizer`. Even though the change corrects a bug, some existing models might be built upon the earlier behavior and could be impacted.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3649,
        "deletions": 0,
        "changed_files": 30,
        "created_at": "2023-10-01T17:49:58Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR adds Google's new SigLIP model (CLIP with a better loss function). It's based on the [Google Colab](https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/SigLIP_demo.ipynb) provided by the authors.\r\n\r\ncc @patil-suraj feel free to take over this one\r\n\r\nTo do:\r\n\r\n- [ ] add SiglipTokenizer (or use `T5Tokenizer` ? The vocab is defined [here](https://github.com/google-research/big_vision/blob/53f18caf27a9419231bbf08d3388b07671616d3d/big_vision/pp/ops_text.py#L40-L41))\r\n- [ ] add tests for the image processor, tokenizer and processor\r\n- [ ] add loss function for training\r\n- [ ] important one: make sure that weights of `SiglipVisionModel` can be properly loaded without `from_pretrained` complaining",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 625,
        "deletions": 10,
        "changed_files": 3,
        "created_at": "2023-10-01T16:28:31Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nFixes #25778 \r\n\r\n# Review \r\n@gante @LysandreJik as discussed. Also, @oobabooga @ArthurZucker @jorgemcgomes feel free to look at this. \r\nThis is in large parts inspired by [rellm](https://github.com/r2d4/rellm/tree/main) and [parserllm](https://github.com/r2d4/parserllm) by @r2d4 so I am tagging him here too for visibility.\r\n\r\n# Discussion \r\nThis is totally WIP still. I added a notebook showcasing how it might work when using the generation API from a low-level perspective. Please educate me if that is ok that way. Long-term I want to add tests. \r\nGenerally, we use Lark to parse the grammar. The grammar itself would most likely come from a text file or a string but Lark also has a grammar object that one can use. I built a class [CfgParsingStepper](https://github.com/jvhoffbauer/transformers/blob/cfg_masking_logits_processor/src/transformers/generation/logits_process.py#L1721) that we can use to get the state of the parser for any input string. It will give us the terminal symbol we are processing and the regex for this symbol that we can use to find valid tokens. The [CfgLogitsProcessor](https://github.com/jvhoffbauer/transformers/blob/cfg_masking_logits_processor/src/transformers/generation/logits_process.py#L1774) gets the state every turn for every beam. We can consider persisting the state during generation which might be faster, instead of recalculating it. \r\n\r\nThe whole codebase is still very much WIP. Most importantly, we still need to \r\n\r\n- Add tests (instead of a Jupyter Notebook) \r\n- Add error handling to make sure the parser throws an error if it is started with an invalid input \r\n- Connect the logits processor to the actual generation APIs. I am thinking of something like `model.generate(..., grammar=Grammar(...))`\r\n- Think about cases where you want the grammar constraint to start only for the new generations and not for the input prompt\r\n\r\nAnyhow, I wanted to put this out there for discussion. Let me know if it makes sense this way and whether you would like me to continue working in this way or if I should change anything! ",
        "comments": 6
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-01T16:14:19Z",
        "closed_at": "2023-10-03T09:21:25Z",
        "merged_at": "2023-10-03T09:21:25Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #19487 \r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-01T13:33:22Z",
        "closed_at": "2023-10-02T08:52:51Z",
        "merged_at": "2023-10-02T08:52:51Z",
        "body": "# What does this PR do?\r\n\r\nFixes #26517\r\n\r\n## Who can review?\r\n\r\n @muellerzr @pacman100",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 8,
        "changed_files": 3,
        "created_at": "2023-10-01T12:34:01Z",
        "closed_at": "2023-10-04T15:17:07Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\nI change the data flow of prepare_dataset function, make a case to avoid remove `speech` columns\r\n\r\nWhile examining the 'wav2vec2' workflow, I noticed that the `prepare_dataset` function typically takes the path of audio files and converts them into audio arrays. However, I believe this approach may not be ideal for several reasons:\r\n- Not all data entries contain a `path` column, or the `path` column may not always be correctly populated (e.g., in the case of 'vivos' data). When attempting to use this code with such data, errors can occur.\r\n- This process is somewhat redundant, especially in cases like 'common voice' datasets, where we already have the audio data stored in the `audio` column. In these instances, it would be more efficient to directly pass the audio array to the `speech` column.\r\n\r\nTo address these issues, I've adjusted the data flow to accept the audio file path as an input column, ensuring that the sampling rate matches the feature extractor's requirements. Additionally, I've created a list of columns to exclude during data processing to prevent inadvertently removing the 'speech' column.\"\r\n\r\nI would like cc @sanchit-gandhi to review my PR.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 594,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-01T10:57:24Z",
        "closed_at": "2023-10-03T17:25:50Z",
        "merged_at": "2023-10-03T17:25:50Z",
        "body": "<!-- PR\uc758 \uc81c\ubaa9\uc740 \"\ud83c\udf10 [i18n-KO] Translated `<your_file>.md` to Korean\" \uc73c\ub85c \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4! -->\r\n# What does this PR do?\r\n\r\nTranslated the `semantic_segmentation.md` file of the documentation to Korean.\r\nThank you in advance for your review. \r\n\r\nPart of https://github.com/huggingface/transformers/issues/20179\r\n\r\n## Before reviewing\r\n- [x] Check for missing / redundant translations (\ubc88\uc5ed \ub204\ub77d/\uc911\ubcf5 \uac80\uc0ac)\r\n- [x] Grammar Check (\ub9de\ucda4\ubc95 \uac80\uc0ac)\r\n- [x] Review or Add new terms to glossary (\uc6a9\uc5b4 \ud655\uc778 \ubc0f \ucd94\uac00)\r\n- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)\r\n- [ ] Check live-preview for gotchas (live-preview\ub85c \uc815\uc0c1\uc791\ub3d9 \ud655\uc778)\r\n\r\n## Who can review? (Initial)\r\n\r\n<!-- 1. \uc704 \uccb4\ud06c\uac00 \ubaa8\ub450 \uc644\ub8cc\ub41c \ub4a4\uc5d0, \uc774 \uc544\ub798\uc5d0 \ub9ac\ubdf0\ub97c \uc694\uccad\ud560 \ud300\uc6d0\ub4e4\uc744 \uba58\uc158\ud574\uc8fc\uc138\uc694! -->\r\nTeam PseudoLab, may you please review this PR? @0525hhgus, @kihoon71, @sim-so, @gabrielwithappy, @HanNayeoniee, @wonhyeongseo\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review? (Final)\r\n\r\n<!-- 2. \ud300\uc6d0\ub4e4\uacfc \ub9ac\ubdf0\uac00 \ub05d\ub09c \ud6c4\uc5d0\ub9cc \ud5c8\uae45\ud398\uc774\uc2a4 \uc9c1\uc6d0\ub4e4\uc5d0\uac8c \ub9ac\ubdf0 \uc694\uccad\ud558\ub294 \uc544\ub798 \uc8fc\uc11d\uc744 \ub178\ucd9c\ud574\uc8fc\uc138\uc694! -->\r\n<!-- May you please review this PR? @sgugger, @ArthurZucker, @stevhliu -->",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-09-30T23:37:08Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26497\r\n\r\nTodo:\r\n- [ ] Approve the issue\r\n- [x] Implement\r\n- [ ] Test\r\n- [ ] Write the documentation\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-09-30T15:16:29Z",
        "closed_at": "2023-10-02T07:24:29Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFixes the auto-config Falcon tests that were failing on `main` after merging #26476. Note that the latest version of the Falcon model on the Hub specifies a model type of `falcon`, following @Rocketknight1's Hub commit [898df13](https://huggingface.co/tiiuae/falcon-7b/commit/898df1396f35e447d5fe44e0a3ccaaaa69f30d36). Thus, we update the config tests in `transformers` to reflect this.\r\n\r\nThere are still config tests to check that `trust_remote_code` gives the correct configs for older versions (with specific commit ids): https://github.com/huggingface/transformers/blob/0b192de1f353b0e04dad4813e02e2c672de077be/tests/models/falcon/test_modeling_falcon.py#L603",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-30T11:41:32Z",
        "closed_at": "2023-10-03T18:20:22Z",
        "merged_at": "2023-10-03T18:20:22Z",
        "body": "# What does this PR do?\r\nAdds a notebook resource of CLIP on: \r\n- How to fine-tune CLIP with Korean multimodal dataset\r\n- How to use CLIP for image-text similarity.\r\n\r\nThe notebook is created by our OSSCA community.\r\nPart of https://github.com/huggingface/transformers/issues/20055\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@bolizabeth, @nuatmochoi, @heuristicwave, @mjk0618, @keonju2, @harheem, @HongB1, @junejae, @54data, @seank021, @augustinLib, @sronger, @TaeYupNoh, @kj021, @eenzeenee, @stevhliu\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 47,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-09-30T03:51:03Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\nCurrently, when using `PeftModel`, `transformers` only save the weights of the adapter and support resuming training from these saved weights. However, if `deepspeed` is used on top of `PeftModel`, the entire model weights are saved. This behavior differs from that of `PeftModel`.\r\n\r\nThis PR integrates a newly added parameter `exclude_frozen_weights` from deepspeed to skip saving frozen weights if using peft.\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@pacman100\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 433,
        "deletions": 19,
        "changed_files": 3,
        "created_at": "2023-09-30T03:22:10Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\nAdds flash attention support for GPT2\r\n\r\n\r\n\r\nContribution to #26350 \r\n\r\n@younesbelkada \r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 51,
        "deletions": 53,
        "changed_files": 4,
        "created_at": "2023-09-29T23:55:06Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nRay 2.7 introduced some backwards-incompatible API changes, which broke the HuggingFace transformers integration with Ray Tune `trainer.hyperparameter_search(backend=\"ray\")`. This PR fixes the integration to use the new APIs. Note that this means that the next transformers version will no longer support `ray<2.7`.\r\n\r\nI have manually tested this with the example in the Ray repo here: https://github.com/ray-project/ray/pull/40125\r\nThis will be regularly tested in CI once this PR is in.\r\n\r\nhttps://github.com/ray-project/ray/issues/39763\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@richardliaw\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 805,
        "deletions": 1,
        "changed_files": 22,
        "created_at": "2023-09-29T18:44:11Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nThis PR implements the MAEST models (based on AST) as proposed in #26491.\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR. \r\n\r\nGiven that MAEST is intended for music applications and is audio-based, I consider that @sanchit-gandhi could be the appropriate member for having a look at it.\r\n\r\nThank you very much for considering this PR!\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 5,
        "created_at": "2023-09-29T16:12:12Z",
        "closed_at": "2023-10-03T08:52:34Z",
        "merged_at": "2023-10-03T08:52:34Z",
        "body": "# What does this PR do?\r\n\r\nWrapping modules in `nn.utils.parameterization.weight_norm` changes the weight signature vs vanilla `nn.utils.weight_norm`: https://github.com/huggingface/transformers/blob/1b8decb04c246ec8e1c4ba7f2749043d0876d24e/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L379-L381\r\n\r\nI've updated the tests to catch these new weight signatures, keeping the old ones in there for backwards comp\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-29T14:41:56Z",
        "closed_at": "2023-10-05T09:08:45Z",
        "merged_at": "2023-10-05T09:08:44Z",
        "body": "Currently, `@dataclass` `ModelOutput` instances can't be pickled, which can be inconvenient in some situations\r\nThis PR fixes this by adding a custom `__reduce__` method to the `ModelOutput` class\r\n\r\nOriginal PR from diffusers : https://github.com/huggingface/diffusers/pull/5234\r\n\r\n**EDIT**: Actual use case for me is passing a ModelOutput instance in a multiprocessing queue\r\n(this is needed if a model `__call__` is wrapped inside the ZeroGPU decorator : `model = spaces.GPU(model)`)",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2023-09-29T11:59:01Z",
        "closed_at": "2023-10-02T08:10:19Z",
        "merged_at": "2023-10-02T08:10:19Z",
        "body": "# What does this PR do?\r\n\r\nThe variable num_key_value_heads in the FlashAttention Module has falsely been named num_heads, which led to reshaping the query_layer using the wrong attention head count. (It would have been enough to use the correct variable self.num_heads instead of num_heads, but I renamed num_heads to num_key_value_heads for clarity)\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker and @younesbelkada\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 320,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-09-29T11:51:29Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nAdds Flash Attention 2 for `DistilBert` as discussed in in this issue - https://github.com/huggingface/transformers/issues/26350 .\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\ncc : @younesbelkada \r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-09-29T08:58:58Z",
        "closed_at": "2023-10-02T09:23:03Z",
        "merged_at": "2023-10-02T09:23:03Z",
        "body": "# What does this PR do?\r\n\r\nThis PR fixes an issue that was reported on Spaces. I was not able to reproduce the issue locally though. \r\nWhen loading a model with `token=True` (i.e. on a gated or private repository), `find_adapter_config` will try to look for an adapter file inside a private repository without the token, leading to an authentication error. \r\n\r\nThe fix is to pass the token to `adapter_kwargs` and remove the duplication here: https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L2529 \r\n\r\ncc @LysandreJik ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-29T08:52:07Z",
        "closed_at": "2023-10-02T09:19:12Z",
        "merged_at": "2023-10-02T09:19:12Z",
        "body": "# What does this PR do?\r\n\r\nIt fixes a broken link in the VideoMAE documentation.\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@stevhliu and @MKhalusova\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 269,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-09-29T08:50:20Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nAdds Flash Attention 2 for `GPT-Neo` as discussed in in this issue - https://github.com/huggingface/transformers/issues/26350 .\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n\r\ncc : @younesbelkada ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-09-29T08:30:52Z",
        "closed_at": "2023-09-29T08:52:18Z",
        "merged_at": "2023-09-29T08:52:18Z",
        "body": "# What does this PR do?\r\n\r\nSkip 2 Persimmon pipeline tests for now.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-09-29T07:31:05Z",
        "closed_at": "2023-10-04T12:29:11Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nBefore this PR we were performing a simple check `if module_name in key` but that lead to some modules silently converted in fp32. \r\n\r\nFor example instructblip models got their `word_embedding` layers converted in fp32 because `_keep_in_fp32_modules` includes `\"wo\"` which is contained in the string `word_embedding`. The fix is to check if `module_name in key.split(\".\")`\r\n\r\ncc @ydshieh \r\n\r\nRelated bnb and T5 test all pass\r\n\r\nNeed to investigate if instructblip tests pass",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 28,
        "changed_files": 4,
        "created_at": "2023-09-29T06:56:19Z",
        "closed_at": "2023-10-02T14:52:01Z",
        "merged_at": "2023-10-02T14:52:01Z",
        "body": "# What does this PR do?\nRemoves some useless warnings and make sure protobuf is not used when not needed",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 176,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-09-29T06:53:43Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nAdds Flash Attention 2 for Persimmon per  #26350 \r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo - see [commit](https://github.com/huggingface/transformers/commit/90d81d0379b9c6e0bc6643f2cd458fa5c7490fa3) \r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n\r\n## Who can review?\r\n@younesbelkada \r\n\r\nRan tests on A100 80G, see attached for venv.\r\n```\r\nRUN_SLOW=1 pytest -sv --disable-warnings -k flash_attn_2 tests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest\r\n================================================================================= test session starts ==================================================================================\r\nplatform linux -- Python 3.9.17, pytest-7.4.2, pluggy-1.3.0 -- /notebooks/virtualenvs/persimmon-fa2/bin/python\r\ncachedir: .pytest_cache\r\nconfigfile: setup.cfg\r\ncollecting ... Using /root/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...\r\nDetected CUDA files, patching ldflags\r\nEmitting ninja build file /root/.cache/torch_extensions/py39_cu118/cuda_kernel/build.ninja...\r\nBuilding extension module cuda_kernel...\r\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\r\nninja: no work to do.\r\nLoading extension module cuda_kernel...\r\nXformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\r\npip install xformers.\r\ncollected 116 items / 110 deselected / 6 selected                                                                                                                                      \r\n\r\ntests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_flash_attn_2_conversion You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nPASSED\r\ntests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_flash_attn_2_generate_left_padding You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nPASSED\r\ntests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_flash_attn_2_generate_padding_right You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nPASSED\r\ntests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_flash_attn_2_generate_use_cache You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nPASSED\r\ntests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_flash_attn_2_inference You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nPASSED\r\ntests/models/persimmon/test_modeling_persimmon.py::PersimmonModelTest::test_flash_attn_2_inference_padding_right You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nYou are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\r\nPASSED\r\n\r\n==================================================================== 6 passed, 110 deselected, 8 warnings in 5.62s =====================================================================\r\n```\r\n[requirements.txt](https://github.com/huggingface/transformers/files/12761751/requirements.txt)\r\n   ",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 1496,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-09-29T06:04:59Z",
        "closed_at": "2023-10-02T16:56:40Z",
        "merged_at": "2023-10-02T16:56:40Z",
        "body": "# What does this PR do?\r\nAnother continue of https://github.com/huggingface/transformers/issues/18564\r\nThis PR adds PR and testing translation for the contribute chapter in docs\r\n\r\nI will do another review on live docs, when its ready\r\n\r\npinging for docs\r\n@stevhliu and @MKhalusova\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-09-29T05:10:30Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nAllow passing keywords argument for the Learning rate scheduler, can be useful for Cosine with Restart and Polynomial LR scheduler (and for potential future lr schedulers) \r\n\r\nIf necessary I can add some check for the parameters to make sure that the user gets a more sensible error message when passing mismatched arguments, right now it assumes the user knows what they are doing \r\n\r\n\r\n- trainer: @muellerzr and @pacman100\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 289,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-09-28T20:36:30Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\nAdds Flash Attention 2 for `GPTBigCode (Starcoder)` as discussed in in this issue - https://github.com/huggingface/transformers/issues/26350 .\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\ncc : @younesbelkada \r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-09-28T16:50:22Z",
        "closed_at": "2023-09-29T07:42:21Z",
        "merged_at": "2023-09-29T07:42:21Z",
        "body": "Updates offline mode docs to include `local_files_only=True` in `from_pretrained`",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 4,
        "changed_files": 3,
        "created_at": "2023-09-28T16:15:20Z",
        "closed_at": "2023-09-29T07:43:39Z",
        "merged_at": "2023-09-29T07:43:39Z",
        "body": "# What does this PR do?\r\n\r\nThis PR addresses issues brought up in  #26280. \r\nIt adds a note about text generation parameters to the `TextGenerationPipeline` and `Text2TextGenerationPipeline` docs to improve navigation and discoverability. Also adds a note on stopping criteria to the `generation_strategies.md` guide.   \r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 68,
        "deletions": 40,
        "changed_files": 3,
        "created_at": "2023-09-28T15:51:59Z",
        "closed_at": "2023-09-29T17:32:38Z",
        "merged_at": "2023-09-29T17:32:38Z",
        "body": "# What does this PR do?\r\n\r\nFixes #26420 with three primary changes:\r\n1. Renaming of `generator` -> `transcriber` in the pipeline tutorial\r\n2. More descriptive error message when `ffmpeg_read` fails, in a move to guide the user towards fixing likely problems\r\n3. Update docs for asr pipe inputs\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 110,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2023-09-28T15:34:31Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n#26369 highlighted that the use of `forward_params` in the TTS pipeline was not clear enough. This PR enrich a bit the docstrings to correct this oversight.\r\n\r\nNote that following a discussion with @Vaibhavs10, @sanchit-gandhi and @Narsil in the same issue, I came to the conclusion that:\r\n- adding `max_new_tokens` would add confusion to the pipeline usage, since generative and non-generative models coexist\r\n- in the same manner, I believe that renaming `forward_params` to `generate_params` would be equally as confusing\r\n\r\nAs @Narsil [noted](https://github.com/huggingface/transformers/issues/26369#issuecomment-1736275171), an user that would like to use advanced parameter controls would probably use the classic transformers usage (tokenizer + model + etc).\r\n\r\nOf course, I'm open to discussion and modification of current behavior if this problem recurs in the future.\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26369\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n\r\n\r\n\r\n## Who can review?\r\nHey @osanseviero, @Narsil and @ArthurZucker ! Do you feel like this resolve the issue for now?",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 226,
        "changed_files": 5,
        "created_at": "2023-09-28T15:32:55Z",
        "closed_at": "2023-10-02T07:13:20Z",
        "merged_at": "2023-10-02T07:13:20Z",
        "body": "Reverts the Falcon exceptions ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 113,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-28T15:27:16Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAddition of Flash Attention 2 to Mosaic Pretrained Transformers (MPT)\r\n\r\nFixes #26470 \r\ns:\r\n\r\n<!-- text models: @ArthurZucker and @younesbelkada -->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-28T13:43:21Z",
        "closed_at": "2023-09-29T09:06:06Z",
        "merged_at": "2023-09-29T09:06:06Z",
        "body": "# What does this PR do?\r\n\r\nThe method `random_attention_mask` used in testing makes sure the last token is non-zero. However, this property will be changed if a causal mask is applied.\r\n\r\nThis causes some issues in CI, see issue reported \r\n\r\nhttps://github.com/pytorch/pytorch/issues/110213\r\n\r\nIn general, a sequence with all zero as attention mask is bad. Let's avoid testing with such case.\r\n\r\n(However, we probably need to do some processing in the modeling code - if torch decide this is undefined behavior and won't make change to have previous behavior). ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-28T11:17:57Z",
        "closed_at": "2023-10-04T12:57:41Z",
        "merged_at": "2023-10-04T12:57:41Z",
        "body": "# What does this PR do?\r\nThis PR adds a telemetry label to the wandb run making it possible to identify W&B usage from the trainer class.\r\n \r\nref: https://github.com/huggingface/transformers/pull/25590\r\n\r\n## Before submitting\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n\r\n## Who can review?\r\n@LysandreJik \r\n\r\n\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 435,
        "deletions": 5,
        "changed_files": 5,
        "created_at": "2023-09-28T11:04:46Z",
        "closed_at": "2023-10-03T11:44:46Z",
        "merged_at": "2023-10-03T11:44:46Z",
        "body": "# What does this PR do?\r\n\r\nAdds Flash Attention 2 for Mistral For Causal - we still need to discuss how to integrate it with local attention\r\n\r\ncc @ArthurZucker @LysandreJik \r\n\r\n```python\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"mistralai/Mistral-7B-v0.1\",\r\n    use_flash_attention_2=True,\r\n    torch_dtype=torch.float16,\r\n    low_cpu_mem_usage=True\r\n).to(0)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\r\n\r\ntext = \"Hello my name is\"\r\ninputs = tokenizer(text, return_tensors=\"pt\").to(0)\r\n\r\nout = model.generate(**inputs, max_new_tokens=4096, use_cache=True, do_sample=True)\r\nprint(tokenizer.batch_decode(out, skip_special_tokens=True))\r\n```",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 233,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-09-28T10:19:38Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAdds flash attention support for GPT-Neo-X\r\n\r\nFixes: https://github.com/huggingface/transformers/issues/26444\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2023-09-28T07:58:00Z",
        "closed_at": "2023-10-05T07:17:38Z",
        "merged_at": null,
        "body": "Hi\r\nI find the subject of audio quite intriguing when considering its future potential. It deserves greater attention and contributions. Currently, I am utilizing this code for my primary project rather than mere testing. I am interested in incorporating the \"num_proc\" feature to enhance the loading of datasets, especially since the data is so big, and I require faster processing capabilities.\r\nThank you for review my PR.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-09-27T23:06:01Z",
        "closed_at": "2023-09-28T17:49:40Z",
        "merged_at": "2023-09-28T17:49:40Z",
        "body": "# What does this PR do?\r\n\r\nTitle: Resolve In-Place Operation Error in ESM Embeddings\r\n\r\nDescription:\r\n\r\nThis pull request addresses a critical issue concerning in-place operations within the ESM embeddings module which was causing a `RuntimeError` during the backward pass when training with certain configurations. The error message being encountered was:\r\n\r\n```plaintext\r\nRuntimeError: Output 0 of MatMul4BitBackward is a view and is being modified inplace.\r\n```\r\n\r\nThe modifications in this pull request ensure that in-place operations are replaced with out-of-place operations to comply with the PyTorch autograd's requirement, which disallows in-place operations on tensor views to ensure correct gradient computation. \r\n\r\nChanges:\r\n1. Replaced in-place operations in `modeling_esm.py` with their out-of-place counterparts.\r\n2. Ensured that all tensors being operated upon are not views to avoid the aforementioned `RuntimeError`.\r\n\r\nThese changes have been tested and verified to resolve the error during the training phase, thus improving the robustness of the ESM module for a wider variety of training configurations.\r\n\r\nThis resolution is crucial for researchers and practitioners working with ESM models, ensuring smooth training and utilization of the models provided within the Transformers library.\r\n\r\nIt's important to note that while resolving the in-place operation error is crucial for correct functionality, the switch from in-place to out-of-place operations may have a slight impact on computational efficiency and memory usage. In-place operations are usually more memory-efficient as they don't require additional memory allocation for storing the results; they directly update the values in the original tensors. On the other hand, out-of-place operations create new tensors to store the results, which can increase the memory footprint of the model. \r\n\r\nHowever, due to the fact that the ESM-2 models are now compatible with QLoRA, this seems negligible and a good compromise. \r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-09-27T21:08:12Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nLoading pre-trained LayoutLMv2 & LayoutLMv3 models with, say `LayoutLMv3Model.from_pretrained(..., device_map=\"auto\")`, currently fails with error message: `ValueError: LayoutLMv3ForTokenClassification does not support `device_map='auto'`. To implement support, the modelclass needs to implement the '_no_split_modules' attribute.`\r\n\r\nThis PR just implements the `_no_split_modules` attribute to the LayoutLMv2 & LayoutLMv3 model classes as suggested by the error message.\r\n\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@NielsRogge, @ArthurZucker \r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-27T16:34:41Z",
        "closed_at": "2023-09-27T16:47:27Z",
        "merged_at": "2023-09-27T16:47:27Z",
        "body": "Fixes a failing doctest by putting the file in the `not_doctested.txt` file for now.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2627,
        "deletions": 0,
        "changed_files": 6,
        "created_at": "2023-09-27T16:18:59Z",
        "closed_at": "2023-09-27T17:48:17Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nAdded Image Processor Class for ProPainter\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)  - https://github.com/huggingface/transformers/issues/26360\r\n\r\n\r\n- [ ] Did you make sure to update the documentation with your changes? \r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@rafaelpadilla @LysandreJik \r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-27T15:08:37Z",
        "closed_at": "2023-09-28T08:00:16Z",
        "merged_at": "2023-09-28T08:00:16Z",
        "body": "# What does this PR do?\r\n\r\nWe should update the device accordingly. See the comments along the changes.\r\n\r\nSo far, running the model on GPU, then change model to CPU and running CPU inputs, the cached tensors (like `cos_cached`) will be on GPU and fail to run with CPU tensors.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1974,
        "deletions": 2,
        "changed_files": 25,
        "created_at": "2023-09-27T14:59:40Z",
        "closed_at": "2023-09-27T16:30:46Z",
        "merged_at": "2023-09-27T16:30:46Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\nSupport for Mistral 7B models \r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-27T14:41:30Z",
        "closed_at": "2023-09-27T15:37:19Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/418",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-27T13:50:20Z",
        "closed_at": "2023-09-27T14:40:19Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/418\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-27T13:12:41Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n![image](https://github.com/huggingface/transformers/assets/6141784/bddb088b-5bae-4bbb-bbb8-feff450c814e)\r\n\r\nbefore this PR: lm_head weights were initialized with variance of 1, and it output activations with variance ~= hidden_dim. this is a very high variance for logits, and resulted in initial cross-entropy loss of ~110, which is Very High.\r\n\r\nafter this PR: lm_head weights initialized with variance of reciprocal of hidden_dim. this outputs activations with variance ~= 1. this is results in initial cross-entropy loss of ~11, which is high, but in line with what we'd expect.\r\n\r\nFixes https://github.com/huggingface/transformers/issues/16749 (again)\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker @younesbelkada",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 350,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-27T12:57:55Z",
        "closed_at": null,
        "merged_at": null,
        "body": null,
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-27T12:32:10Z",
        "closed_at": "2023-09-27T13:41:40Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/417",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-27T11:56:16Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThe goal of this PR is to prevent the overwriting of all the labels including -100s by input_ids in the process of collating the batches with Transformers's DataCollatorForLanguageModeling.\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26357\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. https://github.com/huggingface/transformers/issues/26357 also https://github.com/oobabooga/text-generation-webui/issues/4031 and https://github.com/oobabooga/text-generation-webui/pull/4032\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests? (will add to https://github.com/huggingface/transformers/blob/main/tests/trainer/test_data_collator.py if it gains more traction)\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n\r\nLibrary:\r\n\r\n- trainer: @muellerzr and @pacman100\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 86,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2023-09-27T11:53:03Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do ?\r\nThis PR adds the possibility to choose exllamav2 kernels for GPTQ model. This PR follows the [integration](https://github.com/PanQiWei/AutoGPTQ/pull/349 ) of the kernels in auto-gptq and the [integration]((https://github.com/huggingface/optimum/pull/1419)) in optimum.\r\n\r\n- [ ]  Merge after the optimum PR is merged ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 25,
        "deletions": 25,
        "changed_files": 5,
        "created_at": "2023-09-27T10:16:12Z",
        "closed_at": "2023-09-27T17:25:53Z",
        "merged_at": "2023-09-27T17:25:53Z",
        "body": "# What does this PR do?\r\n\r\nI also updated the labels on runners to have `t4` and `daily-ci`, `push-ci` etc.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-27T09:20:29Z",
        "closed_at": "2023-09-27T12:55:00Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/414\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-27T09:14:30Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFix #25910\r\n\r\nCurrently, for `InstructBlip` which can combine/use different architecture as component, we have the following block\r\n\r\nhttps://github.com/huggingface/transformers/blob/abd253103478b21faafb7c9a6e7a1a7d1effe757/src/transformers/models/instructblip/modeling_instructblip.py#L1280-L1281\r\n\r\nin order to extent the class attribute `_keep_in_fp32_modules` of `InstructBlipPreTrainedModel`. This modification of class attribute causes some issue:\r\n- However, loading `Salesforce/instructblip-flan-t5-xl` changes `_keep_in_fp32_modules` from `[]` to `[\"wo\"]`. Then loading `Salesforce/instructblip-vicuna-7b` will keep `qformer.embeddings.word_embeddings.weight` in `fp32`.\r\n- If we don't load `flan-t5` but just `vicuna`, `qformer.embeddings.word_embeddings.weight` will be in `fp16`.\r\nand the weight values are slightly different -> outputs are different -> CI failure.\r\n\r\nModifying a class attribute is bad in general, and we have failing test. This CI tries to incorporate components' in `_keep_in_fp32_modules` without modifying the `_keep_in_fp32_modules` in the parent model class.\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-27T08:46:04Z",
        "closed_at": "2023-10-02T09:35:07Z",
        "merged_at": "2023-10-02T09:35:07Z",
        "body": "# What does this PR do?\r\n\r\nFixes a bug that was catched by the bnb test that is currently failing on the main branch\r\n\r\nLink to failing job: https://github.com/huggingface/transformers/actions/runs/6307307607/job/17123869223 \r\n\r\n(the bug is very niche)\r\n\r\nWhen loading a trust remote code model from a config loaded with a specific revision, one needs to pass that revision to the model class when calling `from_config`\r\n\r\nThe snippet below:\r\n\r\n<details><summary>Click to check the repro snippet</summary>\r\n\r\n```python\r\nfrom accelerate import init_empty_weights\r\n\r\nfrom transformers import AutoModelForCausalLM\r\n\r\nmodel_id = \"mosaicml/mpt-7b\"\r\nconfig = AutoConfig.from_pretrained(\r\n    model_id, trust_remote_code=True, revision=\"72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7\"\r\n)\r\nwith init_empty_weights():\r\n    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\r\n```\r\n\r\n</details>\r\n\r\nReturns:\r\n\r\n```bash\r\nE           ValueError: The model class you are passing has a `config_class` attribute that is not consistent with the config class you passed (model has <class 'transformers_modules.mosaicml.mpt-7b.0b57768f52b7775563f7cc78c4724e407b39593b.configuration_mpt.MPTConfig'> and you passed <class 'transformers_modules.mosaicml.mpt-7b.72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7.configuration_mpt.MPTConfig'>. Fix one of those so they match!\r\n```\r\n\r\nBecause for the case of `mosaicml/mpt-7b`, `AutoModelForCausalLM.from_config` will load the model class from the latest commit, which is `0b57768` in the case of that repository. \r\n\r\nPassing a specific revision to `from_config`: \r\n\r\n<details><summary>Click to check the repro snippet</summary>\r\n\r\n```python\r\nfrom accelerate import init_empty_weights\r\n\r\nfrom transformers import AutoModelForCausalLM\r\n\r\nmodel_id = \"mosaicml/mpt-7b\"\r\nconfig = AutoConfig.from_pretrained(\r\n    model_id, trust_remote_code=True, revision=\"72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7\"\r\n)\r\nwith init_empty_weights():\r\n    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True, , revision=\"72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7\")\r\n```\r\n\r\n</details>\r\n\r\nGives:\r\n\r\n```bash\r\n    def _from_config(cls, config, **kwargs):\r\n        \"\"\"\r\n        All context managers that the model should be initialized under go here.\r\n    \r\n        Args:\r\n            torch_dtype (`torch.dtype`, *optional*):\r\n                Override the default `torch.dtype` and load the model under this dtype.\r\n        \"\"\"\r\n        torch_dtype = kwargs.pop(\"torch_dtype\", None)\r\n    \r\n        # override default dtype if needed\r\n        dtype_orig = None\r\n        if torch_dtype is not None:\r\n            dtype_orig = cls._set_default_torch_dtype(torch_dtype)\r\n    \r\n        if is_deepspeed_zero3_enabled():\r\n            import deepspeed\r\n    \r\n            logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\r\n            # this immediately partitions the model across all gpus, to avoid the overhead in time\r\n            # and memory copying it on CPU or each GPU first\r\n            with deepspeed.zero.Init(config_dict_or_path=deepspeed_config()):\r\n                model = cls(config, **kwargs)\r\n        else:\r\n>           model = cls(config, **kwargs)\r\nE           TypeError: __init__() got an unexpected keyword argument 'revision'\r\n```\r\n\r\nLooking deeper into the code, it seems the argument `code_revision` is popped, I am unsure whether we should use that argument or `revision`. I propose to have an api that is consistent with `revision` behaviour of `from_pretrained` but I am also happy to revert that and simply pass `code_revision` to the test\r\n\r\ncc @ArthurZucker @LysandreJik ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2023-09-27T05:09:34Z",
        "closed_at": "2023-10-02T05:17:50Z",
        "merged_at": null,
        "body": "This should fix the issue #26381 ",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 18,
        "changed_files": 6,
        "created_at": "2023-09-26T19:16:45Z",
        "closed_at": null,
        "merged_at": null,
        "body": "\r\n# What does this PR do?\r\nreplaces warnings.warn with logging.warning..\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26381\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n --@osanseviero\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-09-26T16:57:32Z",
        "closed_at": "2023-09-26T17:27:11Z",
        "merged_at": "2023-09-26T17:27:11Z",
        "body": "# What does this PR do?\r\n\r\nAdd torch `RMSProp` for easy use in TRL library, particularly to match the default use case of Direct Preference Optimization.\r\nScript is [here](https://github.com/huggingface/trl/blob/main/examples/dpo.py).\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n@younesbelkada and I discussed this.\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-26T15:26:43Z",
        "closed_at": "2023-09-26T22:09:16Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/404\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 42,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-09-26T15:23:14Z",
        "closed_at": "2023-09-28T13:08:36Z",
        "merged_at": "2023-09-28T13:08:36Z",
        "body": "# What does this PR do ? \r\nFixes #26266. This PR fixes the tied weights for mbart model. Before this PR, only `lm_head` was tied to `model.shared`. Now, we also make sure to tie `model.encoder.embed_tokens` and `model.decoder.embed_tokens` to `model.shared` by defining the `_tie_weights` method which will be called when we do `model.tie_weights()`. I've checked that we get the same weights at the end. This issue only happens when we load with `safetensors` + `device_map` because we don't save the shared tensors and the weights are on the meta device. ",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-09-26T15:17:06Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR fixed two problems when using padding mask in `LlamaAttention`. In detail,\r\n- Replace the `bfloat.min` with `float(\"-inf\")` as the default value for the mask. \r\n- Transpose the non-casual mask.\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n\r\n## Before submitting\r\n- [] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-26T14:07:57Z",
        "closed_at": "2023-09-27T09:51:44Z",
        "merged_at": "2023-09-27T09:51:44Z",
        "body": "fixed typo",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 18,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-09-26T11:56:16Z",
        "closed_at": "2023-10-17T13:59:35Z",
        "merged_at": "2023-10-17T13:59:35Z",
        "body": "# What does this PR do?\r\n\r\nFollowing the discussion in #26401, this adds a warning when using `_generate_speech` without speaker embeddings. \r\nIf necessary, and after discussion, we could throw an error instead of a warning!\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26401\r\n\r\n\r\n\r\n## Who can review?\r\n\r\ncc @Vaibhavs10 @sanchit-gandhi and @ArthurZucker \r\n",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 29,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-26T11:25:41Z",
        "closed_at": "2023-09-27T10:21:54Z",
        "merged_at": "2023-09-27T10:21:54Z",
        "body": "# What does this PR do?\r\n\r\nWhile testing out https://github.com/huggingface/transformers/pull/26414 I realised the current tests silently pass as we only check for the immediate next predicted token  \r\nFor small models FA2 is quite flaky so I have decided to not increase `max_new_tokens` in `test_flash_attn_2_generate_padding_right` and `test_flash_attn_2_generate_left_padding` and rather have a separate test that will run `generate` with `use_cache` with a relatively large `max_new_tokens` to catch issues with caching when porting models to FA\r\n\r\ncc @LysandreJik \r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 252,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2023-09-26T10:40:34Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nAdds Flash Attention 2 for `OPT` as discussed in in this issue - #26350 .\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\ncc : @younesbelkada",
        "comments": 23
    },
    {
        "merged": false,
        "additions": 212,
        "deletions": 70,
        "changed_files": 3,
        "created_at": "2023-09-26T10:22:05Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nTo be potentially merged after https://github.com/huggingface/transformers/pull/26407 \r\nThis PR adds the multi-adapter support for `save_pretrained` to be consistent with PEFT API that saves all adapters when calling `save_pretrained`. Note the default adapter is always saved in the root directory of `save_directory`.\r\n\r\ncc @LysandreJik @BenjaminBossan @pacman100 \r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 76,
        "deletions": 11,
        "changed_files": 3,
        "created_at": "2023-09-26T08:03:28Z",
        "closed_at": "2023-09-27T14:45:31Z",
        "merged_at": "2023-09-27T14:45:31Z",
        "body": "# What does this PR do?\r\n\r\nWith https://github.com/huggingface/peft/pull/905 being merged, it added the support for multi-adapter inference (combining multiple adapters at inference). \r\n\r\nThe PR has introduced some changes that are not compatible anymore with peft integration of transformers, for example `active_adapter` is now a property method and returns a list. This PR adds the corresponding fixes to make everything compatible with the newest features of PEFT, while preserving backward compatiblity\r\n\r\ncc @BenjaminBossan @pacman100 ",
        "comments": 9
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-09-26T07:58:37Z",
        "closed_at": "2023-09-28T08:15:48Z",
        "merged_at": "2023-09-28T08:15:48Z",
        "body": "The arguments to Llama2 is `input_ids` and `inputs_embeds` but yet in several places it is mentioned as `decoder_input_ids` and `decoder_inputs_embeds` including in some error messages. This PR fixes that.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 138,
        "deletions": 85,
        "changed_files": 59,
        "created_at": "2023-09-26T07:48:33Z",
        "closed_at": "2023-09-26T07:50:08Z",
        "merged_at": null,
        "body": "Change:\r\n```python \r\nif input_ids is not None and inputs_embeds is not None:\r\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\r\n```        \r\nto\r\n```python \r\nif input_ids is not None and inputs_embeds is not None:\r\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\r\n```        \r\nSimilarly change:\r\n```python\r\nelse:\r\n    raise ValueError(\"You have to specify either python_input_ids or python_inputs_embeds\")\r\n```\r\nto \r\n\r\n```python\r\nelse:\r\n    raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\r\n```",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 17,
        "changed_files": 2,
        "created_at": "2023-09-26T07:04:34Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Hi @ArthurZucker @younesbelkada \r\n\r\nI found that the shape of `past_key_values` in `gpt_bigcode` is different from others, which will cause inconvenience or unexpected errors in `Optimum` and `Optimum-intel`. Therefore, I added some reshaping operations to make the shape insistent with other models. I have tested it on my device and didn't find any performance decay. Would you please help me review it? Thx!\r\n\r\nBTW, could we add a restriction like the shape of `past_key_values` must be (batch_size, num_heads, sequence_length, head_dim)? It could avoid many issues if we have a fixed output and input shape of `past_key_values`",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 28,
        "changed_files": 3,
        "created_at": "2023-09-26T05:34:45Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# Motivation\r\nI faced below errors when run https://github.com/huggingface/transformers/blob/b880508440f43f80e35a78ccd2a32f3bde91cb23/src/transformers/models/marian/convert_marian_to_pytorch.py\r\nRuntimeError: Error(s) in loading state_dict for MarianMTModel:\r\n**size mismatch** for final_logits_bias: copying a param with shape **torch.Size([1, 52])** from checkpoint, the shape in current model is **torch.Size([1, 36]).**\r\nsize mismatch for model.decoder.embed_tokens.weight: copying a param with shape **torch.Size([52, 256])** from checkpoint, the shape in current model is torch.Size(**[36, 256]).**\r\nYou may consider adding ignore_mismatched_sizes=True in the model from_pretrained method.\r\n\r\n# Root cause\r\n\r\nfor my C++ marian model, source vocab has 36 words. the target vocab has 52 words. Current transfomer marian model code treats the source vocab same as the target vocab. That's why the errors said copying a param with shape torch.Size([52, 256]) from checkpoint, the shape in current model is torch.Size([36, 256]).\r\n\r\nI saw the comment in the marianMTModel, there is an assumption for the model, that the source and target vocab should be shared. But the assumption is not suitable for all marian models. at least the model I trained is not shared. (I can not retrain the model from scratch due to some reason)\r\n\r\n# What does this PR do?\r\n\u00a0remove the assumption: source and target vocab should be shared for Marian model\r\n\r\n## Was this discussed/approved via a Github issue?\r\nhttps://github.com/huggingface/transformers/issues/26338\r\nhttps://github.com/huggingface/transformers/issues/15109\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ArthurZucker @younesbelkada @gante",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2023-09-26T05:09:22Z",
        "closed_at": "2023-09-27T04:59:22Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-09-25T21:11:47Z",
        "closed_at": "2023-09-27T08:56:07Z",
        "merged_at": "2023-09-27T08:56:07Z",
        "body": "# What does this PR do?\r\n\r\nFixes padding issues - \r\n```python\r\nfrom transformers import AutoProcessor\r\n\r\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics-9b\", padding_side=\"right\")\r\n\r\nsents = [[\"hello world\"], [\" this is a longer sentence to testing padding\"]]\r\n\r\n# This is the correct behaviour:\r\na = processor(sents, padding=\"max_length\", truncation=True, max_length=20)\r\nprint(processor.tokenizer.decode(a['input_ids'][0]))\r\n# => <s> hello world<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\r\n\r\n# This is the incorrect behaviour:\r\nb = processor(sents, padding=\"longest\", truncation=True, max_length=30)\r\n\r\nprint(processor.tokenizer.decode(b['input_ids'][0]))\r\n# => <unk><unk><unk><unk><unk><unk><s> hello world\r\n```\r\nWith this fix the results would be \r\n```\r\n# for max_length\r\n<s> hello world<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\r\n# for longest\r\n<s> hello world<unk><unk><unk><unk><unk><unk>\r\n```\r\n\r\nFixes #26354\r\n\r\n\r\n## Before submitting\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n\r\n## Who can review?\r\n@VictorSanh @ArthurZucker @LysandreJik \r\n\r\n*let me know if there's anything that needs follow-ups*",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-25T21:03:51Z",
        "closed_at": "2023-09-26T15:19:19Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/408\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-09-25T19:48:18Z",
        "closed_at": "2023-09-26T08:11:28Z",
        "merged_at": "2023-09-26T08:11:28Z",
        "body": "# What does this PR do?\r\n\r\nDeleted duplicate sentence\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-25T17:13:59Z",
        "closed_at": "2023-09-26T08:19:00Z",
        "merged_at": "2023-09-26T08:19:00Z",
        "body": "# What does this PR do?\r\n\r\nThis PR fixes a bug when trying to do inference with Idefics with DeepSpeed Zero-3.\r\nThe model was loaded correctly, but the shapes were found to be incorrect using DS at a step in `IdeficsDecoupledLinear` during inference.\r\n\r\nI checked on different prompts that the output is the same without using DeepSpeed.\r\n\r\n## Who can review?\r\n\r\n@pacman100\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 518,
        "deletions": 518,
        "changed_files": 250,
        "created_at": "2023-09-25T16:53:00Z",
        "closed_at": "2023-09-25T17:48:07Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4254,
        "deletions": 1,
        "changed_files": 13,
        "created_at": "2023-09-25T16:49:17Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAdds ProPainter to transformers.\r\nAuthor - https://github.com/sczhou/ProPainter\r\nhub - https://huggingface.co/shauray/ProPainter-hf/\r\n\r\nFixes https://github.com/huggingface/transformers/issues/26360\r\n*issue contains all the other relevant links*\r\n\r\n## Before submitting\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review?\r\n@rafaelpadilla @LysandreJik \r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1479,
        "deletions": 1479,
        "changed_files": 586,
        "created_at": "2023-09-25T16:31:00Z",
        "closed_at": "2023-09-25T16:38:23Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 521,
        "deletions": 521,
        "changed_files": 252,
        "created_at": "2023-09-25T16:03:56Z",
        "closed_at": "2023-09-25T16:29:13Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-09-25T15:40:45Z",
        "closed_at": "2023-09-26T13:18:08Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nCurrently there seems to be an issue when training with deepspeed zero stage 3 and resizing the token embeddings. Here is an example I used:\r\n\r\n```python\r\nfrom transformers import (\r\n    AutoConfig,\r\n    AutoTokenizer,\r\n    AutoModelForCausalLM,\r\n    HfArgumentParser,\r\n    TrainingArguments,\r\n    DataCollatorForSeq2Seq,\r\n    Trainer,\r\n)\r\n\r\ndef main():\r\n    parser = HfArgumentParser((TrainingArguments))\r\n    training_args, = parser.parse_args_into_dataclasses()\r\n\r\n    model_path = 'facebook/opt-125m'\r\n\r\n    config = AutoConfig.from_pretrained(model_path)\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n        model_path,\r\n        config = config,\r\n    )\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\n        model_path,\r\n        model_max_length=1024,\r\n    )\r\n\r\n    add_new_tokens = True\r\n    if add_new_tokens:\r\n        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",})\r\n        model.resize_token_embeddings(len(tokenizer))\r\n    else:\r\n        tokenizer.pad_token = tokenizer.eos_token\r\n\r\n    from datasets import Dataset\r\n    def gen():\r\n        for _ in range(100):\r\n            yield {\"input_ids\": [1, 2, 3], \"labels\": [1, 1, 1]}\r\n    datasets = Dataset.from_generator(gen)\r\n    datasets.set_format('pt')\r\n\r\n    trainer = Trainer(\r\n        model=model,\r\n        args=training_args,\r\n        tokenizer=tokenizer,\r\n        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, max_length=tokenizer.model_max_length),\r\n        train_dataset=datasets,\r\n    )\r\n    trainer.train()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nExecuted on at least 2 GPUs with: `deepspeed minimal.py --output_dir output_dir --deepspeed deepspeed_config.json`\r\n\r\nwhere deepspeed_config.json is:\r\n\r\n```json\r\n{\r\n  \"bf16\": {\r\n    \"enabled\": \"auto\"\r\n  },\r\n  \"optimizer\": {\r\n    \"type\": \"AdamW\",\r\n    \"params\": {\r\n      \"lr\": \"auto\",\r\n      \"betas\": \"auto\",\r\n      \"eps\": \"auto\",\r\n      \"weight_decay\": \"auto\"\r\n    }\r\n  },\r\n  \"scheduler\": {\r\n    \"type\": \"WarmupDecayLR\",\r\n    \"params\": {\r\n      \"warmup_min_lr\": \"auto\",\r\n      \"warmup_max_lr\": \"auto\",\r\n      \"warmup_num_steps\": \"auto\",\r\n      \"warmup_type\": \"linear\",\r\n      \"total_num_steps\": \"auto\"\r\n    }\r\n  },\r\n  \"zero_optimization\": {\r\n    \"stage\": 3,\r\n    \"overlap_comm\": true,\r\n    \"contiguous_gradients\": true,\r\n    \"sub_group_size\": 1e9,\r\n    \"reduce_bucket_size\": \"auto\",\r\n    \"stage3_prefetch_bucket_size\": \"auto\",\r\n    \"stage3_param_persistence_threshold\": \"auto\",\r\n    \"stage3_max_live_parameters\": 1e9,\r\n    \"stage3_max_reuse_distance\": 1e9,\r\n    \"stage3_gather_16bit_weights_on_model_save\": true\r\n  },\r\n  \"gradient_accumulation_steps\": \"auto\",\r\n  \"gradient_clipping\": \"auto\",\r\n  \"steps_per_print\": 2000,\r\n  \"train_batch_size\": \"auto\",\r\n  \"train_micro_batch_size_per_gpu\": \"auto\",\r\n  \"wall_clock_breakdown\": false\r\n}\r\n```\r\n\r\n\r\n\r\nCurrently the `vocab_size` and `config.vocab_size` are set to 0, when using deepspeed zero in this line https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1537-L1538 . This fails later during training with `RuntimeError: shape '[-1, 0]' is invalid for input of size 804240` when trying to flatten the logits using `vocab_size` shape. Instead it should use approach similar to https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1555-L1559 to set the correct size.\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker @pacman100 \r\n\r\n\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-09-25T14:22:07Z",
        "closed_at": "2023-09-26T08:15:53Z",
        "merged_at": "2023-09-26T08:15:53Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #24602\r\n\r\nAdds gradient checkpointing support for ESM models\r\n\r\n## Before submitting\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n    Issue link: https://github.com/huggingface/transformers/issues/24602\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@Rocketknight1 @amyeroberts : Please review and suggest any required changes, thanks.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 43,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2023-09-25T14:06:07Z",
        "closed_at": "2023-10-13T09:03:15Z",
        "merged_at": "2023-10-13T09:03:14Z",
        "body": "# What does this PR do?\r\n\r\nThis is a continuation of #25715 where @sgugger fix `test_cached_model_has_minimum_calls_to_head` but not `test_cached_pipeline_has_minimum_calls_to_head` (as I didn't mention this to him).\r\n\r\nThis PR mostly copies the logic from #25715.\r\n\r\nThe current failing test and the error are\r\n\r\n```bash\r\ntests/pipelines/test_pipelines_common.py::CustomPipelineTest::test_cached_pipeline_has_minimum_calls_to_head\r\n(line 905)  AssertionError: 2 != 1\r\n```\r\n\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1088,
        "deletions": 10,
        "changed_files": 22,
        "created_at": "2023-09-25T08:40:57Z",
        "closed_at": "2023-10-08T09:50:50Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR adds the OWLv2 checkpoints, and investigates why results weren't the same as the original Colab for v1 as pointed out in #21206. After investigation, it turns out that 1) the model returns the exact same logits as the original implementation on the same input data, but 2) the image preprocessing wasn't done in exactly the same way as the original Scenic repo, which involves padding, causing a difference. This PR fixes that by introducing a new `Owlv2ImageProcessor` which people can use to get equivalent results. \r\n\r\nFixes #26315 #21206\r\n\r\nQuestion:\r\n- the new OWLv2 models have an extra objectness head, which returns `objectness_logits` => we can either add an entirely new `Owlv2ForObjectDetection` which copies 99% of `OwlViTForObjectDetection`, or do it like it's currently implemented (by adding a `config.add_objectness_head` attribute). Happy to read your opinions!\r\n\r\nTo do:\r\n\r\n- [ ] update layer norm eps of LayerNorms, which is 1e-5 in PyTorch but 1e-6 in Flax by default",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 28,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2023-09-25T03:25:49Z",
        "closed_at": "2023-10-10T04:45:24Z",
        "merged_at": "2023-10-10T04:45:24Z",
        "body": "Hi @ArthurZucker \r\n\r\nRelate to [25856](https://github.com/huggingface/transformers/pull/25856). I added a new parameter in config to control the stride for the first bottleneck layer in stages. Would you please help me review it? Thx!",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-24T22:50:38Z",
        "closed_at": "2023-09-25T10:14:31Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/404\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-24T15:33:31Z",
        "closed_at": "2023-09-24T17:29:41Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/396\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-09-24T14:11:55Z",
        "closed_at": "2023-09-25T11:08:26Z",
        "merged_at": "2023-09-25T11:08:26Z",
        "body": "# What does this PR do?\r\n\r\nIssue: #26369 \r\n\r\nLoading\r\n\r\n```python\r\npipe = pipeline(\"text-to-audio\", model=\"facebook/musicgen-small\")\r\ndata = pipe(\"latin salsa with violins\")\r\n```\r\n\r\nat the moment, throws a large error\r\n\r\n\r\n## Who can review?\r\n\r\n@ylacombe @sanchit-gandhi \r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-24T12:44:06Z",
        "closed_at": "2023-09-25T16:59:18Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nWhile reviewing the \"run_mlm_flax.py\" file, I encountered an issue when attempting to execute the \"model.enable_gradient_checkpointing()\" function. Upon further investigation, I observed that there was no \"enable_gradient_checkpointing\" available. As a solution, I configured FlaxRobertaForMasked to include the \"enable_gradient_checkpoint\" feature.\r\n\r\nI would like to cc @sanchit-gandhi to review my code",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-24T12:25:47Z",
        "closed_at": "2023-09-24T15:13:52Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/401\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 20,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2023-09-24T08:22:07Z",
        "closed_at": "2023-09-25T10:58:12Z",
        "merged_at": "2023-09-25T10:58:12Z",
        "body": "fixed typos\r\n\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 179,
        "deletions": 41,
        "changed_files": 2,
        "created_at": "2023-09-24T08:21:41Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2023-09-24T05:48:32Z",
        "closed_at": "2023-09-24T06:13:58Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nIn the sample code for running a language model in the Flax framework, I believe that \"unshuffled_deduplicated_no\" is not a suitable example due to its size, which consists of 3.18M samples. Loading such a large dataset can be time-consuming, especially for beginners. Therefore, I suggest changing the dataset to something smaller, like 1M samples, for testing purposes. As a result, I have modified the data source to \"unshuffled_deduplicated_vi\"\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 764,
        "deletions": 0,
        "changed_files": 10,
        "created_at": "2023-09-23T16:47:44Z",
        "closed_at": null,
        "merged_at": null,
        "body": "### Model description\r\n\r\nI would like to contribute AugViT Tensorflow implementation to the Transformers library. You can find the implementation in TensorFlow here https://github.com/ushareng/AugViT\r\n\r\nI have created Model card here https://huggingface.co/tensorgirl/TFaugvit/tree/main\r\n\r\nKindly let me know if the above is correct.\r\n\r\n### Open source status\r\n\r\n- [X] The model implementation is available\r\n- [x] The model weights are available\r\n\r\n### Provide useful links for the implementation\r\n\r\nTensorFlow implementation of AugViT https://github.com/ushareng/AugViT",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 47,
        "deletions": 13,
        "changed_files": 1,
        "created_at": "2023-09-22T21:36:17Z",
        "closed_at": null,
        "merged_at": null,
        "body": "A problem with timm to Vit converter code was identified in #26219 \r\n\r\nThis is caused by a rather strict index based parsing of the timm model identifier which fails for several cases. This PR does two things:\r\n\r\n1. Fixes that issue by using a regex parser for patch and image size. \r\n2. Fixes ViT configuration differences (ViT Small, missing Vit-Tiny, Vit-giant/gigantic). \r\n\r\n\r\n\r\n @amyeroberts, @ArthurZucker\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-09-22T20:07:04Z",
        "closed_at": "2023-09-22T21:42:18Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nWhen running the ./downloads.sh the options available to download are (\"7B,13B,70B,7B-chat,13B-chat,70B-chat\"). The download.sh script creates the folder in the following format based on the model chosen ('llama-2' + model_size.lower())\r\n\r\nHugging face script does not support the above directory naming convention. Resulting in having to rename the folders required by hugging face options (\"7B\", \"7Bf\", \"13B\", \"13Bf\", \"30B\", \"34B\", \"65B\", \"70B\", \"70Bf\", \"tokenizer_only\"). \r\n\r\nAlso, the existing options does not support 7B-chat, 13B-chat and 70B-chat. \r\n\r\nThis scripts adds support to the following\r\n\r\n1. llama-chat models\r\n2. New directory naming format\r\n\r\n## Before submitting\r\n- [] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [X] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@chauhang @HamidShojanazeri",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-09-22T15:14:28Z",
        "closed_at": "2023-10-04T16:12:10Z",
        "merged_at": "2023-10-04T16:12:10Z",
        "body": "# What does this PR do?\r\n\r\nThe PR #23909 removed `unique_no_split_tokens` as an attribute of the Wav2Vec2 tokenizer, however it is required to set the language in the method `.set_target_lang`:\r\n\r\nhttps://github.com/huggingface/transformers/blob/dcbfd93d7aeb14f8ff08a48866d2a68950d4c69a/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L237\r\n\r\nThus, calling `.set_target_lang` currently throws an error:\r\n\r\n```python\r\nfrom transformers import Wav2Vec2CTCTokenizer\r\n\r\ntokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/mms-1b-all\")\r\ntokenizer.set_target_lang(\"spa\")\r\n```\r\n\r\n**Output:**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/sanchitgandhi/transformers/debug_tokenizer.py\", line 4, in <module>\r\n    tokenizer.set_target_lang(\"spa\")\r\n  File \"/Users/sanchitgandhi/transformers/src/transformers/models/wav2vec2/tokenization_wav2vec2.py\", line 237, in set_target_lang\r\n    self.unique_no_split_tokens.append(token)\r\nAttributeError: 'Wav2Vec2CTCTokenizer' object has no attribute 'unique_no_split_tokens'\r\n```\r\n\r\nThis PR re-instates `unique_no_split_tokens` as an attribute of the tokenizer. Note that this should already be tested for in the following test: https://github.com/huggingface/transformers/blob/dcbfd93d7aeb14f8ff08a48866d2a68950d4c69a/tests/models/wav2vec2/test_tokenization_wav2vec2.py#L798",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-09-22T15:08:37Z",
        "closed_at": "2023-09-22T15:55:55Z",
        "merged_at": "2023-09-22T15:55:55Z",
        "body": "# What does this PR do?\r\n\r\nThe PR #26136 added the `sampling_rate` as a **property** to the MusicGen config, such that it is compatible with the TTA pipeline. However, using a property means it is not registered when we call `config.to_dict()`, since it is not an attribute of the config.\r\n\r\nThis PR updates the TTA pipeline to avoid converting the config to a dict, thus keeping compatibility with MusicGen.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-22T15:05:36Z",
        "closed_at": "2023-09-25T13:42:00Z",
        "merged_at": "2023-09-25T13:41:59Z",
        "body": "# What does this PR do?\r\n\r\nThe PR addresses  #25967:\r\n\r\n`MaskFormerSwin` and `TimmBackbone` should not be listed in the framework support table on the index.md page because these models are backbones and so not meant to be loaded and used on their own. Instead, they define architectures which can be loaded using the AutoBackbone API. See the [comment](https://github.com/huggingface/transformers/issues/25967#issuecomment-1717818649) in the issue. \r\n\r\nThis PR updates the table, and the script that builds the table so that these models are excluded when building the table. \r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 17,
        "changed_files": 1,
        "created_at": "2023-09-22T15:00:31Z",
        "closed_at": "2023-10-03T09:13:46Z",
        "merged_at": "2023-10-03T09:13:46Z",
        "body": null,
        "comments": 1
    },
    {
        "merged": false,
        "additions": 95,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-22T14:58:46Z",
        "closed_at": "2023-09-22T14:59:48Z",
        "merged_at": null,
        "body": null,
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-09-22T13:25:13Z",
        "closed_at": "2023-09-22T14:29:27Z",
        "merged_at": "2023-09-22T14:29:27Z",
        "body": "# What does this PR do?\r\n\r\nPR addresses #25662, and fixes the indices in the doc example",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-22T13:16:48Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\ntest\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 363,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-22T12:26:04Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nAMD daily CI workflow file. There are something to do with the runners and report channels, but nothing is big.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 491,
        "deletions": 98,
        "changed_files": 9,
        "created_at": "2023-09-22T11:15:27Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFollowing on from #26182, which ported `torchaudio.compliance.kaldi.fbank` to `numpy` in `audio_utils`, this PR aims to enable the use of numpy porting in previous Feature Extractors (AST and SpeechToText) that used `torchaudio`.  It was discussed [here](https://github.com/huggingface/transformers/pull/26182#pullrequestreview-1631301297).\r\n\r\nThis serves two purposes:\r\n1. to give some examples of how to use `audio_utils` instead of `torchaudio` for future Feature Extractors\r\n2. the possibility of removing torchaudio altogether in the future.\r\n\r\nA next step would be to port `audio_utils` to torch, which might be faster (cc @sanchit-gandhi), but this is still open to discussion. Is this really relevant? And will it be really faster? \r\n\r\ncc @ArthurZucker and @sanchit-gandhi ",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-09-22T00:26:58Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Simplifies the way to get the number of embedding tokens.\r\n\r\nFollowing additional discussions on PR #26024 \r\n\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-21T20:57:10Z",
        "closed_at": "2023-09-22T09:37:59Z",
        "merged_at": null,
        "body": "This PR adds an option to use peft for finetuning text-classification task. It is required to run meta-llama/Llama-2-7b-hf which we would like to add to our internal benchmarking pipelines. Also adds an explicit padding token which resolves padding issues.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 20,
        "changed_files": 1,
        "created_at": "2023-09-21T19:56:35Z",
        "closed_at": "2023-09-22T12:52:27Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\nI've added gradient accumulation for Flax file training, recognizing that resource constraints can be a limitation. Gradient accumulation assists people in training models with large batch sizes, mitigating these limitations.\r\nI would like to cc dual @sanchit-gandhi and @ArthurZucker for review my PR\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-09-21T19:20:59Z",
        "closed_at": "2023-09-25T17:12:24Z",
        "merged_at": null,
        "body": "Hi,\r\nI just do a simple remove numpy and change to jax numpy, seen at now jax is more robust and can replace to numpy.\r\nI would like to cc @stevhliu to review my PR.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-09-21T19:13:57Z",
        "closed_at": "2023-10-05T08:18:27Z",
        "merged_at": "2023-10-05T08:18:27Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #26327 \r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ X] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [X ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 24,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-09-21T18:09:11Z",
        "closed_at": "2023-09-22T17:22:47Z",
        "merged_at": "2023-09-22T17:22:47Z",
        "body": "# What does this PR do?\r\nAs the demand for LLM models continues to grow, the size of the data being downloaded can slow down the process. To address this issue, I've submitted a PR to incorporate the \"num_proc\" feature into the \"load_dataset\" function, making data loading faster and more efficient.\r\n\r\nI would like cc @sanchit-gandhi to review my PR.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-21T15:38:22Z",
        "closed_at": "2023-09-22T06:58:31Z",
        "merged_at": "2023-09-22T06:58:31Z",
        "body": "# What does this PR do?\r\n\r\nThe daily doctest CI is killed due to memory issue (from `PersimmonForCausalLM.forward`'s docstring). It's not only about GPU, even running with the 60G CPU memory, the job is still killed.\r\n\r\nIf I  run that test only - it could pass (with running on CPU), but the process is killed when the whole suite is run. \r\n(There might be some memory (leak) issue to check as a whole).\r\n\r\nThis PR put `src/transformers/models/persimmon/modeling_persimmon.py` to `not_doctested.txt`.\r\n\r\nWe probably better to further separate what are not doctested yet and what are ignored intentionally, so we won't forget to try to put them back to doctests.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 38,
        "deletions": 18,
        "changed_files": 9,
        "created_at": "2023-09-21T12:31:24Z",
        "closed_at": "2023-10-02T11:55:47Z",
        "merged_at": "2023-10-02T11:55:47Z",
        "body": "# What does this PR do?\r\nFixes the CI broken by #23909 ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-09-21T09:29:22Z",
        "closed_at": "2023-09-26T05:06:39Z",
        "merged_at": "2023-09-26T05:06:39Z",
        "body": "# What does this PR do?\r\n\r\nThis PR adds a link to a demo notebook for ViTMatte.\r\n\r\nIt also adds a figure to make the docs a bit less boring :)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 119,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-09-21T08:02:27Z",
        "closed_at": null,
        "merged_at": null,
        "body": "<!-- PR\uc758 \uc81c\ubaa9\uc740 \"\ud83c\udf10 [i18n-KO] Translated `<your_file>.md` to Korean\" \uc73c\ub85c \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4! -->\r\n# What does this PR do?\r\n\r\nTranslated the `code_llama.md` file of the documentation to Korean.\r\nThank you in advance for your review. \r\n\r\nPart of https://github.com/huggingface/transformers/issues/20179\r\n\r\n## Before reviewing\r\n- [x] Check for missing / redundant translations (\ubc88\uc5ed \ub204\ub77d/\uc911\ubcf5 \uac80\uc0ac)\r\n- [x] Grammar Check (\ub9de\ucda4\ubc95 \uac80\uc0ac)\r\n- [x] Review or Add new terms to glossary (\uc6a9\uc5b4 \ud655\uc778 \ubc0f \ucd94\uac00)\r\n- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)\r\n- [ ] Check live-preview for gotchas (live-preview\ub85c \uc815\uc0c1\uc791\ub3d9 \ud655\uc778)\r\n\r\n## Who can review? (Initial)\r\n@bolizabeth, @nuatmochoi, @heuristicwave, @mjk0618, @keonju2, @harheem, @HongB1, @junejae, @54data, @seank021, @augustinLib, @sronger, @TaeYupNoh, @kj021, @eenzeenee\r\n<!-- 1. \uc704 \uccb4\ud06c\uac00 \ubaa8\ub450 \uc644\ub8cc\ub41c \ub4a4\uc5d0, \uc774 \uc544\ub798\uc5d0 \ub9ac\ubdf0\ub97c \uc694\uccad\ud560 \ud300\uc6d0\ub4e4\uc744 \uba58\uc158\ud574\uc8fc\uc138\uc694! -->\r\n<!-- May you please review this PR? @member1 @member2 ... -->\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review? (Final)\r\n\r\n<!-- 2. \ud300\uc6d0\ub4e4\uacfc \ub9ac\ubdf0\uac00 \ub05d\ub09c \ud6c4\uc5d0\ub9cc \ud5c8\uae45\ud398\uc774\uc2a4 \uc9c1\uc6d0\ub4e4\uc5d0\uac8c \ub9ac\ubdf0 \uc694\uccad\ud558\ub294 \uc544\ub798 \uc8fc\uc11d\uc744 \ub178\ucd9c\ud574\uc8fc\uc138\uc694! -->\r\n<!-- May you please review this PR? @sgugger, @ArthurZucker, @eunseojo -->",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2636,
        "deletions": 0,
        "changed_files": 8,
        "created_at": "2023-09-21T07:57:15Z",
        "closed_at": "2023-09-27T18:33:05Z",
        "merged_at": "2023-09-27T18:33:05Z",
        "body": "# What does this PR do ?\r\nThis PR translates all the file of the first toc chapter to german\r\nIt continues https://github.com/huggingface/transformers/issues/18564\r\n\r\n- [x] add new model\r\n- [x] add new pipeline\r\n- [x] add tensorflow model\r\n- [x] llm tutorial\r\n- [x] peft\r\n- [x] run scripts\r\n- [x] transformers agents\r\n- [x] re-read at the live documentation preview from PR",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 9103,
        "deletions": 0,
        "changed_files": 31,
        "created_at": "2023-09-21T07:21:21Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR adds the ImageBind model ([paper](https://arxiv.org/abs/2305.05665), [code](https://github.com/facebookresearch/ImageBind)), a multimodal model which can map six different modalities to the same shared representation space.\r\n\r\nAs stated in their [blog post](https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/),\r\n\r\n> \"[ImageBind is] the first AI model capable of binding information from six modalities. The [model](https://github.com/facebookresearch/ImageBind) learns a single embedding, or shared representation space, not just for text, image/video, and audio, but also for sensors that record depth (3D), thermal (infrared radiation), and inertial measurement units (IMU), which calculate motion and position.\"\r\n\r\n<img width=\"625\" alt=\"imagebind_figure_2\" src=\"https://github.com/huggingface/transformers/assets/58458699/2eb66af1-883b-4705-9a1b-cdc009bf82fc\">\r\n\r\nFixes #23240. Based on a previous PR #23284.\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@amyeroberts\r\n@ArthurZucker\r\n@shehanmunasinghe\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 205,
        "deletions": 43,
        "changed_files": 7,
        "created_at": "2023-09-21T03:23:05Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR moves `rotate_half` and `apply_rotary_pos_emb` inside the `LlamaRotaryEmbedding` base class. When all of the rotary embedding computations are in one class, the [export of LLaMA-2 to ONNX](https://github.com/pytorch/pytorch/pull/109759) can be done with the `export_modules_as_functions` option for the `LlamaRotaryEmbedding` class in `torch.onnx.export`. With all of the rotary embedding computations in one function in the exported ONNX model, it is easier to optimize the model and [integrate with Optimum](https://github.com/huggingface/optimum/pull/1289).\r\n\r\n## Who can review?\r\n\r\n@fxmarty, @ArthurZucker, @younesbelkada",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 221,
        "deletions": 25,
        "changed_files": 1,
        "created_at": "2023-09-21T00:05:54Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nThis is to address issue in https://github.com/huggingface/transformers/issues/22639 \r\nThis PR is based on the idea from\r\nhttps://github.com/huggingface/transformers/issues/22639#issuecomment-1507525155\r\n\r\nThe original implementation of beam search effectively multiplies the batch size memory-wise and compute-wise by the batch size. If you have a batch size of 1 and a beam search of 8, model.forward sees 8 samples as effective batch size. This implementation is not necessary per se and can consume a lot of memory.\r\n\r\nThe new implementation split the full_batch(num_beam x batch size) inputs into a list of reduced_batch(beam_search_batch_size), run them sequentially and concat them back to a single model_output object. It involves two helper function:\r\n-  `def concat_model_outputs(objs: List[ModelOutput]) -> ModelOutput` \r\n-  `def split_model_inputs( obj: Union[ModelOutput, Dict], split_size: int, full_batch_size: int ) -> List[Union[ModelOutput, Dict]]`\r\n\r\nThe new implementation can be used in 4 decoding methods: `beam_search`, `beam_sample`, `group_beam_search` and `constrained_beam_search`\r\n\r\nThe expected behavior is that it produces exactly the same output(logits) as the original implementation.\r\n\r\nI tested it with the following quick test and it works regardless of input\r\n\r\n```python\r\n\r\nfrom\r\ntransformers import GPT2Tokenizer, AutoModelForCausalLM\r\nimport numpy as\r\nnp\r\nfrom transformers import (\r\n    AutoTokenizer,\r\n\r\nAutoModelForSeq2SeqLM,\r\n    LogitsProcessorList,\r\n\r\nMinLengthLogitsProcessor,\r\n    BeamSearchScorer,\r\n)\r\n\r\ntokenizer =\r\nGPT2Tokenizer.from_pretrained(\"gpt2\")\r\nmodel =\r\nAutoModelForCausalLM.from_pretrained(\"gpt2\")\r\ntokenizer.pad_token_id =\r\ntokenizer.eos_token_id\r\nmodel_inputs = tokenizer('I enjoy walking with my\r\ncute dog', return_tensors='pt')\r\n\r\n# activate beam search and\r\nearly_stopping\r\nbeam_output = model.generate(\r\n    **model_inputs,\r\n\r\nmax_new_tokens=40,\r\n    num_beams=5,\r\n\r\nearly_stopping=True\r\n)\r\n\r\nprint(\"Output:\\n\" + 100 *\r\n'-')\r\nprint(tokenizer.decode(beam_output[0],\r\nskip_special_tokens=True))\r\n\r\nbeam_output_w_subbatch = model.generate(\r\n\r\n**model_inputs,\r\n    max_new_tokens=40,\r\n    num_beams=5,\r\n\r\nearly_stopping=True,\r\n    beam_search_batch_size=1\r\n)\r\n\r\nprint(\"Output:\\n\" +\r\n100 * '-')\r\nprint(tokenizer.decode(beam_output_w_subbatch[0],\r\nskip_special_tokens=True))\r\n\r\nassert (beam_output ==\r\nbeam_output_w_subbatch).all(), \"Beam search results from sub batch and\r\nfull batch are different\"\r\n```\r\n\r\nTODO: only did it for pytorch models, if you think this PR is promising, I can do it for tensorflow too.\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n\r\nFixes #22639 \r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@gante @ArthurZucker \r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2023-09-20T17:27:53Z",
        "closed_at": "2023-09-26T15:52:20Z",
        "merged_at": "2023-09-26T15:52:20Z",
        "body": "InternLM is based on the LLaMA code but adds a `config.bias` parameter. We can support those models by adding `config.bias` to LLaMA, and preserve backward compatibility by defaulting it to `False`",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-20T16:05:04Z",
        "closed_at": "2023-09-21T12:57:36Z",
        "merged_at": "2023-09-21T12:57:36Z",
        "body": "# What does this PR do?\nFixes  #26276, `tokenizers==0.14` requires `\"huggingface-hub>=0.16.4,<1.0\"`, so updated the dependency in setup.py",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 19,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-09-20T15:47:40Z",
        "closed_at": "2023-10-02T16:29:28Z",
        "merged_at": "2023-10-02T16:29:28Z",
        "body": "# What does this PR do?\nFixes #26156, the `fast` CodeLLama tokenizer excepts the fill token to be != None. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 30,
        "deletions": 34,
        "changed_files": 2,
        "created_at": "2023-09-20T15:32:53Z",
        "closed_at": "2023-09-28T18:02:27Z",
        "merged_at": "2023-09-28T18:02:27Z",
        "body": "# What does this PR do?\r\n\r\nFollowing the update to the Whisper tokenizer to handle encoding/decoding timestamps (#26054), there is one line in the decoding which takes extremely long: https://github.com/huggingface/transformers/blob/f94c9b3d863c1a95b44b5b3ea9ce3cbd27fc7609/src/transformers/models/whisper/tokenization_whisper.py#L614\r\n\r\nHere we do an order `N * M`  operation to filter out all the timestamp tokens, where `N` is the length of the token ids, and `M` the number of timestamp tokens (for each token, check whether it\u2019s in the timestamp token list).\r\n\r\nIn practice, this is causing decoding to take **extremely** long for typical validation sets, e.g. LibriSpeech test clean took ~30 mins for the tokenizer to decode on a TPU v3 (which has lots of CPU power to run this operation).\r\n\r\nThis PR switches the timestamp filtering to a regex string operation, which in a toy benchmark was a factor of > 2000 faster. Would love to hear from @ArthurZucker whether we're happy to sacrifice a bit of readability for this speed-up!",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-20T15:11:19Z",
        "closed_at": "2023-10-02T16:30:45Z",
        "merged_at": "2023-10-02T16:30:45Z",
        "body": "# What does this PR do?\nFixes #26287 where the ouptuts of `prepare_for_model` are different. There is a test for  `prepare_for_model` but we don't really make sure that the outputs match. ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-09-20T15:07:39Z",
        "closed_at": "2023-09-21T16:36:05Z",
        "merged_at": "2023-09-21T16:36:05Z",
        "body": "Followup to #26291 - I missed one of the messages! This PR also adds some linebreaks so they display without messy long lines and wrapping.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-20T14:34:39Z",
        "closed_at": "2023-09-24T15:24:24Z",
        "merged_at": null,
        "body": "testing https://github.com/huggingface/doc-builder/pull/398",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 8,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-09-20T14:23:12Z",
        "closed_at": "2023-09-20T14:48:36Z",
        "merged_at": "2023-09-20T14:48:36Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n`BrosModel` will only work if `bbox` is not `None` and currently there is no validation code for this case.\r\nAdd input validation code to the beginning of `BrosModel.forward` method as @ydshieh suggested. \r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. https://github.com/huggingface/transformers/pull/23190#issuecomment-1727796291\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ydshieh \r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-20T14:15:26Z",
        "closed_at": "2023-09-22T01:33:30Z",
        "merged_at": "2023-09-22T01:33:30Z",
        "body": "Fix link\r\n\r\n# What does this PR do?\r\n\r\n\r\nFix broken link.\r\nChange `https://huggingface.co/docs/transformers/model_doc/auto.` \r\nfor `https://huggingface.co/docs/transformers/model_doc/auto`\r\n\r\n<img width=\"1093\" alt=\"Captura de pantalla 2023-09-20 a las 16 10 06\" src=\"https://github.com/huggingface/transformers/assets/24204714/c1d801bf-691e-4591-9071-1e3584e26ada\">\r\n\r\n\r\n\r\n\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 21,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-20T14:08:37Z",
        "closed_at": "2023-09-21T12:46:06Z",
        "merged_at": "2023-09-21T12:46:06Z",
        "body": "The FSMT weight sharing needs to take into account whether `tie_word_embeddings` is `True` or not, given that the model has checkpoints that have it set to `True`, and others set to `False`.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 7,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-09-20T13:55:06Z",
        "closed_at": "2023-09-20T14:18:49Z",
        "merged_at": "2023-09-20T14:18:49Z",
        "body": "I was seeing these a lot while working on InternLM and it's time to fix them!",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-20T11:39:15Z",
        "closed_at": "2023-10-02T09:06:41Z",
        "merged_at": "2023-10-02T09:06:41Z",
        "body": "The current `forward` requires (the shape of) `input_ids` for deriving other variables whenever `input_ids` or `inputs_embeds` is provided. Change this to use the given one instead of `input_ids` all the time.\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes  #26288 \r\n\r\n## Who can review?\r\n\r\n@ArthurZucker and @younesbelkada\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 174,
        "deletions": 30,
        "changed_files": 9,
        "created_at": "2023-09-20T10:04:09Z",
        "closed_at": "2023-09-25T16:08:13Z",
        "merged_at": "2023-09-25T16:08:13Z",
        "body": "# What does this PR do?\r\n\r\nUpdate tiny model information and pipeline tests",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-20T06:35:07Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nuse model_wrapped in evaluation_loop #26282\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 313,
        "deletions": 40,
        "changed_files": 7,
        "created_at": "2023-09-20T05:52:23Z",
        "closed_at": "2023-09-20T05:56:17Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nRemove redundant code: The variable \"has_default_max_length\" will always be \"False\" for the 2 lines of code:\r\n\r\n  1. model_kwargs = generation_config.update(**kwargs)\r\n  2. has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\r\n\r\nSo I remove the code related to it.\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker @gante\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-09-20T05:24:46Z",
        "closed_at": "2023-09-20T06:02:56Z",
        "merged_at": "2023-09-20T06:02:56Z",
        "body": "# What does this PR do?\r\n1. Fixes the non-torch test failures `ERROR tests/fsdp/test_fsdp.py - NameError: name 'require_fsdp_version' is not defined`",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 22,
        "deletions": 25,
        "changed_files": 4,
        "created_at": "2023-09-20T02:02:20Z",
        "closed_at": "2023-09-20T08:22:07Z",
        "merged_at": "2023-09-20T08:22:07Z",
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nUpdate [Bros](https://arxiv.org/abs/2108.04539) checkpoint.\r\n\r\nThe `naver-clova-ocr/bros-base-uncased` checkpoint has `bbox_projection` layer in `BrosEmbeddings` class but this layer is moved to `BrosBboxEmbeddings` class. And users are expected to use `jinho8345/bros-base-uncased` checkpoint instead of `naver-clova-ocr/bros-base-uncased`.\r\n\r\n* `naver-clova-ocr/bros-base-uncased` : original pretrained checkpoint from naver-clova-ocr\r\n* `jinho8345/bros-base-uncased` :  weights renamed version from `naver-clova-ocr/bros-base-uncased` using [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bros/convert_bros_to_pytorch.py)\r\n\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. - https://github.com/huggingface/transformers/pull/23190#issuecomment-1725882060\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ydshieh \r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 7
    },
    {
        "merged": true,
        "additions": 442,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-09-19T20:01:49Z",
        "closed_at": "2023-10-12T12:48:02Z",
        "merged_at": "2023-10-12T12:48:02Z",
        "body": "# What does this PR do?\r\n\r\nThis PR addresses part 2.2 (\"Prompting\" ) of the issue [#24575](https://github.com/huggingface/transformers/issues/24575)\u2028 \r\n\r\nIt adds an LLM Prompting Guide to the docs that covers the following topics:\r\n* basics of prompting,\r\n* encoder-decoder models vs decoder-only models,\r\n* base vs instruct models,\r\n* basic prompts to solve common NLP tasks,\r\n* best practices for prompting,\r\n* advanced techniques like few-shot learning and chain-of-thought\r\n* prompting vs fine-tuning\r\n\r\nLet me know, if there's anything missing that has to be included.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 4661,
        "deletions": 2683,
        "changed_files": 203,
        "created_at": "2023-09-19T18:59:03Z",
        "closed_at": "2023-09-19T19:58:44Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR adds an LLM Prompting Guide to the docs that covers the following topics: \r\n\r\n- basics of prompting, \r\n- encoder-decoder models vs decoder-only models, \r\n- base vs instruct models, \r\n- basic prompts to solve common NLP tasks, \r\n- best practices for prompting, \r\n- advanced techniques like few-shot learning and chain-of-thought\r\n- prompting vs fine-tuning\r\n\r\nLet me know, if there's anything missing that has to be included.",
        "comments": 0
    },
    {
        "merged": true,
        "additions": 68,
        "deletions": 9,
        "changed_files": 6,
        "created_at": "2023-09-19T17:53:16Z",
        "closed_at": "2023-09-28T09:13:04Z",
        "merged_at": "2023-09-28T09:13:04Z",
        "body": "# What does this PR do?\r\n\r\nCurrently on main branch the script below fails:\r\n\r\n```python\r\nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\n\r\nmodel_id = \"andrewrreed/falcon-7b-guanaco-qlora-arr\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\r\n    torch_dtype=torch.bfloat16, \r\n    load_in_4bit=True,\r\n)\r\n\r\nprint(model)\r\n```\r\n\r\nThis is because of falcon models the revision parameter gets overriden here: https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon/modeling_falcon.py#L779 for BC and to make sure users will load transformers model \r\n\r\nThis introduced an error in the logic of loading adapters as the same `revision` argument was used all over the place. As this scenario might occur more often, I propose to introduce a new argument `adapter_revision` for users that want to load a base model and an adapter from different revisions.\r\n\r\ncc @ArthurZucker @LysandreJik \r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 170,
        "changed_files": 2,
        "created_at": "2023-09-19T17:03:16Z",
        "closed_at": "2023-09-19T17:08:17Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nSimply tests if we can successfully build FA2 on docker\r\n\r\nAddresses: https://github.com/huggingface/transformers/pull/25598/files#r1324800667",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-09-19T13:35:16Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n`resize_token_embeddings` intends to update model vocab_size to the size of the first dimension of the newly-built embedding's weight.\r\n\r\nHowever, if this method is call on an uninitialized model (e.g.: in `no_init_weight` context) and the parameter `new_num_tokens` equals to the old vocab_size, the internal function `_resize_token_embeddings` will return the old embedding directly whose weight is not initialized.\r\n\r\nIn this scenario, the model's vocab_size will be set to 0 which is unexpected.\r\n\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker and @younesbelkada\r\n\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 192,
        "deletions": 4,
        "changed_files": 8,
        "created_at": "2023-09-19T13:26:43Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nThis PR adds `GitTokenizer`\r\n\r\nFixes part of https://github.com/huggingface/transformers/issues/21110\r\nIt was discussed with @amyeroberts in https://github.com/huggingface/transformers/pull/25509#issuecomment-1719288200\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker @Narsil\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerzr and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 54,
        "deletions": 19,
        "changed_files": 1,
        "created_at": "2023-09-19T12:38:18Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR adds the following:\r\n- [x] Job to build AMD specific docker images for the CI\r\n- [x] Update the GA docker dependencies to their latest version(s)",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 14,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-09-19T11:35:15Z",
        "closed_at": "2023-09-20T15:19:31Z",
        "merged_at": "2023-09-20T15:19:31Z",
        "body": "# What does this PR do?\nFixes #26239, where we have an edge case of the tokenization. When porting from CodeLllama a condition in `_tokenize` was ommited, which leads to merging the unk token with part of the input, and then stripping it. ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 53,
        "deletions": 60,
        "changed_files": 1,
        "created_at": "2023-09-19T11:16:54Z",
        "closed_at": "2023-09-19T19:04:56Z",
        "merged_at": "2023-09-19T19:04:56Z",
        "body": "# What does this PR do?\r\n1. PRs https://github.com/huggingface/transformers/pull/25394 and https://github.com/huggingface/transformers/pull/25732 resulted in 17 DeepSpeed slow tests failing. The reason is that those PRs might have missed the fact that the shape will be 0 when working with layers that have been initialized by DeepSpeed init for ZeRO-3. With this PR, all the failing tests pass.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-09-19T10:55:02Z",
        "closed_at": "2023-09-19T11:25:12Z",
        "merged_at": "2023-09-19T11:25:12Z",
        "body": "Related to this [slack thread](https://huggingface.slack.com/archives/C01NE71C4F7/p1692855942544019) (private).\r\n\r\nSince end of August (https://github.com/huggingface/transformers/commit/68fa9a5937ae7aa707f5ff2639aa36a37a0a9928), gated repo tests were skipped because failing to pass. This was due to a server-side change that now makes the README files readable even on gated repo (since users have to be able to read a model card before requesting access). This PR fixes the tests -and unskip them- by trying to download [gated_file.txt](https://huggingface.co/hf-internal-testing/dummy-gated-model/blob/main/gated_file.txt) instead.\r\n\r\ncc @ydshieh ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 11,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-19T09:54:32Z",
        "closed_at": "2023-09-20T14:40:15Z",
        "merged_at": "2023-09-20T14:40:15Z",
        "body": "As per title, make the deepspeed available function more robust as https://github.com/huggingface/accelerate/blob/69e4c3c54da3201eda288b500d138761e7a5221c/src/accelerate/utils/imports.py#L72\r\n\r\nHaving `tests/deepspeed` & [`get_env`](https://github.com/huggingface/transformers/blob/eb8489971ac1415f67b0abdd1584fde8b659ced9/src/transformers/testing_utils.py#L1344) that adds `tests/` in the path then makes `is_deepspeed_availebl()` returns `True` although it should not, and in turn trainer.py [tries to import](https://github.com/huggingface/transformers/blob/eb8489971ac1415f67b0abdd1584fde8b659ced9/src/transformers/trainer.py#L217) `DeepSpeedSchedulerWrapper` that is [not imported in accelerate](https://github.com/huggingface/accelerate/blob/69e4c3c54da3201eda288b500d138761e7a5221c/src/accelerate/utils/__init__.py#L92) as accelerate rightfully detects that DeepSpeed is not available.\r\n\r\nThis issue makes the test `tests/extended/test_trainer_ext.py::TestTrainerExt::test_run_seq2seq_apex` fail when APEX is installed but DeepSpeed is not.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 5,
        "changed_files": 5,
        "created_at": "2023-09-19T08:39:14Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nThis PR will fix the keypoint 0.0 at this transformers/models/detr/image_processing_detr.py as suggested by @duckheada\r\n\r\nTo fix this, I had to edit this file in transformers library:\r\n.conda/lib/python3.9/site-packages/transformers/models/detr/image_processing_detr.py\r\n\r\nI changed this as suggested by: @duckheada\r\n  \r\n\r\n    if annotations and \"keypoints\" in annotations[0]:\r\n       keypoints = [obj[\"keypoints\"] for obj in annotations]\r\n       print(\"keypoints\", keypoints) #TODO: remove\r\n       keypoints = np.asarray(keypoints, dtype=np.float32)\r\n       num_keypoints = keypoints.shape[0]\r\n       keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\r\n       new_target[\"keypoints\"] = keypoints[keep]\r\n\r\nTo this:\r\n\r\n    if annotations and \"keypoints\" in annotations[0]:\r\n     keypoints = [obj[\"keypoints\"] for obj in annotations]\r\n     # Apply the keep mask here to filter the relevant annotations\r\n     keypoints = [keypoints[i] for i in range(len(keypoints)) if keep[i]]\r\n     # converting the filtered keypoints list to a numpy array and reshape it\r\n     keypoints = np.asarray(keypoints, dtype=np.float32)\r\n     num_keypoints = keypoints.shape[0]\r\n     keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\r\n     new_target[\"keypoints\"] = keypoints # We no longer apply keep mask here\r\n\r\nWhy?\r\nTo ensure that the filtering applied to the key points respects its original structure (number of keypoints per annotation). When you reshape keypoints with key points. reshape((-1, 3)), it loses the information about which keypoints belong to which annotation.\r\n\r\nHere is what needed to be done (at least in my little hack-ish workaround):\r\n\r\nBefore reshaping the key points array, I had to apply the keep mask to retain only the annotations I was interested in. Only after this could I reshape the keypoints array to apply further operations.\r\nThen, I applied the keep mask on the keypoints list before converting it into a numpy array and reshaping it. This ensured that I only keep the keypoints corresponding to the bounding boxes that satisfy the condition in the keep mask.\r\n\r\nFixes #26126 \r\n\r\n## Who can review?\r\n\r\n@ArthurZucker, @younesbelkada, and @amyeroberts \r\nPlease let me know if I need to do anything else for this issue.\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 16,
        "deletions": 12,
        "changed_files": 1,
        "created_at": "2023-09-19T07:26:50Z",
        "closed_at": "2023-09-20T15:39:00Z",
        "merged_at": "2023-09-20T15:39:00Z",
        "body": "# What does this PR do?\r\n\r\nFixes https://github.com/huggingface/transformers/issues/26207\r\n\r\nSome users try to perform pure fine-tuning on quantized models; this is simply not supported.\r\nBefore this PR, in that case we only pass a logger.info which is not a strong enough warning for users to understand why training does not work.\r\n\r\ncc @SunMarc @ArthurZucker \r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3629,
        "deletions": 0,
        "changed_files": 22,
        "created_at": "2023-09-19T07:03:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "[PatchTSMixer](https://arxiv.org/pdf/2306.09364.pdf) ([KDD 2023](https://dl.acm.org/doi/abs/10.1145/3580305.3599533)) is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.\r\n\r\n@kashif \r\n\r\nDone: ~~TODOs~~\r\n\r\n- [x] Add generate method\r\n- [x] Make pretrained dataset publicly available\r\n- [x] Make pretrained weights publicly available ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 308,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-19T06:51:05Z",
        "closed_at": "2023-09-27T20:47:44Z",
        "merged_at": "2023-09-27T20:47:44Z",
        "body": "# What does this PR do?\r\n\r\n@kj021 translated the `debugging.md` file of the documentation to Korean. I made sure hanging suggestions were resolved.\r\nThank you in advance for your review. \r\n\r\nPart of https://github.com/huggingface/transformers/issues/20179\r\nMoved from https://github.com/huggingface/transformers/pull/24869\r\n\r\n## Before reviewing\r\n- [x] Check for missing / redundant translations (\ubc88\uc5ed \ub204\ub77d/\uc911\ubcf5 \uac80\uc0ac)\r\n- [x] Grammar Check (\ub9de\ucda4\ubc95 \uac80\uc0ac)\r\n- [x] Review or Add new terms to glossary (\uc6a9\uc5b4 \ud655\uc778 \ubc0f \ucd94\uac00)\r\n- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)\r\n- [ ] Check live-preview for gotchas (live-preview\ub85c \uc815\uc0c1\uc791\ub3d9 \ud655\uc778)\r\n\r\n## Who can review? (Initial)\r\n\r\n<!-- 1. \uc704 \uccb4\ud06c\uac00 \ubaa8\ub450 \uc644\ub8cc\ub41c \ub4a4\uc5d0, \uc774 \uc544\ub798\uc5d0 \ub9ac\ubdf0\ub97c \uc694\uccad\ud560 \ud300\uc6d0\ub4e4\uc744 \uba58\uc158\ud574\uc8fc\uc138\uc694! -->\r\n<!-- Team OSSCA, may you please review this PR? @bolizabeth, @nuatmochoi, @heuristicwave, @mjk0618, @keonju2, @harheem, @HongB1, @junejae, @54data, @Sunmin0520, @seank021, @augustinLib, @sronger, @TaeYupNoh, @kj021, @eenzeenee -->\r\nTeam PseudoLab, may you please review this PR? @0525hhgus, @KIHOON71, @sim-so, @gabrielwithappy, @HanNayeoniee, @wonhyeongseo, @jungnerd\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review? (Final)\r\n\r\n<!-- 2. \ud300\uc6d0\ub4e4\uacfc \ub9ac\ubdf0\uac00 \ub05d\ub09c \ud6c4\uc5d0\ub9cc \ud5c8\uae45\ud398\uc774\uc2a4 \uc9c1\uc6d0\ub4e4\uc5d0\uac8c \ub9ac\ubdf0 \uc694\uccad\ud558\ub294 \uc544\ub798 \uc8fc\uc11d\uc744 \ub178\ucd9c\ud574\uc8fc\uc138\uc694! -->\r\n<!-- May you please review this PR? @sgugger, @ArthurZucker, @stevhliu -->",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 124,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-19T06:41:35Z",
        "closed_at": "2023-10-12T22:00:13Z",
        "merged_at": "2023-10-12T22:00:13Z",
        "body": "# What does this PR do?\r\n\r\n@bolizabeth translated the `big_models.md` file of the documentation to Korean. I made sure hanging suggestions were resolved.\r\nThank you in advance for your review. \r\n\r\nPart of https://github.com/huggingface/transformers/issues/20179\r\nMoved from https://github.com/huggingface/transformers/pull/24985\r\n\r\n## Before reviewing\r\n- [x] Check for missing / redundant translations (\ubc88\uc5ed \ub204\ub77d/\uc911\ubcf5 \uac80\uc0ac)\r\n- [x] Grammar Check (\ub9de\ucda4\ubc95 \uac80\uc0ac)\r\n- [x] Review or Add new terms to glossary (\uc6a9\uc5b4 \ud655\uc778 \ubc0f \ucd94\uac00)\r\n- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)\r\n- [x] Check live-preview for gotchas (live-preview\ub85c \uc815\uc0c1\uc791\ub3d9 \ud655\uc778)\r\n\r\n## Who can review? (Initial)\r\n\r\n<!-- 1. \uc704 \uccb4\ud06c\uac00 \ubaa8\ub450 \uc644\ub8cc\ub41c \ub4a4\uc5d0, \uc774 \uc544\ub798\uc5d0 \ub9ac\ubdf0\ub97c \uc694\uccad\ud560 \ud300\uc6d0\ub4e4\uc744 \uba58\uc158\ud574\uc8fc\uc138\uc694! -->\r\n<!-- Team OSSCA, may you please review this PR? @bolizabeth, @nuatmochoi, @heuristicwave, @mjk0618, @keonju2, @harheem, @HongB1, @junejae, @54data, @Sunmin0520, @seank021, @augustinLib, @sronger, @TaeYupNoh, @kj021, @eenzeenee -->\r\nTeam PseudoLab, may you please review this PR? @0525hhgus, @KIHOON71, @sim-so, @gabrielwithappy, @HanNayeoniee, @wonhyeongseo, @jungnerd\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review? (Final)\r\n\r\n<!-- 2. \ud300\uc6d0\ub4e4\uacfc \ub9ac\ubdf0\uac00 \ub05d\ub09c \ud6c4\uc5d0\ub9cc \ud5c8\uae45\ud398\uc774\uc2a4 \uc9c1\uc6d0\ub4e4\uc5d0\uac8c \ub9ac\ubdf0 \uc694\uccad\ud558\ub294 \uc544\ub798 \uc8fc\uc11d\uc744 \ub178\ucd9c\ud574\uc8fc\uc138\uc694! -->\r\n<!-- May you please review this PR? @sgugger, @ArthurZucker, @stevhliu -->",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 535,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-19T06:31:40Z",
        "closed_at": "2023-09-27T20:51:16Z",
        "merged_at": "2023-09-27T20:51:16Z",
        "body": "# What does this PR do?\r\n\r\n@hyunhp translated the `perf_train_gpu_many.md` file of the documentation to Korean. I made sure hanging suggestions were resolved.\r\nThank you in advance for your review. \r\n\r\nPart of https://github.com/huggingface/transformers/issues/20179\r\nMoved from https://github.com/huggingface/transformers/pull/24983\r\n\r\n## Before reviewing\r\n- [x] Check for missing / redundant translations (\ubc88\uc5ed \ub204\ub77d/\uc911\ubcf5 \uac80\uc0ac)\r\n- [x] Grammar Check (\ub9de\ucda4\ubc95 \uac80\uc0ac)\r\n- [x] Review or Add new terms to glossary (\uc6a9\uc5b4 \ud655\uc778 \ubc0f \ucd94\uac00)\r\n- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)\r\n- [x] Check live-preview for gotchas (live-preview\ub85c \uc815\uc0c1\uc791\ub3d9 \ud655\uc778)\r\n\r\n## Who can review? (Initial)\r\n\r\n<!-- 1. \uc704 \uccb4\ud06c\uac00 \ubaa8\ub450 \uc644\ub8cc\ub41c \ub4a4\uc5d0, \uc774 \uc544\ub798\uc5d0 \ub9ac\ubdf0\ub97c \uc694\uccad\ud560 \ud300\uc6d0\ub4e4\uc744 \uba58\uc158\ud574\uc8fc\uc138\uc694! -->\r\n<!-- Team OSSCA, may you please review this PR? @bolizabeth, @nuatmochoi, @heuristicwave, @mjk0618, @keonju2, @harheem, @HongB1, @junejae, @54data, @Sunmin0520, @seank021, @augustinLib, @sronger, @TaeYupNoh, @kj021, @eenzeenee -->\r\nTeam PseudoLab, may you please review this PR? @0525hhgus, @KIHOON71, @sim-so, @gabrielwithappy, @HanNayeoniee, @wonhyeongseo, @jungnerd\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review? (Final)\r\n\r\n<!-- 2. \ud300\uc6d0\ub4e4\uacfc \ub9ac\ubdf0\uac00 \ub05d\ub09c \ud6c4\uc5d0\ub9cc \ud5c8\uae45\ud398\uc774\uc2a4 \uc9c1\uc6d0\ub4e4\uc5d0\uac8c \ub9ac\ubdf0 \uc694\uccad\ud558\ub294 \uc544\ub798 \uc8fc\uc11d\uc744 \ub178\ucd9c\ud574\uc8fc\uc138\uc694! -->\r\n<!-- May you please review this PR? @sgugger, @ArthurZucker, @stevhliu -->",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 255,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-19T06:17:02Z",
        "closed_at": "2023-10-02T16:55:33Z",
        "merged_at": "2023-10-02T16:55:33Z",
        "body": "# What does this PR do?\r\n\r\n@HanNayeoniee translated the `tokenizer_summary.md` file of the documentation to Korean. I made sure hanging suggestions were resolved.\r\nThank you in advance for your review. \r\n\r\nPart of https://github.com/huggingface/transformers/issues/20179\r\nMoved from https://github.com/huggingface/transformers/pull/25023\r\n\r\n## Before reviewing\r\n- [x] Check for missing / redundant translations (\ubc88\uc5ed \ub204\ub77d/\uc911\ubcf5 \uac80\uc0ac)\r\n- [x] Grammar Check (\ub9de\ucda4\ubc95 \uac80\uc0ac)\r\n- [x] Review or Add new terms to glossary (\uc6a9\uc5b4 \ud655\uc778 \ubc0f \ucd94\uac00)\r\n- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)\r\n- [x] Check live-preview for gotchas (live-preview\ub85c \uc815\uc0c1\uc791\ub3d9 \ud655\uc778)\r\n\r\n## Who can review? (Initial)\r\n\r\n<!-- 1. \uc704 \uccb4\ud06c\uac00 \ubaa8\ub450 \uc644\ub8cc\ub41c \ub4a4\uc5d0, \uc774 \uc544\ub798\uc5d0 \ub9ac\ubdf0\ub97c \uc694\uccad\ud560 \ud300\uc6d0\ub4e4\uc744 \uba58\uc158\ud574\uc8fc\uc138\uc694! -->\r\n<!-- Team OSSCA, may you please review this PR? @bolizabeth, @nuatmochoi, @heuristicwave, @mjk0618, @keonju2, @harheem, @HongB1, @junejae, @54data, @Sunmin0520, @seank021, @augustinLib, @sronger, @TaeYupNoh, @kj021, @eenzeenee -->\r\nTeam PseudoLab, may you please review this PR? @0525hhgus, @KIHOON71, @sim-so, @gabrielwithappy, @HanNayeoniee, @wonhyeongseo, @jungnerd\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review? (Final)\r\n\r\n<!-- 2. \ud300\uc6d0\ub4e4\uacfc \ub9ac\ubdf0\uac00 \ub05d\ub09c \ud6c4\uc5d0\ub9cc \ud5c8\uae45\ud398\uc774\uc2a4 \uc9c1\uc6d0\ub4e4\uc5d0\uac8c \ub9ac\ubdf0 \uc694\uccad\ud558\ub294 \uc544\ub798 \uc8fc\uc11d\uc744 \ub178\ucd9c\ud574\uc8fc\uc138\uc694! -->\r\n<!-- May you please review this PR? @sgugger, @ArthurZucker, @stevhliu -->",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 13,
        "changed_files": 7,
        "created_at": "2023-09-19T04:31:19Z",
        "closed_at": "2023-09-22T18:39:28Z",
        "merged_at": "2023-09-22T18:39:28Z",
        "body": "# What does this PR do?\r\n\r\nFixed unclosed p tags\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 55,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-09-19T01:30:33Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\nFixes #22505 \r\n\r\nOur `center_crop` function does not give proper results if `orig_height - crop_height` is odd or if `orig_width - crop_width` is odd. \r\n\r\nIt seems that our code [here](https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L461C66-L461C88) and [here](https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L464) alerts about this problem.\r\n\r\nThis problem explains why:\r\n* Our `test_center_crop` fails [here](https://github.com/huggingface/transformers/blob/e469be340673d1f6931eb22562efd2be7f5a5b8d/tests/test_image_transforms.py#L329) if we try image sizes with odd height or width. e.g. changing this line [here](https://github.com/huggingface/transformers/blob/e469be340673d1f6931eb22562efd2be7f5a5b8d/tests/test_image_transforms.py#L317) to `image = np.random.randint(0, 256, (3, 223, 223))` will make the test fail.\r\n* Our results are different than OpenAI image features, as reported in issue #22505 \r\n\r\nThis PR fixes this problem and includes pytests comparing the output of our `center_crop` function to `torchvision.transforms.CenterCrop` with different image sizes and expected output sizes.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@ArthurZucker and @amyeroberts\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-18T23:19:10Z",
        "closed_at": "2023-09-18T23:49:03Z",
        "merged_at": "2023-09-18T23:49:03Z",
        "body": "# What does this PR do?\r\nThis fixes a wrong suggested tag of a Github user not acquainted with this project to the correct contributor. \r\n\r\n#### Personal Note\r\nI can definitely see that his and my username seem switched given our clear names and I'm sorry for any confusion. That username was given to me by a teacher in school who didn't want to spell my full last name and it stuck.\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\nDocumentation: @stevhliu and @MKhalusova ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-18T22:09:30Z",
        "closed_at": "2023-09-27T09:58:04Z",
        "merged_at": "2023-09-27T09:58:04Z",
        "body": "# What does this PR do?\r\n\r\nThis PR fixes the tokenization of the `<s>` and `</s>` tokens, when `transformers` is installed but `tokenizers` is not installed, fixing the string representation of the `AddedToken` class.\r\n\r\nUsing `transformers` _without_ `tokenizers` installed results in the following problem:\r\n```\r\nimport transformers\r\ntokenizer = transformers.AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\r\n\r\nprint(tokenizer.convert_tokens_to_ids(['<s>', 'a', '</s>']))\r\nprint(tokenizer.encode('a'))\r\n```\r\nPrints:\r\n```\r\n>>> [0, 102, 2]\r\n>>> [50265, 102, 50266]\r\n```\r\n\r\nIn other words, the tokenizer knows that the correct IDs for `<s>` and `</s>` are `0` and `2`, but when encoding an arbitrary string, it adds the new IDs `50265` and `50266` (which are not known to the model!).\r\n\r\nUsing this solution, the tokenizer does *not* add additional token IDs 50265 and up, because it recognizes them as existing already in IDs 0-3.\r\nThen, encoding a string using a tokenizer results in adding `0` and `2` as the `<s>` and `</s>` tokens.\r\nThe two lines of:\r\n\r\n```\r\nprint(tokenizer.convert_tokens_to_ids(['<s>', 'a', '</s>']))\r\nprint(tokenizer.encode('a'))\r\n```\r\nresult in the same output of `[0, 102, 2]`, as expected.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@ArthurZucker @ydshieh @hvaara @amyeroberts ",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 12,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2023-09-18T20:29:26Z",
        "closed_at": "2023-09-19T05:35:42Z",
        "merged_at": "2023-09-19T05:35:42Z",
        "body": "# What does this PR do?\r\n\r\nAs Amy mentioned this to me in one of my PR.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 52,
        "deletions": 9,
        "changed_files": 6,
        "created_at": "2023-09-18T19:28:03Z",
        "closed_at": "2023-10-03T08:25:10Z",
        "merged_at": "2023-10-03T08:25:10Z",
        "body": "\r\nThis pr addresses #25994 by adding tokenizer_kwargs as an input preprocessing parameter to the fill mask pipeline. \r\n\r\nAttn: @BramVanroy @Narsil.\r\n\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-09-18T19:22:24Z",
        "closed_at": "2023-10-05T07:17:37Z",
        "merged_at": null,
        "body": "Just doing some documents for easier to read\r\nI would like to cc @MKhalusova to review my code.",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2023-09-18T18:39:48Z",
        "closed_at": "2023-09-28T08:14:20Z",
        "merged_at": "2023-09-28T08:14:20Z",
        "body": "Hi,\r\nI create a concise document outlining the process of improving code readability by replacing 'assert' statements with 'raise' statements, along with the addition of documentation and 'logger.info' statements.\r\nI would like cc @stevhliu to review my PR.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2023-09-18T18:27:39Z",
        "closed_at": null,
        "merged_at": null,
        "body": "A bug in nn.Conv2d in PyTorch 2.0.1 on RoCm systems make the output of TVLT significantly diverge vs CPU / Nvidia GPUs. The issue is fixed on nightly and on 2.1.0 RC.\r\n\r\nSome slow tests may be affected as well, thus marking as draft for now. Adding relevant tensors / code to reproduce the issue just in case.\r\n[conv2d_mi210.zip](https://github.com/huggingface/transformers/files/12652047/conv2d_mi210.zip)\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 13,
        "deletions": 15,
        "changed_files": 4,
        "created_at": "2023-09-18T17:42:51Z",
        "closed_at": "2023-10-04T14:31:38Z",
        "merged_at": "2023-10-04T14:31:38Z",
        "body": "Hi,\r\nAs mentioned in issue #26069, I have created a new PR with the aim of modifying the block_size for all files. The goal is to set the block size = min(1024, config.max_position_embeddings). This change is intended to ensure synchronization and prevent errors that can occur when the block size exceeds the maximum position embeddings value.\r\nI would like to cc @sanchit-gandhi to review my PR, thank you so much",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-18T17:17:18Z",
        "closed_at": "2023-09-18T17:49:51Z",
        "merged_at": "2023-09-18T17:49:51Z",
        "body": "# What does this PR do?\r\n\r\nRebases and runs `make fix-copies` to fix the red CI on `main`",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2023-09-18T16:56:43Z",
        "closed_at": "2023-09-20T16:51:57Z",
        "merged_at": "2023-09-20T16:51:57Z",
        "body": "# What does this PR do?\r\n\r\nIt fixed a issue discovered during discussion of PR https://github.com/huggingface/transformers/pull/26152. \r\n\r\n> @ArthurZucker: the `ALL_LAYERNORM_LAYERS` should contain all the custom layer norm classes (from `transformers` modeling files) and should be updated if that is not the case\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2023-09-18T16:34:37Z",
        "closed_at": null,
        "merged_at": null,
        "body": "As per title.\r\n\r\nAPEX `FusedRMSNorm` initialize the returned tensor to the `input` dtype, which raises an error in case the model is set on fp16, where we may have `fp32` input to the layer norm layer:\r\n```\r\n  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/apex-0.1-py3.8-linux-x86_64.egg/apex/normalization/fused_layer_norm.py\", line 189, in fused_rms_norm_affine\r\n    return FusedRMSNormAffineFunction.apply(*args)\r\n  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/torch/autograd/function.py\", line 506, in apply\r\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\r\n  File \"/opt/conda/envs/py_3.8/lib/python3.8/site-packages/apex-0.1-py3.8-linux-x86_64.egg/apex/normalization/fused_layer_norm.py\", line 69, in forward\r\n    output, invvar = fused_layer_norm_cuda.rms_forward_affine(\r\n```\r\n\r\nComparing to [T5LayerNorm](https://github.com/huggingface/transformers/blob/820c46a707ddd033975bc3b0549eea200e64c7da/src/transformers/models/t5/modeling_t5.py#L247-L261) where the output is on the weight dtype. That is exactly what `MixedFusedRMSNorm` is for, see https://github.com/NVIDIA/apex/blob/52e18c894223800cb611682dce27d88050edf1de/apex/normalization/fused_layer_norm.py#L420 and https://github.com/NVIDIA/apex/blob/52e18c894223800cb611682dce27d88050edf1de/csrc/layer_norm_cuda.cpp#L205\r\n\r\nFor example, the test `pytest tests/test_pipeline_mixin.py::VisualQuestionAnsweringPipelineTests::test_small_model_pt_blip2 -s -vvvvv` fails when `apex` is available (and accelerate is installed). This error was never detected because the docker images used for testing do not have APEX installed (e.g. `nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04`)\r\n\r\nThis is an issue for the AMD CI as the image used `rocm/pytorch:rocm5.6_ubuntu20.04_py3.8_pytorch_2.0.1` has RoCm APEX installed by default.\r\n\r\n---\r\nSlightly out of topic: something I don't get is why [neither t5x](https://github.com/google-research/t5x/blob/ea66ec835a5b413ca9d211de96aa899900a84c13/t5x/examples/t5/layers.py#L445) nor transformers seem to recast to fp16 after the FFN. Due to the `keep_in_fp32` attribute, [this weight](https://github.com/huggingface/transformers/blob/820c46a707ddd033975bc3b0549eea200e64c7da/src/transformers/models/t5/modeling_t5.py#L284C14-L284C14) is always in fp32 and then fp32 is propagated in the model. t5x seem to do the same.\r\n\r\nRelated: https://github.com/huggingface/transformers/pull/26225",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 32,
        "deletions": 24,
        "changed_files": 2,
        "created_at": "2023-09-18T16:29:01Z",
        "closed_at": "2023-09-21T10:00:03Z",
        "merged_at": "2023-09-21T10:00:03Z",
        "body": "As per title, aligns the behavior of `PreTrainedModel.from_pretrained(..., torch_dtype=torch.float16)` when accelerate is installed and when it is not.\r\n\r\nPreviously,\r\n\r\n```python\r\nimport torch\r\nfrom transformers import AutoModelForSeq2SeqLM\r\n\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", torch_dtype=torch.float16)\r\n\r\nprint(model.encoder.block[0].layer[1].DenseReluDense.wo.weight.dtype)\r\n```\r\nwould print `torch.float16` when accelerate was not installed, and `torch.float32` when installed. Having different dtype depending on an external package being installed or not is bug prone. [As `accelerate` is a hard requirement](https://github.com/huggingface/transformers/blob/e4e55af79c9b3dfd15cc2224f8f5b80680d83f03/setup.py#L260), it could be also reasonable to simply raise an error in `from_pretrained` if using pytorch & accelerate is not installed.\r\n\r\nNote:\r\n```python\r\nfor name, param in model.named_parameters():\r\n    param = param.to(torch.float32)\r\n```\r\ndoes nothing out of the loop scope.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 8705,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-09-18T16:16:26Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR is a very-very-very work in progress effort to add Keras Core support, and to prepare for the transition to Keras 3.0. Because Keras Core is still a beta/preview, this PR is likely to be fairly slow and cautious, we don't want to lock ourselves into an API that may change under our feet!\r\n\r\nThe goal of this PR is to create a \"minimum viable port(duct)\" of our `tf.keras` code to `keras-core` to assess how difficult it will be to support Keras Core. Therefore, this port has the following properties:\r\n\r\n- Only loading from `safetensors` will be supported for now, so I don't have to support every possible combination of (weights_format, model_framework)\r\n- All mixins like `GenerationMixin` will be inherited directly from the framework that corresponds to the active `keras-core` framework. This means that `keras-core` classes will change their inheritance depending on which framework is live, which is risky but greatly reduces the amount of code I need to write.\r\n- We will mainly be supporting TF and JAX as Keras Core frameworks. All of our models have PyTorch code already, which will probably be more stable and better-tested than using Keras Core + PyTorch, so I doubt we'd see much usage there. In addition, TF and JAX fit Keras Core's assumptions much more naturally than PyTorch does.\r\n\r\nThe plan for this PR is:\r\n- [x] Create `modeling_keras_outputs.py` (ported from `modeling_tf_outputs.py`)\r\n- [ ] Create `modeling_keras_utils.py` (ported from `modeling_tf_utils.py`)\r\n- [ ] Port a single model to keras-core (probably BERT or DistilBERT)\r\n- [ ] Add model tests to ensure that outputs match \r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-18T16:09:56Z",
        "closed_at": "2023-09-22T06:56:54Z",
        "merged_at": "2023-09-22T06:56:54Z",
        "body": "# What does this PR do?\r\n\r\n@ArthurZucker loves it.",
        "comments": 8
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-09-18T15:54:23Z",
        "closed_at": "2023-09-18T17:58:02Z",
        "merged_at": "2023-09-18T17:58:02Z",
        "body": "# What does this PR do?\r\n\r\nFixes https://github.com/huggingface/transformers/pull/26183/files#r1328944428 by removing the debugging statements in the config docstring checker.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 35,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-09-18T13:42:37Z",
        "closed_at": "2023-09-18T14:08:57Z",
        "merged_at": "2023-09-18T14:08:57Z",
        "body": "This PR fixes the failing conversation pipeline tests in the CI. The causes were:\r\n\r\n1) `BlenderBotSmall` was missing a `default_chat_template`\r\n2) The old `Conversation` object distinguished between processed and unprocessed user inputs. The new object doesn't! This caused a couple of tests that were expecting the old behaviour to fail. `ConversationalPipeline` has already been updated to handle the new behaviour, so only a few test values needed to be changed.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 15,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-09-18T09:33:54Z",
        "closed_at": "2023-09-19T15:27:51Z",
        "merged_at": "2023-09-19T15:27:51Z",
        "body": "# What does this PR do?\r\n\r\nIn https://app.circleci.com/pipelines/github/huggingface/transformers/72920/workflows/33795bcf-6b08-4c2f-b040-aa54bb44b12f/jobs/921247/artifacts, \r\n\r\nwe have `errors.txt` but not `failures_short.txt`. But we only checked the file `failures_short.txt`, therefore the error is not detected, and showing all test passed.\r\n\r\nThis PR fixes this.",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 0,
        "deletions": 501,
        "changed_files": 5,
        "created_at": "2023-09-18T08:55:32Z",
        "closed_at": "2023-09-18T11:33:01Z",
        "merged_at": "2023-09-18T11:33:01Z",
        "body": "# What does this PR do?\r\n\r\nThis file is already removed in #25680 (we decided to use `no_doctested.txt` to explicitly exclude some files - all others should be doctested). \r\n\r\nHowever, #24085 added it back (which was a mistake of not having on top of the latest main at that time). This PR just removes this file again.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-18T08:32:05Z",
        "closed_at": "2023-09-18T11:31:59Z",
        "merged_at": "2023-09-18T11:31:59Z",
        "body": "# What does this PR do?\r\n\r\nNo need to doctest this file.",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 2,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-09-18T08:21:06Z",
        "closed_at": "2023-09-28T09:27:43Z",
        "merged_at": "2023-09-28T09:27:43Z",
        "body": "# What does this PR do?\r\n\r\nAdds `[r\"decoder\"]` to both `T5EncoderModel` and `LongT5EncoderModel`, as both models do not have any decoder layers and loading pretrained model checkpoints like `t5-small` will give warnings about keys found in the checkpoint but not in the model itself. To prevent this issue, `r\"decoder\"` has been added to `_keys_to_ignore_on_load_unexpected` for both model classes.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker @younesbelkada ",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 551,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-09-18T00:33:46Z",
        "closed_at": "2023-09-25T16:42:24Z",
        "merged_at": "2023-09-25T16:42:24Z",
        "body": "# What does this PR do?\r\n\r\nAdd Russian localization for README.\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests)",
        "comments": 5
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-17T19:58:23Z",
        "closed_at": "2023-09-18T22:45:12Z",
        "merged_at": "2023-09-18T22:45:12Z",
        "body": "# What does this PR do?\r\n\r\nThis PR fixes a typo in the docs, more specifically the model reference, since it it defined as `detector` and later referenced as predictor which is not defined. Settled on `detector`, since on example in other locale (`docs/source/ko/tasks/zero_shot_object_detection.md`) is `detector`.\r\n\r\n## Before submitting\r\n- [X] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 49,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-09-17T05:14:32Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# This PR fixes a tokenizer truncation bug.\r\n\r\nSetting max_length=0 or 1 leads to the following problem that should be fixed: \r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=False)\r\ntext = \"hello world hello world\"\r\ntokens = tokenizer(text, max_length=1, truncation=True, add_special_tokens=True)\r\nprint(\"tokens: \", tokens)\r\n\r\n\r\n>>> [ERROR|/root/data/code/transformers/src/transformers/tokenization_utils_base.py:3468] 2023-09-17 13:08:29,020 >> >>> We need to remove 4 to truncate the input but the first sequence has a length 4. \r\n>>> tokens:  {'input_ids': [1, 22172, 3186, 22172, 3186], 'attention_mask': [1, 1, 1, 1, 1]}\r\n```\r\n\r\nThis PR also make the length of tokens in consistent with fast-tokenizer. \r\n\r\nAlso added unit-test for this fix. \r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 333,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-09-16T13:21:18Z",
        "closed_at": "2023-09-25T17:24:45Z",
        "merged_at": "2023-09-25T17:24:45Z",
        "body": "<!-- PR\uc758 \uc81c\ubaa9\uc740 \"\ud83c\udf10 [i18n-KO] Translated `<your_file>.mdx` to Korean\" \uc73c\ub85c \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4 -->\r\n# What does this PR do?\r\n\r\nTranslated the `audio_classification.mdx` file of the documentation to Korean.\r\nThank you in advance for your review. \r\n\r\nPart of https://github.com/huggingface/transformers/issues/20179\r\n<!-- \uba54\uc778 \uc774\uc288\uc5d0 \uae30\ub85d\uc774 \ub0a8\uc544\uc694! \uac00\uc9dc\uc5f0\uad6c\uc18c \ub9ac\ud3ec\ub97c \uc0ac\uc6a9\ud574 \uc5f0\uc2b5\ud558\uc2e4\ub54c\ub294 \uc81c\uac70\ud574\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4! :smile: -->\r\n\r\n## Before reviewing\r\n- [x] Check for missing / redundant translations (\ubc88\uc5ed \ub204\ub77d/\uc911\ubcf5 \uac80\uc0ac)\r\n- [x] Grammar Check (\ub9de\ucda4\ubc95 \uac80\uc0ac)\r\n- [x] Review or Add new terms to glossary (\uc6a9\uc5b4 \ud655\uc778 \ubc0f \ucd94\uac00)\r\n- [x] Check Inline TOC (e.g. `[[lowercased-header]]`)\r\n- [ ] Check live-preview for gotchas (live-preview\ub85c \uc815\uc0c1\uc791\ub3d9 \ud655\uc778)\r\n\r\n## Who can review? (Initial)\r\n\r\n<!-- 1. \uc704 \uccb4\ud06c\uac00 \ubaa8\ub450 \uc644\ub8cc\ub41c \ub4a4\uc5d0\ub9cc \uac00\uc9dc\uc5f0\uad6c\uc18c \ud300\uc6d0\ub4e4\uc5d0\uac8c \ub9ac\ubdf0 \uc694\uccad\ud558\ub294 \uc544\ub798 \uc8fc\uc11d\uc744 \ub178\ucd9c\ud574\uc8fc\uc138\uc694! -->\r\nTeam PseudoLab, may you please review this PR? @0525hhgus, @KIHOON71, @sim-so, @gabrielwithappy, @HanNayeoniee, @wonhyeongseo, @jungnerd\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n## Who can review? (Final)\r\n\r\n<!-- 2. \uac00\uc9dc\uc5f0\uad6c\uc18c \ud300\uc6d0\ub4e4\uacfc \ub9ac\ubdf0\uac00 \ub05d\ub09c \ud6c4\uc5d0\ub9cc \ud5c8\uae45\ud398\uc774\uc2a4 \uc9c1\uc6d0\ub4e4\uc5d0\uac8c \ub9ac\ubdf0 \uc694\uccad\ud558\ub294 \uc544\ub798 \uc8fc\uc11d\uc744 \ub178\ucd9c\ud574\uc8fc\uc138\uc694! -->\r\n<!-- @sgugger, @ArthurZucker, @eunseojo May you please review this PR? -->",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 19,
        "changed_files": 3,
        "created_at": "2023-09-16T12:10:01Z",
        "closed_at": null,
        "merged_at": null,
        "body": "When Falcon has been ported to transformers, it appears that methods `_convert_to_rw_cache` and `_convert_cache_to_standard_format` were directly called in the model, which is NOT what is traditionally done in Transformers, with the precedent of Bloom: https://github.com/huggingface/transformers/blob/0a55d9f7376f72ad3ff296d4249840021b03bcc4/src/transformers/models/bloom/modeling_bloom.py#L853 & https://github.com/huggingface/transformers/blob/0a55d9f7376f72ad3ff296d4249840021b03bcc4/src/transformers/models/bloom/modeling_bloom.py#L949\r\n\r\n@Rocketknight1 I am wondering if it is fine to put back the cache reordering in `prepare_inputs_for_generation` & `_reorder_cache`? Having it in the modeling is a bit bad for people who want to rewrite their own generation, export the model, etc. It is also inconsistent with the information in `FALCON_INPUTS_DOCSTRING`.\r\n\r\nI noticed this working on the ONNX export, where it is not really meaningful to have those ops in the model itself. Another solution is to monkey patch transformers, but I believe this should be upstream.\r\n\r\nRelated: https://github.com/huggingface/transformers/issues/26097",
        "comments": 10
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-09-16T11:41:10Z",
        "closed_at": "2023-09-18T22:02:53Z",
        "merged_at": "2023-09-18T22:02:53Z",
        "body": "Fixed a few typos\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 6,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-09-16T11:23:46Z",
        "closed_at": "2023-09-18T22:41:16Z",
        "merged_at": "2023-09-18T22:41:16Z",
        "body": "fixed a few typos\r\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-09-16T02:42:38Z",
        "closed_at": "2023-10-04T12:57:11Z",
        "merged_at": "2023-10-04T12:57:11Z",
        "body": "# What does this PR do?\r\n\r\nThis PR intends to extend Transformers Trainer Class with NpuFusedAdamw optimizer for model training when using Ascend NPU.\r\n\r\nVerified with text-classification task:\r\n```bash\r\nexport TASK_NAME=sst2\r\n\r\npython run_glue.py \\\r\n  --model_name_or_path bert-base-cased \\\r\n  --task_name $TASK_NAME \\\r\n  --do_train \\\r\n  --do_eval \\\r\n  --max_seq_length 128 \\\r\n  --per_device_train_batch_size 32 \\\r\n  --learning_rate 2e-5 \\\r\n  --num_train_epochs 3 \\\r\n  --optim adamw_torch_npu_fused \\\r\n  --output_dir ./output\r\n```\r\nThe result of `train_samples_per_second` is as follows\r\n\r\n| Device            | `adamw_torch`(default) | `adamw_torch_npu_fused`  |\r\n| ----------------- | --------------------------------- | ------------------------ |\r\n| Ascend 910B     | 96.586                              | 149.876                   |\r\n\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerz and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\ncc @pacman100 and @muellerzr ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-09-15T21:38:40Z",
        "closed_at": "2023-09-15T21:45:54Z",
        "merged_at": null,
        "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nIn [PreTrainedModel.resize_token_embeddings](https://github.com/huggingface/transformers/blob/0a55d9f7376f72ad3ff296d4249840021b03bcc4/src/transformers/modeling_utils.py#L1068), the vocabulary size attributes are not correctly updated if `pad_to_multiple_of!=None`. This led to tensor size mismatches in CausalLM loss calculation.\r\n\r\nThis PR implements a hotfix for this issue. \r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerz and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-09-15T20:58:06Z",
        "closed_at": null,
        "merged_at": null,
        "body": "I was wondering can we do like this? `_custom_save_checkpoint` is that it provides more control and flexibility for customizing how checkpoints are saved. You can define your own naming conventions and include or exclude specific information in the checkpoint. This can be useful if you have specific requirements or want to integrate with external systems or tools that expect a certain checkpoint format. In contrast, `_tune_save_checkpoint` is designed specifically for use with Ray Tune and follows Ray Tune's conventions for managing checkpoints. It may be more convenient when we are using Ray Tune for hyperparameter tuning and need to manage checkpoints in a standardized way.\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker and @younesbelkada\r\n- vision models: @amyeroberts\r\n- speech models: @sanchit-gandhi\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- flax: @sanchit-gandhi\r\n- generate: @gante\r\n- pipelines: @Narsil\r\n- tensorflow: @gante and @Rocketknight1\r\n- tokenizers: @ArthurZucker\r\n- trainer: @muellerz and @pacman100\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @pacman100\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc and @younesbelkada\r\n\r\nDocumentation: @stevhliu and @MKhalusova\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- diffusers: [different repo](https://github.com/huggingface/diffusers)\r\n- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n\r\nMaintained examples (not research project or legacy):\r\n\r\n- Flax: @sanchit-gandhi\r\n- PyTorch: See Models above and tag the person corresponding to the modality of the example.\r\n- TensorFlow: @Rocketknight1\r\n\r\n -->\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-09-15T19:51:30Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nCurrently, `ClapFeatureExtractor`'s padding behaviour is unintuitive. Specifically, \r\n* It works with any value of argument `padding` without any check. \r\n* It uses non default padding strategy if `padding=True`.\r\n\r\nThis was discussed in more detail around [this](https://github.com/huggingface/transformers/issues/23648#issuecomment-1558586027) comment of issue #23648.\r\n\r\nThis PR closes #23648. \r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@sanchit-gandhi ",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 9,
        "deletions": 9,
        "changed_files": 3,
        "created_at": "2023-09-15T17:11:10Z",
        "closed_at": "2023-09-18T16:24:36Z",
        "merged_at": "2023-09-18T16:24:36Z",
        "body": "# What does this PR do?\r\n\r\nSmall style fix to stay consistent with the rest of the modelling code: `torch.nn -> nn`\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 23,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-09-15T15:27:52Z",
        "closed_at": "2023-09-18T14:58:39Z",
        "merged_at": "2023-09-18T14:58:39Z",
        "body": "FSMT shares weights across three layers.\r\n\r\nThe `tie_weights` method wasn't having the intended effect on all three (only on the input and output embeddings), leading to errors when converting the model to safetensors.\r\n\r\nSee https://huggingface.co/facebook/wmt19-en-de/discussions/7",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 55,
        "deletions": 25,
        "changed_files": 5,
        "created_at": "2023-09-15T13:59:27Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR adds an extra `chat_template_kwargs` attribute, so that the behaviour of templates can be modified without rewriting the whole thing. I factored out the messy LLaMA default system message stuff in a cleaner way using these, and I'm going to need it for the templates of some other checkpoints on the Hub as well!\r\n\r\nThis is implemented in a backward-compatible way, so no existing code should break because of it.",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 27,
        "deletions": 18,
        "changed_files": 7,
        "created_at": "2023-09-15T12:03:04Z",
        "closed_at": "2023-09-18T11:52:43Z",
        "merged_at": "2023-09-18T11:52:43Z",
        "body": "redirects should theoretically work, but still updating those repo references for clarity\r\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 140,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2023-09-15T11:38:58Z",
        "closed_at": "2023-09-21T15:52:48Z",
        "merged_at": "2023-09-21T15:52:48Z",
        "body": "# What does this PR do?\r\n\r\n> [Kaldi](https://kaldi-asr.org/doc/) is a toolkit for speech recognition, intended for use by speech recognition researchers and professionals. \r\n\r\nIts [torchaudio `fbank` implementation](https://pytorch.org/audio/stable/compliance.kaldi.html) is sometimes used in the library, notably in the [Audio Spectrogram Transformer](https://github.com/huggingface/transformers/blob/d70fab8b2062526e9c2c60196421a8bc96c7df03/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L97) and [SpeechToText](https://github.com/huggingface/transformers/blob/d70fab8b2062526e9c2c60196421a8bc96c7df03/src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py#L90) models.\r\n\r\nIt will also be used in the future [SeamlessM4T model](https://github.com/huggingface/transformers/pull/25693#pullrequestreview-1624417597).\r\n\r\nThis PR aims to port [the implementation](https://pytorch.org/audio/stable/_modules/torchaudio/compliance/kaldi.html#fbank) to `numpy`, directly in `audio_utils.py`.\r\n\r\nWhy is this important? It will reduce `transformers` dependence on `torchaudio` for some models.\r\n\r\nAtm, I enriched some of `audio_utils` methods and added tests to make sure it has the same results as `torchaudio`.\r\n\r\n\r\n\r\n## Before submitting\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\ncc @ArthurZucker and @sanchit-gandhi , what do you think of this feature ?",
        "comments": 3
    },
    {
        "merged": true,
        "additions": 310,
        "deletions": 24,
        "changed_files": 6,
        "created_at": "2023-09-15T08:07:43Z",
        "closed_at": "2023-09-20T04:56:16Z",
        "merged_at": "2023-09-20T04:56:16Z",
        "body": "# What does this PR do?\r\n1. Fixes certain bugs with checkpointing when using FSDP\r\n2. Adds tests for FSDP integration in Trainer.\r\n3. Different combination runs to check resuming from checkpoints work as expected.\r\n\r\nBelow we will run the different combinations of FSDP `SHARDING_STRATEGY` and `STATE_DICT_TYPE` for the `run_glue.py` transformers example\r\nInitial setup:\r\n```\r\ncd transformers\r\nexport CUDA_VISISBLE_DEVICES=0,1\r\nexport TASK_NAME=mrpc\r\n```\r\n\r\na.  **FULL_SHARD + FULL_STATE_DICT**\r\n\r\ni. command to run:\r\n```\r\ntorchrun --nnodes 1 --nproc-per-node 2 ./examples/pytorch/text-classification/run_glue.py --model_name_or_path bert-base-cased  --task_name $TASK_NAME  --do_train  --do_eval  --max_seq_length 128  --per_device_train_batch_size 16  --learning_rate 5e-5  --num_train_epochs 3  --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --lr_scheduler_type cosine --save_strategy \"epoch\" --evaluation_strategy \"epoch\" --logging_steps 1 --fsdp \"full_shard auto_wrap\"  --fsdp_transformer_layer_cls_to_wrap BertLayer --bf16\r\n```\r\n\r\nKill the process after epoch 1. Run the above command with --resume_from_checkpoint as below:\r\n```\r\ntorchrun --nnodes 1 --nproc-per-node 2 ./examples/pytorch/text-classification/run_glue.py --model_name_or_path bert-base-cased  --task_name $TASK_NAME  --do_train  --do_eval  --max_seq_length 128  --per_device_train_batch_size 16  --learning_rate 5e-5  --num_train_epochs 3  --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --lr_scheduler_type cosine --save_strategy \"epoch\" --evaluation_strategy \"epoch\" --logging_steps 1 --fsdp \"full_shard auto_wrap\"  --fsdp_transformer_layer_cls_to_wrap BertLayer --bf16  --resume_from_checkpoint /tmp/$TASK_NAME/checkpoint-115/\r\n```\r\n\r\niii. Plots of loss and learning rate:\r\n![Screenshot 2023-09-15 at 2 07 43 PM](https://github.com/huggingface/transformers/assets/13534540/672c3362-9f98-499d-9a22-0e49adc33c64)\r\n\r\nb. **SHARD_GRAD_OP + FULL_STATE_DICT**\r\nSame as above but with the following cmd arg `--fsdp \"shard_grad_op auto_wrap\"`\r\n\r\nPlots:\r\n![Screenshot 2023-09-15 at 2 09 29 PM](https://github.com/huggingface/transformers/assets/13534540/b1b069d2-9069-4bd5-9d1f-3c1047df853f)\r\n\r\nc. **FULL_SHARD + SHARDED_STATE_DICT**\r\n\r\ni. Here, we will need to use the accelerate launcher as the option to choose `SHARDED_STATE_DICT` is currently available via `accelerate config`. Below is the config file `fsdp_config.yaml`:\r\n\r\n```yaml\r\ncompute_environment: LOCAL_MACHINE\r\ndebug: false\r\ndistributed_type: FSDP\r\ndowncast_bf16: 'no'\r\nfsdp_config:\r\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\r\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\r\n  fsdp_forward_prefetch: false\r\n  fsdp_offload_params: false\r\n  fsdp_sharding_strategy: 1\r\n  fsdp_state_dict_type: SHARDED_STATE_DICT\r\n  fsdp_sync_module_states: true\r\n  fsdp_transformer_layer_cls_to_wrap: BertLayer\r\n  fsdp_use_orig_params: true\r\nmachine_rank: 0\r\nmain_training_function: main\r\nmixed_precision: bf16\r\nnum_machines: 1\r\nnum_processes: 2\r\nrdzv_backend: static\r\nsame_network: true\r\ntpu_env: []\r\ntpu_use_cluster: false\r\ntpu_use_sudo: false\r\nuse_cpu: false\r\n```\r\n\r\nii. command to run:\r\n```\r\naccelerate launch --config_file \"fsdp_config.yaml\" ./examples/pytorch/text-classification/run_glue.py --model_name_or_path bert-base-cased --task_name $TASK_NAME --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 16 --learning_rate 5e-5 --num_train_epochs 3 --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --lr_scheduler_type cosine --save_strategy \"epoch\" --evaluation_strategy \"epoch\" --logging_steps 1\r\n```\r\nKill the process after epoch 1. Run the above command with --resume_from_checkpoint as below:\r\n```\r\naccelerate launch --config_file \"fsdp_config.yaml\" ./examples/pytorch/text-classification/run_glue.py --model_name_or_path bert-base-cased --task_name $TASK_NAME --do_eval --max_seq_length 128 --per_device_train_batch_size 16 --learning_rate 5e-5 --num_train_epochs 5 --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --lr_scheduler_type cosine --save_strategy \"epoch\" --evaluation_strategy \"epoch\" --logging_steps 1  --resume_from_checkpoint /tmp/$TASK_NAME/checkpoint-115/\r\n```\r\n\r\niii. Plots:\r\n![Screenshot 2023-09-15 at 2 14 16 PM](https://github.com/huggingface/transformers/assets/13534540/982cae5e-fb50-4a2f-a0da-3e0cdc0c6977)\r\n\r\nd. **SHARD_GRAD_OP + SHARDED_STATE_DICT**\r\nJust run the `accelerate config` command and choose `SHARD_GRAD_OP` Sharding strategy and get `fsdp_config.yaml` similar to the above case. The rest is the same.\r\n\r\nPlots:\r\n![Screenshot 2023-09-15 at 2 16 02 PM](https://github.com/huggingface/transformers/assets/13534540/4199460c-ad57-44ff-9143-84da6ecd6254)",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 46,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-09-15T06:44:41Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nQ: How to train models in trainer.py with megatron_lm_plugin\uff1f\r\n\r\nI think there are 3 main steps that need to be done\uff1a\r\n- support megatron-lm plugin in trainer of transformers.\r\n- give compatible api for transformers in accelerate.\r\n- deveplot a tool to convert checkpoint in Megatron\r\n\r\nthis pr is for the first step.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#start-contributing-pull-requests),\r\n      Pull Request section?\r\n- [x] discussed via the forum [50804](https://discuss.huggingface.co/t/how-to-run-trainer-py-with-megatron-lm-plugin/50804)\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@pacman100 \r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 57,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2023-09-15T06:14:36Z",
        "closed_at": "2023-10-09T07:58:05Z",
        "merged_at": "2023-10-09T07:58:04Z",
        "body": "# What does this PR do?\r\n\r\nThis PR converts the DINOv2 checkpoints of image classification on ImageNet-1k ([source](https://github.com/facebookresearch/dinov2#pretrained-heads---image-classification)).\r\n\r\nIt also improves the doc test for `Dinov2ForImageClassification`.\r\n\r\nFixes #26167",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-09-15T04:08:37Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# What does this PR do?\r\nThis PR adds the `'device_map': \"auto\"` functionality for BERT Models for ease in multi-GPU training.\r\n\r\nFixes #25296 \r\n\r\n## Who can review?\r\n@younesbelkada",
        "comments": 6
    }
]