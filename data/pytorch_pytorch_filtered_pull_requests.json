[
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-18T09:32:52Z",
        "closed_at": null,
        "merged_at": null,
        "body": "May relate to: https://github.com/pytorch/pytorch/pull/82695 ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-18T09:26:32Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Other test files that rely on `max_autotune` mode being enabled already conditionalise the UT suite on this condition (e.g. test_select_algorithm). Proposing to add this condition for test_max_autotune. \r\n\r\nCurrently we are observing failures on these UTs on the ROCm runners but using MI200+ these tests will pass again (context: https://github.com/pytorch/pytorch/pull/111381#issuecomment-1768048732)\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-18T08:00:17Z",
        "closed_at": null,
        "merged_at": null,
        "body": "cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-18T07:38:50Z",
        "closed_at": null,
        "merged_at": null,
        "body": "I want to create a quant tensor through `PerTensorAffineQuantizer`. But I found that it will throw error because of the lake of judgment for PrivateUse1.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-18T07:27:28Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #111222\r\nThis pull request adds tests for factory functions that create tensors with a strided layout. The tests are added to the `test_ops.py` file and check the behavior of the `empty`, `zeros`, `ones`, and `rand` factory functions when used with the `layout=torch.strided` argument.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 48,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-18T06:24:11Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111461\n\nAs titled, when calling get_dim_groups, DeviceMeshVariable should be\nde-sugared to ProcessGroup Variable in dynamo\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng\n\nDifferential Revision: [D50400451](https://our.internmc.facebook.com/intern/diff/D50400451)",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 11,
        "changed_files": 1,
        "created_at": "2023-10-18T05:52:18Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1041,
        "deletions": 50,
        "changed_files": 15,
        "created_at": "2023-10-18T05:47:20Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111459\n* #111402\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 30,
        "deletions": 31,
        "changed_files": 1,
        "created_at": "2023-10-18T03:27:06Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Differential Revision: D50348807\n\n\n\n\ncc @avikchaudhuri @gmagogsfm @tugsbayasgalan",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-18T02:26:24Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111453\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 77,
        "changed_files": 3,
        "created_at": "2023-10-18T01:50:01Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 170448,
        "changed_files": 313,
        "created_at": "2023-10-18T01:18:22Z",
        "closed_at": null,
        "merged_at": null,
        "body": "removing nvfuser code base.\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2023-10-18T00:54:24Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary: see Shiyan's design doc for ATM TS publish weights dedupe https://fb.quip.com/HnUVAjUMaXMQ\n\nTest Plan: tested in N4454041 after D50341352 that multiforward method is working for ts model\n\nDifferential Revision: D45750812\n\n\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-18T00:44:30Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111445\n\n\n\ncc @avikchaudhuri @gmagogsfm @zhxchen17",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-18T00:19:52Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/_update-commit-hash.yml).\nUpdate the pinned vision hash.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 137,
        "deletions": 2,
        "changed_files": 4,
        "created_at": "2023-10-17T23:58:42Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Adds support for torch.cummin and torch.cummax to the MPS backend.\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-17T23:55:32Z",
        "closed_at": null,
        "merged_at": null,
        "body": "- 2.7.2 version + few ROCm related commits\r\n\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 81,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-10-17T22:55:08Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111440\n\r\nThis PR supports sym_ite. This is useful for converting SymBool to SymInt in e.g. #109916. Internally, it uses sympy.Piecewise. We cannot use sympy.ITE because it expects the arguments and output all to be boolean type but we want return SymInt type when converting a SymBool to SymInt. So we use sympy.Piecewise to denote the symbolic relationship. \r\n\r\nNote that this pr uses the range analysis for sympy.Piecewise implemented in https://github.com/pytorch/pytorch/blob/main/torch/utils/_sympy/value_ranges.py. \r\n\r\nTest Plan:\r\nSee added test.\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 58,
        "deletions": 111,
        "changed_files": 30,
        "created_at": "2023-10-17T22:53:03Z",
        "closed_at": "2023-10-18T04:54:55Z",
        "merged_at": null,
        "body": "<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at 21e87dc</samp>\n\n> _We're sailing on the sea of code, with warnings to avoid_\n> _We use the `C10_UNUSED` macro for variables unexploited_\n> _We heave and ho and pull and push, and make the code more neat_\n> _We sing this shanty as we go, to keep us in good spirits_\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-17T22:41:41Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary:\nWhen raising an exception here this causes pybind11's dispatcher to kick in, which causes aiplatform's logic to kick in (aiplatform::error_reporting::util::printAddressesWithBestEffortLocationInfo), which ultimately uses `folly::symbolizer::Symbolizer::symbolize` for building up the stack trace.  In 3.8 this uses about 3.62% of the CPU time per pyperf (https://fburl.com/scuba/pyperf_experimental/on_demand/oi554uvy).  In Cinder 3.8 for some reason this is worse - using 5.94% of the CPU.\n\nThis exception is happening when doing a hasattr() on `prims` for things like `bitwise_left_shift` which don't exist: https://www.internalfb.com/code/fbsource/[2d695f650d00]/fbcode/caffe2/torch/_inductor/lowering.py?lines=590\n\nThat exception is ultimately going to be swallowed anyway, and the stack trace has no meaningful value.  Furthermore because this is kind of an expected outcome in the code versus some random C++ exception the stack trace is less valuable as well.\n\nThis changes this to return a (None, None) on the failure case instead of returning a valid op/overload list, avoiding the exception, and reclaiming the 3.62%-5.94% of time.\n\nTest Plan: Existing CI and perf run: https://fburl.com/scuba/pyperf_experimental/on_demand/oi554uvy\n\nDifferential Revision: D50018789\n\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 96,
        "deletions": 16,
        "changed_files": 5,
        "created_at": "2023-10-17T22:35:33Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Reland https://github.com/pytorch/pytorch/pull/108115\r\n\r\nThe main fix is to disallow nop nodes to be included in foreach scheduler nodes\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 261,
        "deletions": 104,
        "changed_files": 4,
        "created_at": "2023-10-17T21:23:45Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Reduce number of backend compiler invocations and number of compiled artifacts\r\n\r\nFixes: https://github.com/pytorch/pytorch/issues/111003\r\n\r\nTODO:\r\n1. Handle backedges in nested graph breaks properly (now we just killswitch to eager as long as one of the outer functions has a backedge. Might be a little too strict - let's try to accomodate backedges in outer functions if possible).\r\n2. May not handle certain edge-cases properly, need to scrutinize.\r\n- [ ] Test for bad interaction with generator - e.g. generator calling into function with graph breaks.\r\n\r\ncc @voznesenskym @jansel @yanboliang ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-17T21:07:21Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Bumps [urllib3](https://github.com/urllib3/urllib3) from 2.0.6 to 2.0.7.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/releases\">urllib3's releases</a>.</em></p>\n<blockquote>\n<h2>2.0.7</h2>\n<ul>\n<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses. (GHSA-g4mx-q9vg-27p4)</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/urllib3/urllib3/blob/main/CHANGES.rst\">urllib3's changelog</a>.</em></p>\n<blockquote>\n<h1>2.0.7 (2023-10-17)</h1>\n<ul>\n<li>Made body stripped from HTTP requests changing the request method to GET after HTTP 303 &quot;See Other&quot; redirect responses.</li>\n</ul>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/56f01e088dc006c03d4ee6ea9da4ab810f1ed700\"><code>56f01e0</code></a> Release 2.0.7</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/4e50fbc5db74e32cabd5ccc1ab81fc103adfe0b3\"><code>4e50fbc</code></a> Merge pull request from GHSA-g4mx-q9vg-27p4</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/80808b04bfa68fbd099828848c96ee25df185f1d\"><code>80808b0</code></a> Fix docs build on Python 3.12 (<a href=\"https://redirect.github.com/urllib3/urllib3/issues/3144\">#3144</a>)</li>\n<li><a href=\"https://github.com/urllib3/urllib3/commit/f28deff1cf162c673b50d88d3552e91bda6d68a8\"><code>f28deff</code></a> Add 1.26.17 to the current changelog</li>\n<li>See full diff in <a href=\"https://github.com/urllib3/urllib3/compare/2.0.6...2.0.7\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=2.0.6&new-version=2.0.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/pytorch/pytorch/network/alerts).\n\n</details>",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 177,
        "deletions": 12,
        "changed_files": 7,
        "created_at": "2023-10-17T20:59:44Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111434\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 49,
        "deletions": 49,
        "changed_files": 17,
        "created_at": "2023-10-17T20:11:01Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Test Plan:\nnumpy.bool is long deprecated and removed starting numpy-1.20.0 [1]. This replaces all references with equivalent `bool` type using the following oneliner:\n```\nrg -l 'np\\.bool' caffe2 | grep '\\.py$' | xargs perl -pi -e 's,\\bnp\\.bool\\b,bool,'\n```\n1. https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\nDifferential Revision: D50372711\n\n\n\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 63,
        "deletions": 14,
        "changed_files": 7,
        "created_at": "2023-10-17T19:30:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "For synchronous ops (i.e. `asyncOp = False`), we don't want to record streams because we know that the NCCL stream will join back to the \"current\" stream right after this op. So we might just as well keep the stream ownership of the input/output tensors unchanged. The benefit would be that the allocation/free of the tensors would look deterministic to the \"current\" stream so that the caching allocator can reuse memory pool for this stream in a clever way. \r\n\r\nTo prevent the input/output tensors from being recycled by python, we rely on the stashing mechanism in ProcessGroupNCCL (which can be also turned on by setting `TORCH_NCCL_AVOID_RECORD_STREAMS=1`).\r\n\r\nThis mechanism change is for libraries like FSDP which uses `all_gather_into_tensor` and `reduce_scatter_tensor` in a synchronous way and which cannot set `TORCH_NCCL_AVOID_RECORD_STREAMS=1` for their users. And therefore, this change is limited to these two collectives for now.\r\n\r\nCc: @awgu @janeyx99 @albanD ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-17T19:08:23Z",
        "closed_at": "2023-10-18T07:24:08Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111430\n* #111428\n\nReported in https://github.com/pytorch/pytorch/issues/111268",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-17T18:58:41Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/_update-commit-hash.yml).\nUpdate the pinned vision hash.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 122,
        "deletions": 108,
        "changed_files": 6,
        "created_at": "2023-10-17T18:52:49Z",
        "closed_at": "2023-10-18T02:19:56Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111430\n* __->__ #111428\n\r\n`test_typing.py` was written to use `pytest` in https://github.com/pytorch/pytorch/pull/54234 which unfortunately rendered it incompatible with run_test.py, and therefore it was not running in CI all this time.\r\n\r\nIn this PR, same functionality is re-written using unittest framework, and `parametrize` from `torch.testing._internal._common_utils`.\r\n\r\nValid `test_typing.py` with ufmt\r\n\r\nDisable `fail/bitwise_ops.py` and `pass/jit.py` as it regressed at some point as well as one of examples in `namedtuple.py` as `torch.linalg.qr` type is no longer revealed correctly.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-17T18:16:51Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #7541",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 142,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-10-17T17:23:23Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary:\nZion-4s core has poor perf when it comes to reading the large tensor (e.g. 300G), no matter for manifold downloading or reading from files. In this diff, I changed the getRecord function from single thread to multiple threads by passing multiple readers to getRecord function and access the same record at different chunks with different readers.\nWe control the number of additional reader with the`sigrid_model_manager_additional_reader` flag. The default value is 0. When `additional_reader=2`, we allocate `2` extra read client threads.\n\nProfiler Results:\n**multi readers**\nods counter:\nhttps://fburl.com/ods/s12q1t6j\nstrobelight:\nhttps://fburl.com/strobelight/r1jj17qs\nhttps://fburl.com/strobelight/702j72k8\ndownload:\nhttps://fburl.com/scuba/download_client_stats/5fh7nlu7\n {F1118644555}\n**single reader**\nods counter:\nhttps://fburl.com/ods/m9sv2sp0\nstrobelight:\nhttps://fburl.com/strobelight/emc9q6z0\nhttps://fburl.com/strobelight/mn23ctda\ndownload:\nhttps://fburl.com/scuba/download_client_stats/xsl3pq8y\n {F1118644696}\n\nTest Plan:\n**unit test**\n```\nbuck2 run @//mode/dev //caffe2/caffe2/serialize:inline_container_test\n```\n\n**Local model loading**\n```\nSIGRID_ADDITIONAL_READER=4 SIGRID_MAX_RSS_SIZE_BYTES=751619276800 PYTORCH_PREDICTOR_ENABLE_XL_FORMAT_V2=true PYTORCH_PREDICTOR_ENABLE_XL_FORMAT_V2_INPLACE_LOADING=true GIF_LOAD_LOCAL_NET=true RUN_D2H_TOGETHER_WITH_EXECUTION_STREAM=true EVENT_POOL_ENABLE_BLOCKING_SYNC=false ENABLE_DEPLOY_INPLACE_LOADING=true TGIF_REPLICATE_MERGE_BY_TEMPFILE=true USE_STATIC_PATH=1 USE_ALL_TO_ONE_OP=false MAX_NUM_ADS=10240 REQUEST_BATCHING_PARAM_OVERRIDE=\"max_batch_size|${MAX_NUM_ADS};batch_time_us|50000\" NUM_DEPLOY_INTERPRETER=32 NUM_DESER_AND_REMOTE_RO_CPU_WORKERS=20 MODULE_NUM_WORKERS_PER_GPU='merge|8;remote|0' MODEL_ID=962580335 SNAPSHOT_ID=0 SERVER_PORT=7456 CUDA_VISIBLE_DEVICES_FOR_PREDICTOR=\"4,5,6,7\" CPU_NUMA_NODES_FOR_PREDICTOR=\"2,3\" ENABLE_THRIFT_WARMUP=false hpc/inference/scripts/gif/prospector/1_cards/launch_gpu_sigrid_predictor_task_0.sh 2>&1 | tee predictor.txt\n```\nLoad from local: P850152856\nLoad from manifold: P850716434\n\n**Vanguard model loading**\n```\nVANGUARD_ENV=dev vg serving-test -m 959387101 -s 0 --spec ~/fbsource/fbcode/feed_ranking_infra/vanguard/specs/umia_hstu_dev_xlv2.yaml 2>&1 | tee tmp.txt\n```\n1TB model 959387101\nbaseline: 25min, P844194768\npkg a44e273: 9min, P848107231\n\n\nProd model 960881812\nbaseline: 15min\npkg a44e273: 5min, P850724551\n\n**Inference eval**\n```\nVANGUARD_ENV=dev vg predictor launch -m 960881812 -s 602 --spec ~/fbsource/fbcode/feed_ranking_infra/vanguard/specs/umia_hstu_dev_xlv2.yaml 2>&1 | tee tmp.txt\npkg_version: a44e273\n\n## mvai_umia_hstu_fbr:recurring_eval\nhg checkout 1df2db0a09e755e80e7b3a7943f49c1f5e71567b\nlight -d ~/fbsource/fbcode/minimal_viable_ai -d ~/fbsource/fbcode/hpc/ ~/fbsource/fbcode/minimal_viable_ai/sandbox/eval/predictor_eval.py --model-entity-id 960881812 --snapshot 602 --eval-config umia/fbr_hstu_gif_ts_sampling_diversity_disable_both --tier feed.sigrid.predictor.benchmark.vg_tissue030_dev --num-requests 100000 --eval-ts 2023-10-10+11:00:99 2>&1 | tee eval.txt\n```\nbaseline: https://fburl.com/mlhub/yvmx9ald\n> I1010 09:11:52.131 4846 predictor_eval.py:558 [Rank 0] ## Eval Results: {\"recall@1/vvp100\": 0.013800769113004208, \"recall@5/vvp100\": 0.049447201192379, \"recall@20/vvp100\": 0.11355383694171906, \"recall@100/vvp100\": 0.17039309442043304, \"recall@200/vvp100\": 0.25405290722846985, \"recall@500/vvp100\": 0.4610816240310669, \"recall@1000/vvp100\": 0.637019693851471, \"recall@1250/vvp100\": 0.6741645932197571, \"num_interactions/vvp100\": 83826.0}\n\npkg: a44e273 with multi-thread: P850634507\n\n> I1010 11:15:36.873 4038353 predictor_eval.py:558 [Rank 0] ## Eval Results: {\"recall@1/vvp100\": 0.01417488232254982, \"recall@5/vvp100\": 0.050109997391700745, \"recall@20/vvp100\": 0.1138257384300232, \"recall@100/vvp100\": 0.17026753723621368, \"recall@200/vvp100\": 0.25343313813209534, \"recall@500/vvp100\": 0.4619934856891632, \"recall@1000/vvp100\": 0.6371392011642456, \"recall@1250/vvp100\": 0.6746780872344971, \"num_interactions/vvp100\": 93684.0}\n\nDifferential Revision: D49969050\n\n\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2023-10-17T17:10:01Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary: In AOTInductor ABI Compatible-mode, we don't serialize missing args with default value.\r\n\r\nTest Plan: buck2 run mode/dev-nosan deeplearning/aot_inductor/test:test_custom_ops\r\n\r\nDifferential Revision: D50345729\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 371,
        "deletions": 90,
        "changed_files": 4,
        "created_at": "2023-10-17T15:41:55Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111421\n\nSummary: Add support for caching graphs that have tensor args with symbolic shapes. The high-level appraoch is to serialize guards with the on-disk cached object and validating those guards pass before serving a cached object.\n\nTest Plan: New unit tests\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 43,
        "changed_files": 4,
        "created_at": "2023-10-17T09:39:33Z",
        "closed_at": "2023-10-17T13:48:00Z",
        "merged_at": null,
        "body": "As discussed in #111354 , meaning of `_prefetched` has slowly changed. This PR renamed it to better suit its new role.\r\n\r\nI also moved all mutations to `_unsharded` into `unshard_if_not_yet` and `reshard`, this should make our life easier regarding managing the flag and reasoning about the code.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 245,
        "deletions": 491,
        "changed_files": 20,
        "created_at": "2023-10-17T04:33:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111415\n* #111306\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 193,
        "deletions": 4,
        "changed_files": 6,
        "created_at": "2023-10-17T02:38:42Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111413\n\nSummary: When doing quantization int_mm -> mul or int_mm -> mul ->\nto(dtype) is an extremely common op pattern which is currently not\nhandled well by inductor. Ideally, since the output of\nint_mm has dtype int32 we'd prefer to only realize a smaller dtype like\nbf16 or float16. Currently inductor doesn't have a way to force this, in\nmany cases the mul gets fused with a bunch of subsequent pointwise\nops from the dequant creating an increase in memory overhead and a general\nslowdown compared to the fused version.\n\nTest Plan: python test/inductor/test_pattern_matcher.py -k\n\"int_mm_mul\"\n\npython minified_7_nodes.py\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-17T02:36:41Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Finally we are able to cover all c10 CUDA files.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 224,
        "deletions": 5,
        "changed_files": 8,
        "created_at": "2023-10-17T02:35:10Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This fixes the majority of the slowness from https://fb.workplace.com/groups/1405155842844877/permalink/7491314274228973/. In particular, the type of example that suffers the most perf-wise in AOTAutograd looks like this:\r\n```\r\n@torch.compile\r\ndef f(x):\r\n    intermediate = x.mul(2)\r\n    outs = intermediate.unbind(0)\r\n    return *outs\r\n\r\nx = torch.randn(50, 50, requires_grad=True)\r\nouts = f(x)\r\nsum(outs).sum().backward()\r\n```\r\n\r\nThere are 50 output tensors in the above function, that all alias each other. AOTAutograd will dutifully exercise its intermediate base [logic](https://github.com/pytorch/pytorch/blob/main/torch/_functorch/aot_autograd.py#L294), and try to regenerate the aliases outside of the compiled `autograd.Function` at runtime, to ensure that the autograd engine is aware of the aliasing.\r\n\r\nIn this case, this will result in **50 AsStridedBackward nodes in the backward**, because we will fall back to using as_strided to generate each of those 50 outputs. The current PR as is (somewhat unsafely) ensures that the backward graph consists of a single `UnbindBackward`, or a call to `aten.cat()`.\r\n\r\nI left a long comment in the code describing the situation, but the core idea is that **autograd does not let you mutate grad_fn of tensor aliases that come from multi-output views**. So if we have `k` outputs that alias each other, but `k-1` of them are aliases that came from multi-output views, then in eager mode, it would not be possible to mutate one of the aliases in a way that would change the grad_fn of any of the other aliases, without causing an error in the backward. So the claim I'm making is that if we hide this aliasing from the autograd engine, then it is impossible for the user to perform any mutations that would cause autograd metadata to diverge between torch.compile and eager in a way that isn't an error in eager mode.\r\n\r\nTo be fair, I think that taking the approach outlined in https://docs.google.com/document/d/1DlfFq8TKbuAn2zyJxLfoW-X1qkkm5PLdHFtySo03QAk/edit would also help us avoid the as_strided calls in this particularly egregious case, **and** keep the autograd error messages. This relies on both pre-dispatch functionalization being fully hardened **and** adding some pretty invasive changes to AOTAutograd though, and is probably at least several months out.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111411\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 71,
        "deletions": 73,
        "changed_files": 1,
        "created_at": "2023-10-17T00:45:22Z",
        "closed_at": "2023-10-17T21:12:42Z",
        "merged_at": null,
        "body": "Reorganize the priority inside of ```VariableBuilder._wrap```:\r\n* is_allowed returning True -> TorchVariable\r\n* skipfiles.check returning True -> SkipFilesVariable\r\n* UserFunctionVariable/UserMethodVariable (This is means both is_allowed and skipfiles.check returning False, then inlining by default)\r\n* UserDefinedClassVariable\r\n* UserDefinedObjectVariable (the ultimate default value)\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-17T00:19:35Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/_update-commit-hash.yml).\nUpdate the pinned vision hash.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 8,
        "changed_files": 4,
        "created_at": "2023-10-17T00:18:24Z",
        "closed_at": "2023-10-17T21:01:45Z",
        "merged_at": null,
        "body": "Summary: call_spec arg is not used anymore.\n\nTest Plan: CI\n\nReviewed By: SherlockNoMad, tugsbayasgalan\n\nDifferential Revision: D50335365\n\n\n\n\ncc @avikchaudhuri @gmagogsfm @tugsbayasgalan",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 19,
        "changed_files": 7,
        "created_at": "2023-10-17T00:07:27Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Applies clang-tidy to more aten/src/ATen/core/* files\r\n\r\n\r\n\n\ncc @mcarilli @ptrblck @leslie-fang-intel @jgong5",
        "comments": 15
    },
    {
        "merged": false,
        "additions": 47,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-17T00:04:44Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111405\n* #111125\n\nSummary: current issue with minified example, i think i fixed the Rocm\nissue though.\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-10-16T23:30:35Z",
        "closed_at": null,
        "merged_at": null,
        "body": "The purpose of this pr is as titled. Because of some misusage of ghstack, ghimport, and export to github from internal, the stack of https://github.com/pytorch/pytorch/pull/111092 is a mess. I'll try to land them one by one. This is a replacement for #111092 and #111400.\r\n\r\n\r\ncc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1260,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2023-10-16T23:14:23Z",
        "closed_at": "2023-10-17T06:33:26Z",
        "merged_at": null,
        "body": "This is a reland for #110914, #111327 and #111390\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2023-10-16T23:11:04Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111459\n* __->__ #111402\n\nRight now, memory ops are being lowered to strings partly in\nscheduler.codegen() and partly in wrapper.codegen(). But that makes\nstatic memory planning (which is done entirely in `wrapper.codegen()`)\ndifficult to implement as information is \"lost\" by that point.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 55,
        "deletions": 283,
        "changed_files": 16,
        "created_at": "2023-10-16T22:32:12Z",
        "closed_at": "2023-10-16T23:16:45Z",
        "merged_at": null,
        "body": "Summary:\nOriginal commit changeset: 96813f0fac68\n\nOriginal Phabricator Diff: D50161780\n\nThis breaks the integration test on T166457344\n\nTest Plan: Sandcastle.\n\nDifferential Revision: D50344243\n\n\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-10-16T22:07:56Z",
        "closed_at": "2023-10-16T23:38:06Z",
        "merged_at": null,
        "body": "Summary:\nX-link: https://github.com/pytorch/executorch/pull/964\n\n\n\nAs titled.\n\nTest Plan:\nexisting tests.\n\n\ncc avikchaudhuri gmagogsfm zhxchen17 tugsbayasgalan\n\nimported-using-ghimport\n\nDifferential Revision: D50232731\n\nPulled By: ydwu4\n\n\n\n\ncc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 212,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-16T21:39:25Z",
        "closed_at": "2023-10-17T17:59:49Z",
        "merged_at": null,
        "body": "Add two unit tests:\r\n\r\n1. HSDP checkpoint unit test \r\n2. HSDP FSDP checkpoint conversion unit test",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 96,
        "deletions": 45,
        "changed_files": 6,
        "created_at": "2023-10-16T21:34:34Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111397\n\ntriton_meta is intended to be passed directly to triton. Previous we were also putting other metadata into triton_meta; but we should split out the other metadata into a separate dict to avoid possible conficts in the future.\n\nThis PR splits out triton_meta and inductor_meta so we have a place to put additional metadata that isn't intended to be passed to triton.\n\nTests - wait for CI\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 69,
        "deletions": 44,
        "changed_files": 5,
        "created_at": "2023-10-16T21:07:44Z",
        "closed_at": "2023-10-17T18:53:46Z",
        "merged_at": null,
        "body": "This is a reland of #110567 with additional fbcode fixed. \r\n\r\nSummary:\r\nIn ABI compatible mode, We always need op_overload.schema for FallbackKernel.\r\n\r\nApproved by: https://github.com/jansel\r\n\r\nTest Plan: contbuild & OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/37a02659921490d85b2b0712ad52b924e0c431cd\r\n\r\nDifferential Revision: D50339346\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 50,
        "deletions": 20,
        "changed_files": 22,
        "created_at": "2023-10-16T20:55:43Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Updated with latest master and applied approach to more locations as #102868\r\n\r\nIf you like I can put it to one file per PR; as the TensorFlow and Keras people requested when I sent similar PRs.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 41,
        "deletions": 23,
        "changed_files": 3,
        "created_at": "2023-10-16T20:52:13Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes https://github.com/pytorch/test-infra/issues/4516.  As this is not part of trunk, it won't block regular merge.  On the other hand, we can still add `ciflow/rocm` to run it on PR.\r\n\r\n~~I'll add an auto label rule for this after this is merged and the label becomes available~~ Here it is https://github.com/pytorch/test-infra/pull/4647\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 702,
        "changed_files": 17,
        "created_at": "2023-10-16T20:47:29Z",
        "closed_at": "2023-10-16T23:03:08Z",
        "merged_at": null,
        "body": "Summary:\nThis diff is reverting D50250526\nD50250526: Reland #2 \"[C10] PG observability hooks. (#108815, #110907)\" (#111072) by wconstab has been identified to be causing the following test or build failures:\n\nTests affected:\n- [cogwheel:cogwheel_ig_clips_tab_derived_feature_importance#test_ig_clips_tab_derived_feature_importance](https://www.internalfb.com/intern/test/844425021976403/)\n\nHere's the Multisect link:\nhttps://www.internalfb.com/multisect/3290230\nHere are the tasks that are relevant to this breakage:\n\nWe're generating a revert to back out the changes in this diff, please note the backout may land if someone accepts it.\n\nIf you believe this diff has been generated in error you may Commandeer and Abandon it.\n\nTest Plan: NA\n\nReviewed By: amrshennawi\n\nDifferential Revision: D50299914\n\n\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 2,
        "changed_files": 5,
        "created_at": "2023-10-16T20:44:55Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Currently, the only way ProcessGroupNCCL shuts down its background threads and aborts all communicators is via the destructor.\r\n\r\nHowever, given how python GC works and code holding references to the PG in multiple places, in practice calling `destroy_process_group` doesn't actually end up invoking the destructor.\r\n\r\nAs a result, in this PR I'm adding a explicit close method to that users can call to cleanup all resources.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-16T20:40:24Z",
        "closed_at": "2023-10-17T01:09:23Z",
        "merged_at": null,
        "body": "Updates RUFF to the latest and greatest version\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 18,
        "changed_files": 1,
        "created_at": "2023-10-16T20:29:51Z",
        "closed_at": "2023-10-16T23:54:22Z",
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 73,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-16T20:07:31Z",
        "closed_at": "2023-10-17T18:13:06Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111389\n* #111388\n\nSummary:\n\nThis PR adds in torch.compile support for semi-structured sparsity,\nusing the subclass tracing @bdhirsh added.\n\nBased on wether we are using cuSPARSELt or CUTLASS, we return a\ndifferent representation of the inner tensors.\n\nTest Plan:\n```\npython test/test_sparse_semi_structured.py -k compile\n```\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 139,
        "deletions": 61,
        "changed_files": 3,
        "created_at": "2023-10-16T20:07:27Z",
        "closed_at": "2023-10-16T20:19:52Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111389\n* __->__ #111388\n\nSummary:\n\nCurrently we have shape constraints in semi-structured sparsity for both\nCUTLASS and cuSPARSELt\n\nThese shape constraints unfortunately apply to both the dense and sparse\nmatrices in sparsedense matmul.\n\nThis PR adds in support for calling `F.pad` in order to pad dense\nmatrices to the right size with zeros and then pull out the\ncorresponding rows from the resultant result matrix.\n\nWe also throw a warning in this case.\nThe tests have also been updated to take in a dense_input_shape\nparameter.\n\nTest Plan:\n```\npython test/test_sparse_semi_structured.py\n```\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-16T20:02:48Z",
        "closed_at": "2023-10-16T23:27:06Z",
        "merged_at": null,
        "body": "Make it inherit from `Stream` as indeed it is, see https://github.com/pytorch/pytorch/blob/97a513ed077323550b808e690a0b5a0452f87334/torch/csrc/cuda/Stream.cpp#L208 and\r\n```\r\npython3 -c \"import torch;print(torch._C._CudaStreamBase.__base__)\"\r\n<class 'torch.Stream'>\r\n```\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/111268\r\n\r\nTODO (in separate PR): Revive `test_typing` and add regression test\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2023-10-16T20:01:15Z",
        "closed_at": "2023-10-17T18:25:09Z",
        "merged_at": null,
        "body": "Fixes https://github.com/pytorch/pytorch/pull/110193 as it doesn't work as expected:\r\n\r\n* I forgot the timeout on the test step\r\n* Also MacOS test job wasn't covered\r\n\r\n### Testing\r\n\r\nThe job timeout is set correctly to 600 https://github.com/pytorch/pytorch/actions/runs/6541825177/job/17764485473#step:14:7",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-16T19:49:12Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Not making any changes",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 29,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-16T18:36:54Z",
        "closed_at": "2023-10-16T22:56:59Z",
        "merged_at": null,
        "body": "Fixes part 1 of https://github.com/pytorch/pytorch/issues/111370#issuecomment-1764730773\r\n\r\nWhile at it, add a test for numpy ndarray `.size` attribute. This started as an attempt to remove the delegation of what looks like a `.size()` method --- which does not exist in numpy --- on the same line this patch adds a `tolist` to. \r\nBut this is apparently needed for something else and existing tests start failing. Thus, declare it as _ain't broken don't fix_, and only keep the test. Can remove the test if wanted though. \r\n\n\ncc @mruberry @rgommers @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 75,
        "deletions": 27,
        "changed_files": 4,
        "created_at": "2023-10-16T18:28:19Z",
        "closed_at": "2023-10-17T19:16:44Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111381\r\n\r\nreland https://github.com/pytorch/pytorch/pull/109828\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-16T18:24:41Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #111310\r\n* __->__ #111380\r\n* #111309\r\n* #111308\r\n* #111307\r\n\r\nSee title. Makes this consistent with torch.library.{define, impl, impl_device}, where we have named the same argument `qualname`. This is not BC-breaking because we have not released a version of PyTorch with impl_abstract in it yet.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 22,
        "changed_files": 1,
        "created_at": "2023-10-16T17:58:22Z",
        "closed_at": "2023-10-18T04:52:44Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111379\n\nSummary: Since we cache the AOTInductor generated library file, we should not need to write the weights as binary file if the library file already exists.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 6,
        "changed_files": 3,
        "created_at": "2023-10-16T17:19:16Z",
        "closed_at": "2023-10-17T17:27:33Z",
        "merged_at": null,
        "body": "Summary:\nWe continue to allow the user to set clients with a map, but under the hood we use an array of constants.\n\nmodel_container thought it was OK to hand over the map, assume we just\nkept a pointer, and then mutate the map later; I had to fix that. I\nhope there aren't other sites that do the same thing...\n\nTest Plan: Existing tests? Internal benchmark iteration speed roughly doubled again.\n\nReviewed By: muchulee8, khabinov, chenyang78\n\nDifferential Revision: D50111512\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 193,
        "deletions": 46,
        "changed_files": 21,
        "created_at": "2023-10-16T17:13:55Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Part of #109802\n\ncc @mruberry",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2023-10-16T16:34:04Z",
        "closed_at": "2023-10-17T01:22:14Z",
        "merged_at": null,
        "body": "Follow up to #111305 that updates lintrunner's version too.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-16T13:30:07Z",
        "closed_at": "2023-10-16T19:19:47Z",
        "merged_at": null,
        "body": "Return an `at::Generator` from `newIPUGenerator`, not a reference to one.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2023-10-16T13:05:13Z",
        "closed_at": "2023-10-17T02:08:17Z",
        "merged_at": null,
        "body": "This PR uses clang-tidy in torch/csrc/CudaIPCTypes.cpp",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 151,
        "deletions": 16,
        "changed_files": 6,
        "created_at": "2023-10-16T11:04:52Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Use arch compile flags. They are needed for vectorization support on s390x.\r\nImplement new helper functions for inductor.\r\n\r\nThis change fixes multiple tests in test_cpu_repro.py\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-10-16T09:54:58Z",
        "closed_at": "2023-10-16T15:18:29Z",
        "merged_at": null,
        "body": "fixes https://github.com/pytorch/pytorch/issues/110699\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111364\r\n* #111040\r\n\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 9,
        "changed_files": 8,
        "created_at": "2023-10-16T08:08:45Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #106571\r\n\r\nI have fixed the B-026 error codes for Flake8 tests on the codebase. Please review and tell me anything else to do.\r\nThanks and excited for this first contribution to PyTorch.\r\n\r\nAlso I refer this issue which introduced [B-026](https://github.com/PyCQA/flake8-bugbear/issues/286) in `pytest-bugbear` and discuss the error code.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 8,
        "changed_files": 7,
        "created_at": "2023-10-16T07:45:13Z",
        "closed_at": "2023-10-16T07:45:30Z",
        "merged_at": null,
        "body": "Fixes #106571\r\n\r\nI have fixed the B-028 error codes for Flake8 tests on the codebase. Please review and tell me anything else to do.\r\nThanks and excited for this first contribution to PyTorch.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-16T07:40:53Z",
        "closed_at": "2023-10-16T11:53:07Z",
        "merged_at": null,
        "body": "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/_update-commit-hash.yml).\nUpdate the pinned xla hash.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-16T07:38:11Z",
        "closed_at": "2023-10-16T20:37:03Z",
        "merged_at": null,
        "body": "liner -> linear",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-16T07:22:44Z",
        "closed_at": "2023-10-17T08:44:29Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111356\n\nWhen the model returns a constant, we cannot \"release\" its handle,\nbecause the constant doesn't have any handle at all. Instead,\nwe should allocate a new tensor and then return a copy of the constant.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2023-10-16T06:20:04Z",
        "closed_at": "2023-10-16T18:31:42Z",
        "merged_at": null,
        "body": "The `prefetched` flag should be reset upon reshard. Otherwise, for zero2, next access to the corresponding parameter will skip \"unshard\" operation, and results in wrong parameter shape. \r\n\r\nThe need of unsharding is also metioned [in the comment of `FlatParameterHandle.unshard`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/_flat_param.py#L1241-L1242).\r\n\r\nAs [`FlatParameterHandle` already guarded it against unnecessary all gather](https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/_flat_param.py#L1240), this shouldn't incur extra communication overhead.\r\n\r\n_Personally I also find `_prefetched` a bit of mis-named, it should really be `_unsharded`._",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-16T05:41:57Z",
        "closed_at": "2023-10-16T17:18:25Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111353\n\n[tp] fix SP style regression\n\nAlthough we want to remove prepare_input/output, we should still keep\nthe old behavior for SequenceParallel",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-16T05:22:14Z",
        "closed_at": "2023-10-16T16:55:00Z",
        "merged_at": null,
        "body": null,
        "comments": 4
    },
    {
        "merged": false,
        "additions": 78,
        "deletions": 15,
        "changed_files": 3,
        "created_at": "2023-10-16T01:51:38Z",
        "closed_at": "2023-10-17T00:11:22Z",
        "merged_at": null,
        "body": "Re-land of https://github.com/pytorch/pytorch/pull/111011.\r\n\r\nThe original PR ended up having a bad interaction with code that tried to run `torch.compile` under `with torch.inference_mode`, which caused some internal tests to fail.\r\n\r\nThe issue was that:\r\n\r\n(1) AOTInductor invokes the pattern matcher passes in inductor\r\n\r\n(2) The pattern matcher registers some code with [training_graph](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/pad_mm.py#L461)\r\n\r\n(3) The `training_graph` function expects to be able to set the global autograd state to `requires_grad`, and always get out a join graph (assertion [here](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/pattern_matcher.py#L1196)).\r\n\r\n(4) However, when inference_mode is activated, and you try to run AOTAutograd, AOTAutograd will witness that all outputs to the traced function will not require grad, and (now correctly) think that we are tracing an inference graph, which fails the above assert.\r\n\r\nAfter talking to Bin, it sounds like these training-only patterns aren't necessary when we know we are compiling an inference graph (which should always be the case if you're running torch.compile with inference_mode). So I updated the pattern matcher to ignore any pattern matches using `training_graph`, when inference_mode is enabled.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111347\n\r\nThis reverts commit cf6b1cdf6ac74d375b0787bd8f9463cb3a53b0e5.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 58,
        "deletions": 58,
        "changed_files": 2,
        "created_at": "2023-10-16T01:21:51Z",
        "closed_at": "2023-10-16T06:15:17Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111346\n* #111177\n* #111176\n* #111166\n* #111160\n\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 273,
        "deletions": 302,
        "changed_files": 9,
        "created_at": "2023-10-16T00:46:07Z",
        "closed_at": "2023-10-16T00:47:10Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111344\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 268,
        "deletions": 297,
        "changed_files": 6,
        "created_at": "2023-10-16T00:44:15Z",
        "closed_at": "2023-10-16T19:19:23Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2023-10-16T00:44:09Z",
        "closed_at": "2023-10-16T19:19:27Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 273,
        "deletions": 302,
        "changed_files": 9,
        "created_at": "2023-10-16T00:40:03Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This is a cleaned up version of https://github.com/pytorch/pytorch/pull/110353--we should def land the OG PR. I wanted to launch a perf job to see the time difference in the nightly so made this one.\r\n\r\nJob: https://github.com/pytorch/pytorch/actions/runs/6538030862\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 123,
        "deletions": 6,
        "changed_files": 4,
        "created_at": "2023-10-16T00:07:09Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-15T23:23:49Z",
        "closed_at": "2023-10-16T07:34:00Z",
        "merged_at": null,
        "body": "liner -> linear\r\n\r\nlambd -> lambda",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-10-15T21:55:53Z",
        "closed_at": null,
        "merged_at": null,
        "body": "For #110252.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 31,
        "changed_files": 2,
        "created_at": "2023-10-15T20:13:43Z",
        "closed_at": "2023-10-15T21:04:01Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111336\n\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-15T18:52:15Z",
        "closed_at": "2023-10-16T17:59:13Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111334\r\n\r\nFixes #111329.\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 173,
        "deletions": 40,
        "changed_files": 4,
        "created_at": "2023-10-15T17:49:51Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR will contain additions to the `functorch_scan` functionality initially created with the PR https://github.com/pytorch/pytorch/pull/104442\r\n\r\nThe initial changes contain:\r\n\r\n1. Scan being moved to higher_order_ops\r\n2. Docstring being added for scan/scan_impl\r\n3. A testcase to torch/testing/_internal/control_flow_opinfo_db.py\r\n\r\nThe next step would be to enable inductor lowering for scan.\r\n\r\ncc @websterbei @zou3519 @vadimkantorov @ydwu4 @peterbell10 \r\n\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 43,
        "deletions": 41,
        "changed_files": 5,
        "created_at": "2023-10-15T17:18:33Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111332\n\nThis PR simplifes some padding logic to only handle padding when needed,\nand avoid doing redudant work when no padding needed, should help reuse\nsome cpu overhead in eager",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 231,
        "deletions": 153,
        "changed_files": 1,
        "created_at": "2023-10-15T08:45:51Z",
        "closed_at": "2023-10-15T21:53:38Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111327\n* #111314\n\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 81,
        "deletions": 0,
        "changed_files": 6,
        "created_at": "2023-10-15T07:42:05Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111326\n* #111325\n* #111324\n\nSummary:\nAdd update_constants for AOTInductorModel\n\nTest Plan:\nIncluded in commit\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 191,
        "deletions": 0,
        "changed_files": 7,
        "created_at": "2023-10-15T07:41:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111326\n* __->__ #111325\n* #111324\n\nSummary:\nAdd a runner for AOTInductorModel for easier test.\nAdd test for existing AOTInductorModel's interface (create & update constants map)\n\nTest Plan:\nCommit itself is a test.\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 32,
        "deletions": 29,
        "changed_files": 6,
        "created_at": "2023-10-15T07:41:54Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111326\n* #111325\n* __->__ #111324\n\nSummary:\nWe rename the model_runner to model_container_runner to prepare for\nadding tests of pure model without container.\n\nTest Plan:\ncommit itself is a test.\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 29,
        "changed_files": 4,
        "created_at": "2023-10-15T06:42:13Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-10-15T03:28:13Z",
        "closed_at": "2023-10-16T13:52:36Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111321\n\r\nFixes https://github.com/pytorch/pytorch/issues/111319\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-10-15T03:28:13Z",
        "closed_at": "2023-10-16T13:52:36Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111321\n\r\nFixes https://github.com/pytorch/pytorch/issues/111319\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 140,
        "deletions": 31,
        "changed_files": 7,
        "created_at": "2023-10-15T02:52:46Z",
        "closed_at": null,
        "merged_at": null,
        "body": "While not strictly necessary, it will definitely make config serialization easier and more portable, which may better support future efforts and testing/debugging.\r\n\r\nPart of https://github.com/pytorch/pytorch/issues/111235\r\n\r\nSplit out from: https://github.com/pytorch/pytorch/pull/111298\r\n\r\ncc @voznesenskym @ezyang @yanboliang ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-15T00:20:17Z",
        "closed_at": "2023-10-15T04:37:27Z",
        "merged_at": null,
        "body": "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/_update-commit-hash.yml).\nUpdate the pinned vision hash.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-10-14T21:50:05Z",
        "closed_at": "2023-10-15T05:40:10Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111314\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-14T21:08:15Z",
        "closed_at": null,
        "merged_at": null,
        "body": "We've had developers using Dynamo via torch.compile and torch._dynamo.export for months on Windows via SHARK (by just deleting these lines locally). The Windows check is moved to just the inductor backend, since it is based on Triton not supporting Windows.\r\n\r\nProgress on #90768\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 33,
        "changed_files": 2,
        "created_at": "2023-10-14T20:01:07Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111310\n* #111380\n* #111309\n* #111308\n* #111307\n\nWe mention the higher-level torch.library APIs and put the original docs\ninto a low-level API section.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 122,
        "deletions": 8,
        "changed_files": 6,
        "created_at": "2023-10-14T20:01:01Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111310\n* #111380\n* __->__ #111309\n* #111308\n* #111307\n\nUse this to register an implementation for a specific device_type.\nWe map the user provided device-type(s) to DispatchKeys and then call\ntorch.library.impl on them.\n\nThey may also specify multiple device_types at once or specify\n\"device_types=default\" to get a CompositeExplicitAutograd registration.\n\nTest Plan:\n- new tests",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 85,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2023-10-14T20:00:54Z",
        "closed_at": "2023-10-16T22:32:32Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111310\n* #111380\n* #111309\n* __->__ #111308\n* #111307\n\nWe add a new overload to torch.library.impl that accepts an optional\nLibrary arg. If provided, the lifetime of the registration will be\ntied to the Library arg, otherwise, it will live forever.\n\nTest Plan:\n- existing and new tests",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 104,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2023-10-14T20:00:47Z",
        "closed_at": "2023-10-16T22:32:31Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111310\n* #111380\n* #111309\n* #111308\n* __->__ #111307\n\nThis PR introduces a new overload of torch.library.define. Like\nimpl_abstract, and our plans for the rest of the torch.library APIs, we\nallow it to accept an optional library object to tie the lifetime of the\nop definition to.\n\nTest Plan:\n- new tests",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 170,
        "deletions": 23,
        "changed_files": 9,
        "created_at": "2023-10-14T19:07:13Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111415\n* __->__ #111306\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-14T18:47:58Z",
        "closed_at": "2023-10-15T01:55:52Z",
        "merged_at": null,
        "body": null,
        "comments": 4
    },
    {
        "merged": false,
        "additions": 65,
        "deletions": 65,
        "changed_files": 25,
        "created_at": "2023-10-14T18:18:52Z",
        "closed_at": "2023-10-16T23:06:14Z",
        "merged_at": null,
        "body": "This PR fixes typo in comments under `test` directory.\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 194,
        "deletions": 44,
        "changed_files": 7,
        "created_at": "2023-10-14T15:16:03Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #111300\r\n* #111299\r\n* #111298\r\n* __->__ #111303\r\n\r\nFixes: #111221 \r\n\r\ncc @ezyang @voznesenskym @Chillee @yanboliang ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 114,
        "deletions": 23,
        "changed_files": 2,
        "created_at": "2023-10-14T15:03:51Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes https://github.com/pytorch/pytorch/issues/100847\r\n\r\nThis PR follows the comment in https://github.com/pytorch/pytorch/issues/100847#issuecomment-1546247239 by deprecating the `verbose` parameter and removing the print statements. Removing the print statements is technically BC breaking, so I would be okay with putting them back in.\r\n\r\nTo be less annoying, this PR raises a warning only when `verbose` is explicitly passed in.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 28,
        "deletions": 29,
        "changed_files": 8,
        "created_at": "2023-10-14T14:20:04Z",
        "closed_at": "2023-10-17T05:52:27Z",
        "merged_at": null,
        "body": "Applies clang-tidy to aten/src/ATen/core/",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 93,
        "deletions": 25,
        "changed_files": 4,
        "created_at": "2023-10-14T13:58:27Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111300\n* #111299\n* #111298\n* #111303\n\r\n\r\n\r\ncc @voznesenskym @ezyang @Chillee",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 405,
        "deletions": 54,
        "changed_files": 10,
        "created_at": "2023-10-14T13:43:43Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111300\n* __->__ #111299\n* #111298\n* #111303\n\r\n---\r\n\r\nFixes: https://github.com/pytorch/pytorch/issues/110682\r\n\r\nReplaces: https://github.com/pytorch/pytorch/pull/111074\r\n\r\nThe guards are installed based on config that is valid at the call to `torch.compile`, rather than at any subsequent call / triggered compilation. Subsequent compilations will restore the config if there is a config mismatch of the existing global config with the saved config.\r\n\r\nTODO:\r\n- [X] add tests\r\n\r\nFollow up PRs:\r\n- [x] add revised cache size computation (follow up PR: #111300 , based on: https://github.com/pytorch/pytorch/pull/107496) \r\n- [ ] handle run-only mode?\r\n- [ ] config restoration itself is not thread-safe (tracked: https://github.com/pytorch/pytorch/issues/111150)\r\n\r\ncc @voznesenskym @ezyang @Chillee\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-14T13:43:38Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111300\n* #111299\n* __->__ #111298\n* #111303\n\r\nfixes: https://github.com/pytorch/pytorch/issues/111235\r\n\r\ncc @voznesenskym @ezyang @Chillee",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 189,
        "deletions": 40,
        "changed_files": 7,
        "created_at": "2023-10-14T13:43:33Z",
        "closed_at": "2023-10-14T17:42:32Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #111300\r\n* #111299\r\n* #111298\r\n* __->__ #111297\r\n\r\nfixes: https://github.com/pytorch/pytorch/issues/111221\r\n\r\n\r\ncc @voznesenskym @ezyang @Chillee",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 38,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-14T13:42:06Z",
        "closed_at": "2023-10-14T13:44:42Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111296\n* #111295\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 189,
        "deletions": 40,
        "changed_files": 7,
        "created_at": "2023-10-14T13:42:01Z",
        "closed_at": "2023-10-14T13:44:34Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111296\n* __->__ #111295\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 189,
        "deletions": 40,
        "changed_files": 7,
        "created_at": "2023-10-14T13:41:34Z",
        "closed_at": "2023-10-14T13:44:27Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111294\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-14T13:25:27Z",
        "closed_at": "2023-10-14T13:30:13Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111293\n* #111291\n* #111291\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-14T13:02:26Z",
        "closed_at": "2023-10-14T13:30:07Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111293\n* __->__ #111291\n* #111291\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2023-10-14T10:25:28Z",
        "closed_at": "2023-10-14T13:08:40Z",
        "merged_at": null,
        "body": "Stack:\r\n* #111289\r\n* #111249\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/111221\r\n\r\n\r\ncc @voznesenskym @ezyang @Chillee ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 405,
        "deletions": 50,
        "changed_files": 9,
        "created_at": "2023-10-14T10:17:09Z",
        "closed_at": "2023-10-14T17:42:58Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111289\r\n* #111249\r\n\r\nBase: \r\n- https://github.com/pytorch/pytorch/pull/111290\r\n\r\nReplaces: \r\n- https://github.com/pytorch/pytorch/pull/111074\r\n---\r\nFixes: https://github.com/pytorch/pytorch/issues/110682\r\n\r\nThe guards are installed based on config that is valid at the call to `torch.compile`, rather than at any subsequent call / triggered compilation. Subsequent compilations will restore the config if there is a config mismatch with the saved config.\r\n\r\nTODO:\r\n- [X] add tests\r\n\r\nFollow up PRs:\r\n- [x] add revised cache size computation (follow up PR: https://github.com/pytorch/pytorch/pull/111145, based on: https://github.com/pytorch/pytorch/pull/107496) \r\n- [ ] handle run-only mode?\r\n- [ ] config restoration itself is not thread-safe (tracked: https://github.com/pytorch/pytorch/issues/111150)\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @ezyang @Chillee ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 165,
        "deletions": 33,
        "changed_files": 6,
        "created_at": "2023-10-14T09:35:56Z",
        "closed_at": "2023-10-14T09:41:19Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-14T09:34:38Z",
        "closed_at": "2023-10-14T09:35:08Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 8,
        "changed_files": 3,
        "created_at": "2023-10-14T09:34:34Z",
        "closed_at": "2023-10-14T09:35:06Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-10-14T09:34:30Z",
        "closed_at": "2023-10-14T09:35:04Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-14T09:34:26Z",
        "closed_at": "2023-10-14T09:35:01Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 165,
        "deletions": 33,
        "changed_files": 6,
        "created_at": "2023-10-14T09:34:22Z",
        "closed_at": "2023-10-14T09:34:51Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-14T07:42:06Z",
        "closed_at": "2023-10-14T07:43:32Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111282\n* #111111\n* #111120\n* #111110\n* #111109\n* #111275\n* #111107\n* #111106\n\nAs title\n\nDifferential Revision: [D50209748](https://our.internmc.facebook.com/intern/diff/D50209748/)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 380,
        "deletions": 7,
        "changed_files": 4,
        "created_at": "2023-10-14T07:21:21Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111281\r\n* #111280\r\n\r\n**Summary**\r\nThis PR adds ConvBNAdd(ReLU) QAT Annotation into `X86InductorQuantizer`.\r\n\r\n**Test Plan**\r\n```\r\npython -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_binary_with_quantizer_api\r\npython -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_binary_unary_with_quantizer_api\r\npython -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d_add\r\npython -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d_add_relu\r\n```\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 311,
        "deletions": 24,
        "changed_files": 3,
        "created_at": "2023-10-14T07:21:13Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #111281\r\n* __->__ #111280\r\n\r\n**Summary**\r\nThis PR enables PT2 QAT Quantization flow in `X86InductorQuantizer`.\r\n\r\n\r\n\r\n**Test Plan**\r\n```\r\npython -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_with_quantizer_api\r\npython -m pytest test_x86inductor_quantizer.py -k test_qat_conv2d_unary_with_quantizer_api\r\npython -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d\r\npython -m pytest test_mkldnn_pattern_matcher.py -k test_qat_qconv2d_relu\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-14T04:05:31Z",
        "closed_at": "2023-10-15T16:00:42Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111278\n* #111234\n* #110717\n* #109145\n\nThis add __Str__ to op schema and dtensor spec for ease of reading",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 4,
        "changed_files": 2,
        "created_at": "2023-10-14T03:54:31Z",
        "closed_at": "2023-10-15T00:48:06Z",
        "merged_at": null,
        "body": "Fixes #104792\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 60,
        "deletions": 36,
        "changed_files": 3,
        "created_at": "2023-10-14T03:04:35Z",
        "closed_at": "2023-10-14T15:34:59Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111111\n* #111120\n* #111110\n* #111109\n* __->__ #111275\n* #111107\n* #111106\n\r\nThis is a reland PR for https://github.com/pytorch/pytorch/pull/111108 with the proper docstring fix.\r\n\r\n\r\n1. Rename DistributedStateDictOptions to StateDictOptions.\r\n2. Remove cpu_offload as we have not yet required this option.\r\n3. Rename save_frozen_parameters to ignore_frozen_params.\r\n\r\nDifferential Revision: [D50294352](https://our.internmc.facebook.com/intern/diff/D50294352/)",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-14T02:24:10Z",
        "closed_at": "2023-10-16T21:16:35Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111274\n* #111273\n* #111271\n\r\nI suspect in practice this won't matter, but if we do end up tracing this it causes them not to get decomposed.\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-14T02:24:05Z",
        "closed_at": "2023-10-16T21:16:33Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111274\n* __->__ #111273\n* #111271\n\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 70,
        "deletions": 41,
        "changed_files": 8,
        "created_at": "2023-10-14T01:56:07Z",
        "closed_at": "2023-10-16T21:16:32Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111274\n* #111273\n* __->__ #111271\n\r\n\r\nRather than baking the behavior of `AccumulateGrad` nodes into the generated graph (either as `+=`, or as a return value of the graph).  This creates a new `accumulate_grad_` dispatcher op that is included in the generated graph like:\r\n```\r\ndef forward(self, inputs, sizes, hooks):\r\n    getitem = inputs[0]\r\n    getitem_1 = inputs[1]\r\n    getitem_2 = inputs[2]\r\n    getitem_3 = inputs[3]\r\n    getitem_4 = inputs[4]\r\n    getitem_5 = inputs[5]\r\n    getitem_6 = inputs[6]\r\n    getitem_7 = inputs[7]\r\n    getitem_8 = inputs[8]\r\n    getitem_9 = inputs[9];  inputs = None\r\n    expand = torch.ops.aten.expand.default(getitem, [2, 4]);  getitem = None\r\n    threshold_backward = torch.ops.aten.threshold_backward.default(expand, getitem_1, 0);  expand = getitem_1 = None\r\n    t = torch.ops.aten.t.default(getitem_3);  getitem_3 = None\r\n    mm = torch.ops.aten.mm.default(threshold_backward, t);  t = None\r\n    t_1 = torch.ops.aten.t.default(threshold_backward)\r\n    mm_1 = torch.ops.aten.mm.default(t_1, getitem_2);  t_1 = getitem_2 = None\r\n    t_2 = torch.ops.aten.t.default(mm_1);  mm_1 = None\r\n    sum_1 = torch.ops.aten.sum.dim_IntList(threshold_backward, [0], True);  threshold_backward = None\r\n    view = torch.ops.aten.view.default(sum_1, [4]);  sum_1 = None\r\n    t_3 = torch.ops.aten.t.default(t_2);  t_2 = None\r\n    accumulate_grad_ = torch.ops.inductor.accumulate_grad_.default(getitem_4, t_3);  getitem_4 = t_3 = None\r\n    threshold_backward_1 = torch.ops.aten.threshold_backward.default(mm, getitem_5, 0);  mm = getitem_5 = None\r\n    t_4 = torch.ops.aten.t.default(threshold_backward_1)\r\n    mm_2 = torch.ops.aten.mm.default(t_4, getitem_6);  t_4 = getitem_6 = None\r\n    t_5 = torch.ops.aten.t.default(mm_2);  mm_2 = None\r\n    sum_2 = torch.ops.aten.sum.dim_IntList(threshold_backward_1, [0], True);  threshold_backward_1 = None\r\n    view_1 = torch.ops.aten.view.default(sum_2, [4]);  sum_2 = None\r\n    t_6 = torch.ops.aten.t.default(t_5);  t_5 = None\r\n    accumulate_grad__1 = torch.ops.inductor.accumulate_grad_.default(getitem_7, t_6);  getitem_7 = t_6 = None\r\n    accumulate_grad__2 = torch.ops.inductor.accumulate_grad_.default(getitem_8, view_1);  getitem_8 = view_1 = None\r\n    accumulate_grad__3 = torch.ops.inductor.accumulate_grad_.default(getitem_9, view);  getitem_9 = view = None\r\n    return []\r\n\r\n```\r\n\r\nThe motivation here is `AccumulateGrad` nodes are causing trouble in FSDP tracing, since FSDP is in-place resizing parameters and parameter storage in hooks.  We will model this mutation in dynamo, but not during the initial compiled autograd capture.  This allows us to bypass failing shape checks in the initial capture. \r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-14T01:12:29Z",
        "closed_at": "2023-10-16T18:31:49Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111270\n\nSummary: We're checking the original guard.expr in the issued set instead of the simplified expr, leading to duplicate guards in cases where one expression simplifies to another.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-14T01:11:19Z",
        "closed_at": "2023-10-14T02:00:28Z",
        "merged_at": null,
        "body": "Summary:\r\n\r\nNo need to review\r\n\r\nDEBUG\r\n\r\n\r\n\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-14T00:40:21Z",
        "closed_at": "2023-10-17T20:36:20Z",
        "merged_at": null,
        "body": "Summary:\r\n_wrapped_fns_to_patch points to f_globals which might change during iteration due to factors like lazy imports. This diff fixes potential runtime errors like:\r\n\r\n```\r\nRuntimeError: dictionary changed size during iteration\r\n```\r\n\r\nTest Plan: CI\r\n\r\nReviewed By: Kronuz\r\n\r\nDifferential Revision: D50283983\r\n\r\n\r\n",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-14T00:39:16Z",
        "closed_at": "2023-10-14T20:35:11Z",
        "merged_at": null,
        "body": "If `USE_ASAN` is set, compile FBGEMM with ASAN as well, by setting `USE_SANITIZER` to `address,undefined`\r\n\r\nThis fixes regression in sanitizer coverage introduced by https://github.com/pytorch/pytorch/pull/93147  that change effects of sanitizer from the entire project to just torch libraries, and finally allows one to reliably catch regression reported in https://github.com/pytorch/pytorch/issues/111189\r\n\r\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-14T00:38:47Z",
        "closed_at": "2023-10-14T01:08:19Z",
        "merged_at": null,
        "body": "Summary: No need to review\r\n\r\n\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-14T00:18:11Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/_update-commit-hash.yml).\nUpdate the pinned vision hash.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 67,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2023-10-14T00:13:49Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111263\n\n[WIP] Needs tests - specialize on symint if used as a dict key\n\ncc @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 108,
        "deletions": 49,
        "changed_files": 7,
        "created_at": "2023-10-13T23:20:35Z",
        "closed_at": "2023-10-15T05:15:16Z",
        "merged_at": null,
        "body": "Previously we were generating a graph to add runtime assertions on inputs and then running that graph to check input constraints. This PR checks input constraints directly.\r\n\r\nDifferential Revision: D50289970\r\n\r\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 22
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-13T23:05:17Z",
        "closed_at": "2023-10-14T03:24:24Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111261\n* #111108\n* #111107\n* #111106\n\nAs title\n\nDifferential Revision: [D50289357](https://our.internmc.facebook.com/intern/diff/D50289357/)",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-13T22:46:21Z",
        "closed_at": "2023-10-13T22:51:29Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111260\n* #111259\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-13T22:46:08Z",
        "closed_at": "2023-10-14T10:25:06Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #111289\r\n* #111249\r\n* __->__ #111259\r\n\r\n\r\ncc @voznesenskym @ezyang @Chillee ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2023-10-13T22:38:40Z",
        "closed_at": "2023-10-13T22:39:04Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111258\n* #111249\n* #111248\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 7,
        "changed_files": 2,
        "created_at": "2023-10-13T22:32:04Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Differential Revision: D50280766\n\n\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 299,
        "deletions": 43,
        "changed_files": 4,
        "created_at": "2023-10-13T22:13:29Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111253\n\r\nWith this PR, we have full op support for SAM without needing to unwrap subclass into jagged buffer -> run ops -> rewrap manually. Specifically, this was previously happening in the MaskDecoder.\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 41,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-13T21:55:43Z",
        "closed_at": "2023-10-14T13:18:37Z",
        "merged_at": null,
        "body": "\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111249\n* #111248\n* #111259\n* #111259\n\r\nBase: \r\n- https://github.com/pytorch/pytorch/pull/111290\r\n\r\n\r\nFixes: https://github.com/pytorch/pytorch/issues/111235\r\n\r\n\r\ncc @voznesenskym @ezyang @Chillee ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 172,
        "deletions": 36,
        "changed_files": 5,
        "created_at": "2023-10-13T21:55:30Z",
        "closed_at": "2023-10-13T22:58:11Z",
        "merged_at": null,
        "body": "Fixes: #111221 \r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #111249\r\n* __->__ #111248\r\n\r\n\r\n\r\ncc @ezyang @voznesenskym @Chillee ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 28,
        "changed_files": 1,
        "created_at": "2023-10-13T21:50:07Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #109739\n* #111239\n* __->__ #111246\n\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T21:48:32Z",
        "closed_at": "2023-10-14T02:28:05Z",
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T21:34:43Z",
        "closed_at": "2023-10-13T21:35:22Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2023-10-13T21:34:31Z",
        "closed_at": "2023-10-13T21:35:14Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-10-13T21:34:17Z",
        "closed_at": "2023-10-13T21:35:09Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-13T21:34:06Z",
        "closed_at": "2023-10-13T21:35:00Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 178,
        "deletions": 34,
        "changed_files": 6,
        "created_at": "2023-10-13T21:33:54Z",
        "closed_at": "2023-10-13T21:34:39Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 52,
        "deletions": 19,
        "changed_files": 8,
        "created_at": "2023-10-13T21:19:36Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #109739\n* __->__ #111239\n* #111246\n\nShould be purely cosmetic changes",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 172,
        "deletions": 36,
        "changed_files": 5,
        "created_at": "2023-10-13T21:11:20Z",
        "closed_at": "2023-10-13T21:56:22Z",
        "merged_at": null,
        "body": "Fixes: https://github.com/pytorch/pytorch/issues/111221\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111238\r\n\r\n\r\n\r\ncc @voznesenskym  @ezyang @Chillee ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 88,
        "deletions": 31,
        "changed_files": 4,
        "created_at": "2023-10-13T20:56:13Z",
        "closed_at": "2023-10-13T23:19:54Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111237\n\nDifferential Revision: [D50282366](https://our.internmc.facebook.com/intern/diff/D50282366/)\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 493,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-10-13T20:41:31Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111236\n\nAdd libraries to apply tensor parallel transformation to an exported program.\n\nDifferential Revision: [D50214796](https://our.internmc.facebook.com/intern/diff/D50214796/)",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 190,
        "deletions": 75,
        "changed_files": 4,
        "created_at": "2023-10-13T20:30:12Z",
        "closed_at": "2023-10-15T16:00:41Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111278\n* __->__ #111234\n* #110717\n* #109145\n\nAs titled, this also handles sth like [Shard(0), Shard(0)] correctly for\npointwise ops, which was previously errored out",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 122,
        "deletions": 18,
        "changed_files": 5,
        "created_at": "2023-10-13T20:18:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111233\n* #111232\n\r\nImproves perf of llama_v2 locally from 1.55 -> 1.57\r\n\r\nThe initial heuristic is to lower to pointwise if # of inputs is <= 4, and all the inputs are pointwise or InputBuffers or if all the outputs are pointwise.\r\n\r\nI am going to do an OSS perf run, but I'd also be curious to thoughts from reviewers. There are definitely instances where we should be lowering to foreach_kernels, but it's less flexible for fusion. The motivating example was:\r\n\r\n```\r\ndef rotate_half(x):\r\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\r\n    x1 = x[..., : x.shape[-1] // 2]\r\n    x2 = x[..., x.shape[-1] // 2 :]\r\n    return torch.cat((-x2, x1), dim=-1)\r\n\r\ndef apply_rotary_pos_emb(q, k, cos, sin):\r\n    iota =  torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)\r\n    \r\n    # File: /scratch/eellison/work/torchdynamo/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:657, code: position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\r\n    unsqueeze = torch.ops.aten.unsqueeze.default(iota, 0)\r\n    position_ids = torch.ops.aten.reshape.default(unsqueeze, [-1, 512]);  unsqueeze = None\r\n\r\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\r\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\r\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\r\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\r\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\r\n    q_embed = (q * cos) + (rotate_half(q) * sin)\r\n    k_embed = (k * cos) + (rotate_half(k) * sin)\r\n    return q_embed, k_embed\r\n```\r\n\r\nAlso not sure if I should be more worried about concatting reduction->pointwise inputs.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 36,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-13T20:18:55Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111233\n* __->__ #111232\n\r\nImproves llama_v2 perf locally from 1.48x -> 1.55x. \r\n\r\nA good future rewrite would be to unify the freezing batching with the other batching rules that @yanboliang & co were working on. I want to wait for the forthcoming pre-dispatch changes to settle down first.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-13T20:18:26Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary:\n```\nIn [1]: import torch\n   ...: torch.export.Dim('foo', min=1, max=16)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[1], line 2\n      1 import torch\n----> 2 torch.export.Dim('foo', min=1, max=16)\n\nFile /..../torch/export/__init__.py:319, in Dim(name, min, max)\n    317 assert _max > _min, f\"Cannot create Dim with inconsistent min={min}, max={max}\"\n    318 dim = _Dim(name, (int,), {\"min\": _min, \"max\": _max})\n--> 319 dim.__module__ = inspect.getmodule(inspect.stack()[1][0]).__name__  # type: ignore[union-attr]\n    320 return dim\n\nAttributeError: 'NoneType' object has no attribute '__name__'\n```\n\nTest Plan: Repeat above repro\n\nDifferential Revision: D50275165\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-13T19:51:08Z",
        "closed_at": "2023-10-13T20:37:23Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* __->__ #111230\n\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 5,
        "changed_files": 10,
        "created_at": "2023-10-13T19:50:56Z",
        "closed_at": "2023-10-17T16:29:37Z",
        "merged_at": null,
        "body": "https://github.com/pytorch/test-infra/pull/4617 generates `file_test_class_rating.json`. Now we ensure it's available for heuristics to use during the test step.\r\n\r\n(Actual heuristics will come in a separate PR)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-10-13T19:36:03Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111228\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 66,
        "changed_files": 1,
        "created_at": "2023-10-13T19:23:40Z",
        "closed_at": "2023-10-14T00:17:06Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111226\n\n[Wait for CI] [dynamo] collapse local and global guard builders\n\ncc @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-10-13T19:18:07Z",
        "closed_at": "2023-10-14T02:08:55Z",
        "merged_at": null,
        "body": "Summary: fall back to the old nodes when meta val is missing.\n\nTest Plan: buck2 run //executorch/examples/portable/scripts:export -- --model_name=emformer_predict\n\nDifferential Revision: D50278439\n\n\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-10-13T18:20:49Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111219\n* #111216\n\r\nFixes https://github.com/pytorch/pytorch/issues/111200\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 32,
        "deletions": 24,
        "changed_files": 6,
        "created_at": "2023-10-13T18:16:12Z",
        "closed_at": "2023-10-14T02:31:47Z",
        "merged_at": null,
        "body": "Hopefully it will align with internal system and they will detect heap-overlow access reported in https://github.com/pytorch/pytorch/issues/111189 Also, do not build neither Triton, nor protobuf nor DB dependencies (as they are not needed for ASAN builds/tests)\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 16,
        "changed_files": 6,
        "created_at": "2023-10-13T17:47:24Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #111219\r\n* __->__ #111216\r\n\r\nFollow up on https://github.com/pytorch/pytorch/pull/95479\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/111198\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/111197\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/111188\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/111201\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/111202\r\n\r\nI can also do this for some other types, will do this stacked on top.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 18,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-13T17:46:04Z",
        "closed_at": "2023-10-16T18:47:57Z",
        "merged_at": null,
        "body": "Summary: NCCL version is essential for debugging purpose and NCCL rollout monitoring. Log this info for easy access.\n\nTest Plan:\nrun cmf10x on devgpu\n\nhttps://pxl.cl/3B5gf\n\n\nhttps://fburl.com/scuba/pytorch_c10d_logging/lybk2usq\n\nDifferential Revision: D50240853\n\n\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T17:46:02Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Okay, clearly, this doesn't \"just\" work lol. The ABC ordering was depended on for the topological sort.\r\n\r\nThere are ~135/325 (41%) of ops whose orderings change, see attached file:\r\n[extracted_lines.txt](https://github.com/pytorch/pytorch/files/12898208/extracted_lines.txt)\r\n<details>\r\n\r\n```\r\n   static PythonArgParser parser({\r\n-    \"_assert_async(Tensor input)\",\r\n     \"_assert_async(Tensor input, c10::string_view assert_msg)\",\r\n+    \"_assert_async(Tensor input)\",\r\n   static PythonArgParser parser({\r\n-    \"_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank)\",\r\n     \"_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int64_t blank)\",\r\n+    \"_use_cudnn_ctc_loss(Tensor log_probs, Tensor targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank)\",\r\n   static PythonArgParser parser({\r\n-    \"_cudnn_ctc_loss(Tensor log_probs, Tensor targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity)\",\r\n     \"_cudnn_ctc_loss(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int64_t blank, bool deterministic, bool zero_infinity)\",\r\n+    \"_cudnn_ctc_loss(Tensor log_probs, Tensor targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank, bool deterministic, bool zero_infinity)\",\r\n   static PythonArgParser parser({\r\n-    \"add(Tensor input, Scalar alpha, Tensor other, *, Tensor out=None)|deprecated\",\r\n     \"add(Tensor input, Tensor other, *, Scalar alpha=1, Tensor out=None)\",\r\n+    \"add(Tensor input, Scalar alpha, Tensor other, *, Tensor out=None)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"addmv(Scalar beta, Tensor input, Scalar alpha, Tensor mat, Tensor vec, *, Tensor out=None)|deprecated\",\r\n-    \"addmv(Scalar beta, Tensor input, Tensor mat, Tensor vec, *, Tensor out=None)|deprecated\",\r\n     \"addmv(Tensor input, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor out=None)\",\r\n+    \"addmv(Scalar beta, Tensor input, Tensor mat, Tensor vec, *, Tensor out=None)|deprecated\",\r\n+    \"addmv(Scalar beta, Tensor input, Scalar alpha, Tensor mat, Tensor vec, *, Tensor out=None)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"addmv_(Scalar beta, Tensor input, Scalar alpha, Tensor mat, Tensor vec)|deprecated\",\r\n-    \"addmv_(Scalar beta, Tensor input, Tensor mat, Tensor vec)|deprecated\",\r\n     \"addmv_(Tensor input, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1)\",\r\n+    \"addmv_(Scalar beta, Tensor input, Tensor mat, Tensor vec)|deprecated\",\r\n+    \"addmv_(Scalar beta, Tensor input, Scalar alpha, Tensor mat, Tensor vec)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"addr(Scalar beta, Tensor input, Scalar alpha, Tensor vec1, Tensor vec2, *, Tensor out=None)|deprecated\",\r\n-    \"addr(Scalar beta, Tensor input, Tensor vec1, Tensor vec2, *, Tensor out=None)|deprecated\",\r\n     \"addr(Tensor input, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor out=None)\",\r\n+    \"addr(Scalar beta, Tensor input, Tensor vec1, Tensor vec2, *, Tensor out=None)|deprecated\",\r\n+    \"addr(Scalar beta, Tensor input, Scalar alpha, Tensor vec1, Tensor vec2, *, Tensor out=None)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"all(Tensor input, *, Tensor out=None)\",\r\n     \"all(Tensor input, int64_t dim, bool keepdim=False, *, Tensor out=None)\",\r\n+    \"all(Tensor input, *, Tensor out=None)\",\r\n     \"all(Tensor input, Dimname dim, bool keepdim=False, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"any(Tensor input, *, Tensor out=None)\",\r\n     \"any(Tensor input, int64_t dim, bool keepdim=False, *, Tensor out=None)\",\r\n+    \"any(Tensor input, *, Tensor out=None)\",\r\n     \"any(Tensor input, Dimname dim, bool keepdim=False, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"arange(Scalar end, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"arange(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"arange(Scalar start, Scalar end, Scalar step=1, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"arange(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"arange(Scalar end, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"atleast_1d(Tensor input)\",\r\n     \"atleast_1d(TensorList tensors)\",\r\n+    \"atleast_1d(Tensor input)\",\r\n   static PythonArgParser parser({\r\n-    \"atleast_2d(Tensor input)\",\r\n     \"atleast_2d(TensorList tensors)\",\r\n+    \"atleast_2d(Tensor input)\",\r\n   static PythonArgParser parser({\r\n-    \"atleast_3d(Tensor input)\",\r\n     \"atleast_3d(TensorList tensors)\",\r\n+    \"atleast_3d(Tensor input)\",\r\n   static PythonArgParser parser({\r\n-    \"baddbmm(Scalar beta, Tensor input, Scalar alpha, Tensor batch1, Tensor batch2, *, Tensor out=None)|deprecated\",\r\n-    \"baddbmm(Scalar beta, Tensor input, Tensor batch1, Tensor batch2, *, Tensor out=None)|deprecated\",\r\n     \"baddbmm(Tensor input, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor out=None)\",\r\n+    \"baddbmm(Scalar beta, Tensor input, Tensor batch1, Tensor batch2, *, Tensor out=None)|deprecated\",\r\n+    \"baddbmm(Scalar beta, Tensor input, Scalar alpha, Tensor batch1, Tensor batch2, *, Tensor out=None)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"bartlett_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"bartlett_window(int64_t window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"bartlett_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"bernoulli(Tensor input, *, Generator? generator=None, Tensor out=None)\",\r\n     \"bernoulli(Tensor input, double p, *, Generator? generator=None)\",\r\n+    \"bernoulli(Tensor input, *, Generator? generator=None, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"blackman_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"blackman_window(int64_t window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"blackman_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"tensor_split(Tensor input, SymIntArrayRef indices, int64_t dim=0)\",\r\n     \"tensor_split(Tensor input, Tensor tensor_indices_or_sections, int64_t dim=0)\",\r\n+    \"tensor_split(Tensor input, SymIntArrayRef indices, int64_t dim=0)\",\r\n     \"tensor_split(Tensor input, SymInt sections, int64_t dim=0)\",\r\n   static PythonArgParser parser({\r\n-    \"_convolution(Tensor input, Tensor weight, Tensor? bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled)\",\r\n     \"_convolution(Tensor input, Tensor weight, Tensor? bias, IntArrayRef stride, SymIntArrayRef padding, IntArrayRef dilation, bool transposed, SymIntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32)\",\r\n+    \"_convolution(Tensor input, Tensor weight, Tensor? bias, IntArrayRef stride, IntArrayRef padding, IntArrayRef dilation, bool transposed, IntArrayRef output_padding, int64_t groups, bool benchmark, bool deterministic, bool cudnn_enabled)\",\r\n   static PythonArgParser parser({\r\n-    \"conv1d(Tensor input, Tensor weight, Tensor? bias=None, IntArrayRef[1] stride=1, SymIntArrayRef[1] padding=0, IntArrayRef[1] dilation=1, int64_t groups=1)\",\r\n     \"conv1d(Tensor input, Tensor weight, Tensor? bias=None, IntArrayRef[1] stride=1, c10::string_view padding=\\\"valid\\\", IntArrayRef[1] dilation=1, int64_t groups=1)\",\r\n+    \"conv1d(Tensor input, Tensor weight, Tensor? bias=None, IntArrayRef[1] stride=1, SymIntArrayRef[1] padding=0, IntArrayRef[1] dilation=1, int64_t groups=1)\",\r\n   static PythonArgParser parser({\r\n-    \"conv2d(Tensor input, Tensor weight, Tensor? bias=None, IntArrayRef[2] stride=1, SymIntArrayRef[2] padding=0, IntArrayRef[2] dilation=1, int64_t groups=1)\",\r\n     \"conv2d(Tensor input, Tensor weight, Tensor? bias=None, IntArrayRef[2] stride=1, c10::string_view padding=\\\"valid\\\", IntArrayRef[2] dilation=1, int64_t groups=1)\",\r\n+    \"conv2d(Tensor input, Tensor weight, Tensor? bias=None, IntArrayRef[2] stride=1, SymIntArrayRef[2] padding=0, IntArrayRef[2] dilation=1, int64_t groups=1)\",\r\n   static PythonArgParser parser({\r\n-    \"conv3d(Tensor input, Tensor weight, Tensor? bias=None, IntArrayRef[3] stride=1, SymIntArrayRef[3] padding=0, IntArrayRef[3] dilation=1, int64_t groups=1)\",\r\n     \"conv3d(Tensor input, Tensor weight, Tensor? bias=None, IntArrayRef[3] stride=1, c10::string_view padding=\\\"valid\\\", IntArrayRef[3] dilation=1, int64_t groups=1)\",\r\n+    \"conv3d(Tensor input, Tensor weight, Tensor? bias=None, IntArrayRef[3] stride=1, SymIntArrayRef[3] padding=0, IntArrayRef[3] dilation=1, int64_t groups=1)\",\r\n   static PythonArgParser parser({\r\n-    \"ctc_loss(Tensor log_probs, Tensor targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank=0, int64_t reduction=at::Reduction::Mean, bool zero_infinity=False)\",\r\n     \"ctc_loss(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int64_t blank=0, int64_t reduction=at::Reduction::Mean, bool zero_infinity=False)\",\r\n+    \"ctc_loss(Tensor log_probs, Tensor targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank=0, int64_t reduction=at::Reduction::Mean, bool zero_infinity=False)\",\r\n   static PythonArgParser parser({\r\n-    \"_ctc_loss(Tensor log_probs, Tensor targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank=0, bool zero_infinity=False)\",\r\n     \"_ctc_loss(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int64_t blank=0, bool zero_infinity=False)\",\r\n+    \"_ctc_loss(Tensor log_probs, Tensor targets, IntArrayRef input_lengths, IntArrayRef target_lengths, int64_t blank=0, bool zero_infinity=False)\",\r\n   static PythonArgParser parser({\r\n-    \"diagonal(Tensor input, *, Dimname outdim, Dimname dim1, Dimname dim2, int64_t offset=0)\",\r\n     \"diagonal(Tensor input, int64_t offset=0, int64_t dim1=0, int64_t dim2=1)\",\r\n+    \"diagonal(Tensor input, *, Dimname outdim, Dimname dim1, Dimname dim2, int64_t offset=0)\",\r\n   static PythonArgParser parser({\r\n-    \"gradient(Tensor input, *, IntArrayRef dim, int64_t edge_order=1)\",\r\n-    \"gradient(Tensor input, *, Scalar spacing, IntArrayRef dim, int64_t edge_order=1)\",\r\n-    \"gradient(Tensor input, *, Scalar? spacing=None, int64_t? dim=None, int64_t edge_order=1)\",\r\n     \"gradient(Tensor input, *, ScalarList spacing, int64_t? dim=None, int64_t edge_order=1)\",\r\n-    \"gradient(Tensor input, *, ScalarList spacing, IntArrayRef dim, int64_t edge_order=1)\",\r\n+    \"gradient(Tensor input, *, Scalar? spacing=None, int64_t? dim=None, int64_t edge_order=1)\",\r\n+    \"gradient(Tensor input, *, Scalar spacing, IntArrayRef dim, int64_t edge_order=1)\",\r\n+    \"gradient(Tensor input, *, IntArrayRef dim, int64_t edge_order=1)\",\r\n     \"gradient(Tensor input, *, TensorList spacing, int64_t? dim=None, int64_t edge_order=1)\",\r\n+    \"gradient(Tensor input, *, ScalarList spacing, IntArrayRef dim, int64_t edge_order=1)\",\r\n     \"gradient(Tensor input, *, TensorList spacing, IntArrayRef dim, int64_t edge_order=1)\",\r\n   static PythonArgParser parser({\r\n-    \"div(Tensor input, Tensor other, *, Tensor out=None)\",\r\n     \"div(Tensor input, Tensor other, *, c10::string_view? rounding_mode, Tensor out=None)\",\r\n+    \"div(Tensor input, Tensor other, *, Tensor out=None)\",\r\n     \"div(Tensor input, Scalar other, *, c10::string_view? rounding_mode)\",\r\n   static PythonArgParser parser({\r\n-    \"divide(Tensor input, Tensor other, *, Tensor out=None)\",\r\n     \"divide(Tensor input, Tensor other, *, c10::string_view? rounding_mode, Tensor out=None)\",\r\n-    \"divide(Tensor input, Scalar other)\",\r\n+    \"divide(Tensor input, Tensor other, *, Tensor out=None)\",\r\n     \"divide(Tensor input, Scalar other, *, c10::string_view? rounding_mode)\",\r\n+    \"divide(Tensor input, Scalar other)\",\r\n   static PythonArgParser parser({\r\n-    \"embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int64_t? padding_idx)\",\r\n     \"embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int64_t mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False)\",\r\n+    \"embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int64_t mode, bool sparse, Tensor? per_sample_weights, bool include_last_offset, int64_t? padding_idx)\",\r\n   static PythonArgParser parser({\r\n-    \"empty(IntArrayRef size, *, DimnameList? names, MemoryFormat? memory_format=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"empty(SymIntArrayRef size, *, MemoryFormat? memory_format=None, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"empty(IntArrayRef size, *, DimnameList? names, MemoryFormat? memory_format=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"eye(SymInt n, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"eye(SymInt n, SymInt m, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"eye(SymInt n, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"flatten(Tensor input, int64_t start_dim, int64_t end_dim, Dimname out_dim)\",\r\n     \"flatten(Tensor input, int64_t start_dim=0, int64_t end_dim=-1)\",\r\n-    \"flatten(Tensor input, Dimname start_dim, Dimname end_dim, Dimname out_dim)\",\r\n+    \"flatten(Tensor input, int64_t start_dim, int64_t end_dim, Dimname out_dim)\",\r\n     \"flatten(Tensor input, DimnameList dims, Dimname out_dim)\",\r\n+    \"flatten(Tensor input, Dimname start_dim, Dimname end_dim, Dimname out_dim)\",\r\n   static PythonArgParser parser({\r\n-    \"unflatten(Tensor input, Dimname dim, SymIntArrayRef sizes, DimnameList names)\",\r\n     \"unflatten(Tensor input, int64_t dim, SymIntArrayRef sizes)\",\r\n+    \"unflatten(Tensor input, Dimname dim, SymIntArrayRef sizes, DimnameList names)\",\r\n   static PythonArgParser parser({\r\n-    \"full(IntArrayRef size, Scalar fill_value, *, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"full(SymIntArrayRef size, Scalar fill_value, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"full(IntArrayRef size, Scalar fill_value, *, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"hann_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"hann_window(int64_t window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"hann_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"hamming_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"hamming_window(int64_t window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"hamming_window(int64_t window_length, bool periodic, double alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"hamming_window(int64_t window_length, bool periodic, double alpha, double beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"hamming_window(int64_t window_length, bool periodic, double alpha, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"hamming_window(int64_t window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"hamming_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"kaiser_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"kaiser_window(int64_t window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"kaiser_window(int64_t window_length, bool periodic, double beta, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"kaiser_window(int64_t window_length, bool periodic, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"kaiser_window(int64_t window_length, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n     \"isin(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor out=None)\",\r\n-    \"isin(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor out=None)\",\r\n     \"isin(Tensor elements, Scalar test_element, *, bool assume_unique=False, bool invert=False, Tensor out=None)\",\r\n+    \"isin(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"fbgemm_pack_quantized_matrix(Tensor input)\",\r\n     \"fbgemm_pack_quantized_matrix(Tensor input, int64_t K, int64_t N)\",\r\n+    \"fbgemm_pack_quantized_matrix(Tensor input)\",\r\n   static PythonArgParser parser({\r\n     \"linspace(Tensor start, Tensor end, int64_t steps, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"linspace(Scalar start, Tensor end, int64_t steps, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"linspace(Tensor start, Scalar end, int64_t steps, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"linspace(Scalar start, Tensor end, int64_t steps, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"linspace(Scalar start, Scalar end, int64_t steps, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n     \"xlogy(Tensor input, Tensor other, *, Tensor out=None)\",\r\n-    \"xlogy(Scalar self, Tensor other, *, Tensor out=None)\",\r\n     \"xlogy(Tensor input, Scalar other, *, Tensor out=None)\",\r\n+    \"xlogy(Scalar self, Tensor other, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n     \"logspace(Tensor start, Tensor end, int64_t steps, double base=10.0, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"logspace(Scalar start, Tensor end, int64_t steps, double base=10.0, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"logspace(Tensor start, Scalar end, int64_t steps, double base=10.0, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"logspace(Scalar start, Tensor end, int64_t steps, double base=10.0, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"logspace(Scalar start, Scalar end, int64_t steps, double base=10.0, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"_aminmax(Tensor input)\",\r\n     \"_aminmax(Tensor input, int64_t dim, bool keepdim=False)\",\r\n+    \"_aminmax(Tensor input)\",\r\n   static PythonArgParser parser({\r\n-    \"max(Tensor input, *, Tensor out=None)\",\r\n-    \"max(Tensor input, Tensor other, *, Tensor out=None)\",\r\n     \"max(Tensor input, int64_t dim, bool keepdim=False, *, TensorList[2] out=None)\",\r\n+    \"max(Tensor input, Tensor other, *, Tensor out=None)\",\r\n+    \"max(Tensor input, *, Tensor out=None)\",\r\n     \"max(Tensor input, Dimname dim, bool keepdim=False, *, TensorList[2] out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"mean(Tensor input, *, ScalarType? dtype=None)\",\r\n     \"mean(Tensor input, IntArrayRef[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor out=None)\",\r\n+    \"mean(Tensor input, *, ScalarType? dtype=None)\",\r\n     \"mean(Tensor input, DimnameList[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"median(Tensor input)\",\r\n     \"median(Tensor input, int64_t dim, bool keepdim=False, *, TensorList[2] out=None)\",\r\n+    \"median(Tensor input)\",\r\n     \"median(Tensor input, Dimname dim, bool keepdim=False, *, TensorList[2] out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"nanmedian(Tensor input)\",\r\n     \"nanmedian(Tensor input, int64_t dim, bool keepdim=False, *, TensorList[2] out=None)\",\r\n+    \"nanmedian(Tensor input)\",\r\n     \"nanmedian(Tensor input, Dimname dim, bool keepdim=False, *, TensorList[2] out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"min(Tensor input, *, Tensor out=None)\",\r\n-    \"min(Tensor input, Tensor other, *, Tensor out=None)\",\r\n     \"min(Tensor input, int64_t dim, bool keepdim=False, *, TensorList[2] out=None)\",\r\n+    \"min(Tensor input, Tensor other, *, Tensor out=None)\",\r\n+    \"min(Tensor input, *, Tensor out=None)\",\r\n     \"min(Tensor input, Dimname dim, bool keepdim=False, *, TensorList[2] out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, bool training, double momentum, double eps, *, TensorList[3] out=None)\",\r\n     \"_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, bool training, double momentum, double eps, *, TensorList[3] out=None)\",\r\n+    \"_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, bool training, double momentum, double eps, *, TensorList[3] out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"ones(IntArrayRef size, *, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"ones(SymIntArrayRef size, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"ones(IntArrayRef size, *, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"rand(SymIntArrayRef size, *, Generator? generator, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"rand(SymIntArrayRef size, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"rand(SymIntArrayRef size, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"rand(SymIntArrayRef size, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"rand(SymIntArrayRef size, *, Generator? generator, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"rand(SymIntArrayRef size, *, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"randint(SymInt high, SymIntArrayRef size, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"randint(SymInt high, SymIntArrayRef size, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"randint(SymInt low, SymInt high, SymIntArrayRef size, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"randint(SymInt low, SymInt high, SymIntArrayRef size, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"randint(SymInt low, SymInt high, SymIntArrayRef size, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"randint(SymInt high, SymIntArrayRef size, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"randint(SymInt high, SymIntArrayRef size, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"randint_like(Tensor input, SymInt high, *, MemoryFormat? memory_format=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"randint_like(Tensor input, SymInt low, SymInt high, *, MemoryFormat? memory_format=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"randint_like(Tensor input, SymInt high, *, MemoryFormat? memory_format=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"randn(SymIntArrayRef size, *, Generator? generator, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n-    \"randn(SymIntArrayRef size, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"randn(SymIntArrayRef size, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"randn(SymIntArrayRef size, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"randn(SymIntArrayRef size, *, Generator? generator, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"randn(SymIntArrayRef size, *, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"randperm(SymInt n, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"randperm(SymInt n, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"randperm(SymInt n, *, Generator? generator, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"repeat_interleave(Tensor input, Tensor repeats, int64_t? dim=None, *, SymInt? output_size=None)\",\r\n     \"repeat_interleave(Tensor repeats, *, SymInt? output_size=None)\",\r\n+    \"repeat_interleave(Tensor input, Tensor repeats, int64_t? dim=None, *, SymInt? output_size=None)\",\r\n     \"repeat_interleave(Tensor input, SymInt repeats, int64_t? dim=None, *, SymInt? output_size=None)\",\r\n   static PythonArgParser parser({\r\n-    \"round(Tensor input, *, Tensor out=None)\",\r\n     \"round(Tensor input, *, int64_t decimals, Tensor out=None)\",\r\n+    \"round(Tensor input, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"round_(Tensor input)\",\r\n     \"round_(Tensor input, *, int64_t decimals)\",\r\n+    \"round_(Tensor input)\",\r\n   static PythonArgParser parser({\r\n-    \"select(Tensor input, Dimname dim, int64_t index)\",\r\n     \"select(Tensor input, int64_t dim, SymInt index)\",\r\n+    \"select(Tensor input, Dimname dim, int64_t index)\",\r\n   static PythonArgParser parser({\r\n-    \"split(Tensor input, SymInt split_size, int64_t dim=0)\",\r\n     \"split(Tensor input, SymIntArrayRef split_size, int64_t dim=0)\",\r\n+    \"split(Tensor input, SymInt split_size, int64_t dim=0)\",\r\n   static PythonArgParser parser({\r\n-    \"squeeze(Tensor input)\",\r\n     \"squeeze(Tensor input, int64_t dim)\",\r\n+    \"squeeze(Tensor input)\",\r\n     \"squeeze(Tensor input, IntArrayRef dim)\",\r\n     \"squeeze(Tensor input, Dimname dim)\",\r\n   static PythonArgParser parser({\r\n-    \"sspaddmm(Scalar beta, Tensor input, Scalar alpha, Tensor mat1, Tensor mat2)|deprecated\",\r\n-    \"sspaddmm(Scalar beta, Tensor input, Tensor mat1, Tensor mat2)|deprecated\",\r\n     \"sspaddmm(Tensor input, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor out=None)\",\r\n+    \"sspaddmm(Scalar beta, Tensor input, Tensor mat1, Tensor mat2)|deprecated\",\r\n+    \"sspaddmm(Scalar beta, Tensor input, Scalar alpha, Tensor mat1, Tensor mat2)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"stft(Tensor input, int64_t n_fft, int64_t? hop_length=None, int64_t? win_length=None, Tensor? window=None, bool center=True, c10::string_view pad_mode=\\\"reflect\\\", bool normalized=False, bool? onesided=None, bool? return_complex=None)\",\r\n     \"stft(Tensor input, int64_t n_fft, int64_t? hop_length=None, int64_t? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None)\",\r\n+    \"stft(Tensor input, int64_t n_fft, int64_t? hop_length=None, int64_t? win_length=None, Tensor? window=None, bool center=True, c10::string_view pad_mode=\\\"reflect\\\", bool normalized=False, bool? onesided=None, bool? return_complex=None)\",\r\n   static PythonArgParser parser({\r\n-    \"sum(Tensor input, *, ScalarType? dtype=None)\",\r\n     \"sum(Tensor input, IntArrayRef[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor out=None)\",\r\n+    \"sum(Tensor input, *, ScalarType? dtype=None)\",\r\n     \"sum(Tensor input, DimnameList[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"std(Tensor input, IntArrayRef[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor out=None)\",\r\n-    \"std(Tensor input, IntArrayRef[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor out=None)\",\r\n     \"std(Tensor input, bool unbiased=True)\",\r\n-    \"std(Tensor input, DimnameList[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor out=None)\",\r\n+    \"std(Tensor input, IntArrayRef[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor out=None)\",\r\n+    \"std(Tensor input, IntArrayRef[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor out=None)\",\r\n     \"std(Tensor input, DimnameList[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor out=None)\",\r\n+    \"std(Tensor input, DimnameList[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"std_mean(Tensor input, IntArrayRef[1]? dim, bool unbiased=True, bool keepdim=False)\",\r\n-    \"std_mean(Tensor input, IntArrayRef[1]? dim=None, *, Scalar? correction=None, bool keepdim=False)\",\r\n     \"std_mean(Tensor input, bool unbiased=True)\",\r\n-    \"std_mean(Tensor input, DimnameList[1] dim, bool unbiased=True, bool keepdim=False)\",\r\n+    \"std_mean(Tensor input, IntArrayRef[1]? dim=None, *, Scalar? correction=None, bool keepdim=False)\",\r\n+    \"std_mean(Tensor input, IntArrayRef[1]? dim, bool unbiased=True, bool keepdim=False)\",\r\n     \"std_mean(Tensor input, DimnameList[1] dim, *, Scalar? correction=None, bool keepdim=False)\",\r\n+    \"std_mean(Tensor input, DimnameList[1] dim, bool unbiased=True, bool keepdim=False)\",\r\n   static PythonArgParser parser({\r\n-    \"prod(Tensor input, *, ScalarType? dtype=None)\",\r\n     \"prod(Tensor input, int64_t dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor out=None)\",\r\n+    \"prod(Tensor input, *, ScalarType? dtype=None)\",\r\n     \"prod(Tensor input, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"trapz(Tensor y, *, double dx=1, int64_t dim=-1)\",\r\n     \"trapz(Tensor y, Tensor x, *, int64_t dim=-1)\",\r\n+    \"trapz(Tensor y, *, double dx=1, int64_t dim=-1)\",\r\n   static PythonArgParser parser({\r\n-    \"var(Tensor input, IntArrayRef[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor out=None)\",\r\n-    \"var(Tensor input, IntArrayRef[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor out=None)\",\r\n     \"var(Tensor input, bool unbiased=True)\",\r\n-    \"var(Tensor input, DimnameList[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor out=None)\",\r\n+    \"var(Tensor input, IntArrayRef[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor out=None)\",\r\n+    \"var(Tensor input, IntArrayRef[1]? dim, bool unbiased=True, bool keepdim=False, *, Tensor out=None)\",\r\n     \"var(Tensor input, DimnameList[1] dim, *, Scalar? correction=None, bool keepdim=False, Tensor out=None)\",\r\n+    \"var(Tensor input, DimnameList[1] dim, bool unbiased=True, bool keepdim=False, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"var_mean(Tensor input, IntArrayRef[1]? dim, bool unbiased=True, bool keepdim=False)\",\r\n-    \"var_mean(Tensor input, IntArrayRef[1]? dim=None, *, Scalar? correction=None, bool keepdim=False)\",\r\n     \"var_mean(Tensor input, bool unbiased=True)\",\r\n-    \"var_mean(Tensor input, DimnameList[1] dim, bool unbiased=True, bool keepdim=False)\",\r\n+    \"var_mean(Tensor input, IntArrayRef[1]? dim=None, *, Scalar? correction=None, bool keepdim=False)\",\r\n+    \"var_mean(Tensor input, IntArrayRef[1]? dim, bool unbiased=True, bool keepdim=False)\",\r\n     \"var_mean(Tensor input, DimnameList[1] dim, *, Scalar? correction=None, bool keepdim=False)\",\r\n+    \"var_mean(Tensor input, DimnameList[1] dim, bool unbiased=True, bool keepdim=False)\",\r\n   static PythonArgParser parser({\r\n-    \"where(Tensor condition)\",\r\n     \"where(Tensor condition, Tensor input, Tensor other, *, Tensor out=None)\",\r\n-    \"where(Tensor condition, Scalar self, Tensor other)\",\r\n+    \"where(Tensor condition)\",\r\n     \"where(Tensor condition, Tensor input, Scalar other)\",\r\n+    \"where(Tensor condition, Scalar self, Tensor other)\",\r\n     \"where(Tensor condition, Scalar self, Scalar other)\",\r\n   static PythonArgParser parser({\r\n-    \"zeros(IntArrayRef size, *, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n     \"zeros(SymIntArrayRef size, *, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"zeros(IntArrayRef size, *, DimnameList? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n   static PythonArgParser parser({\r\n-    \"native_norm(Tensor input, Scalar p=2)\",\r\n     \"native_norm(Tensor input, Scalar? p, IntArrayRef[1] dim, bool keepdim, ScalarType? dtype)\",\r\n+    \"native_norm(Tensor input, Scalar p=2)\",\r\n   static PythonArgParser parser({\r\n-    \"_sparse_sum(Tensor input)\",\r\n-    \"_sparse_sum(Tensor input, *, ScalarType dtype)\",\r\n-    \"_sparse_sum(Tensor input, IntArrayRef[1] dim)\",\r\n     \"_sparse_sum(Tensor input, IntArrayRef[1] dim, *, ScalarType dtype)\",\r\n+    \"_sparse_sum(Tensor input, IntArrayRef[1] dim)\",\r\n+    \"_sparse_sum(Tensor input, *, ScalarType dtype)\",\r\n+    \"_sparse_sum(Tensor input)\",\r\n   static PythonArgParser parser({\r\n-    \"norm(Tensor input, Scalar p=2)\",\r\n-    \"norm(Tensor input, Scalar? p, *, ScalarType dtype)\",\r\n-    \"norm(Tensor input, Scalar? p, IntArrayRef[1] dim, bool keepdim, *, ScalarType dtype, Tensor out=None)\",\r\n     \"norm(Tensor input, Scalar? p, IntArrayRef[1] dim, bool keepdim=False, *, Tensor out=None)\",\r\n-    \"norm(Tensor input, Scalar? p, DimnameList[1] dim, bool keepdim, *, ScalarType dtype, Tensor out=None)\",\r\n+    \"norm(Tensor input, Scalar? p, IntArrayRef[1] dim, bool keepdim, *, ScalarType dtype, Tensor out=None)\",\r\n+    \"norm(Tensor input, Scalar? p, *, ScalarType dtype)\",\r\n+    \"norm(Tensor input, Scalar p=2)\",\r\n     \"norm(Tensor input, Scalar? p, DimnameList[1] dim, bool keepdim=False, *, Tensor out=None)\",\r\n+    \"norm(Tensor input, Scalar? p, DimnameList[1] dim, bool keepdim, *, ScalarType dtype, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"nuclear_norm(Tensor input, IntArrayRef[2] dim, bool keepdim=False, *, Tensor out=None)\",\r\n     \"nuclear_norm(Tensor input, bool keepdim=False, *, Tensor out=None)\",\r\n+    \"nuclear_norm(Tensor input, IntArrayRef[2] dim, bool keepdim=False, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"sub(Tensor input, Scalar alpha, Tensor other, *, Tensor out=None)|deprecated\",\r\n     \"sub(Tensor input, Tensor other, *, Scalar alpha=1, Tensor out=None)\",\r\n+    \"sub(Tensor input, Scalar alpha, Tensor other, *, Tensor out=None)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"addmm(Scalar beta, Tensor input, Scalar alpha, Tensor mat1, Tensor mat2, *, Tensor out=None)|deprecated\",\r\n-    \"addmm(Scalar beta, Tensor input, Tensor mat1, Tensor mat2, *, Tensor out=None)|deprecated\",\r\n     \"addmm(Tensor input, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor out=None)\",\r\n+    \"addmm(Scalar beta, Tensor input, Tensor mat1, Tensor mat2, *, Tensor out=None)|deprecated\",\r\n+    \"addmm(Scalar beta, Tensor input, Scalar alpha, Tensor mat1, Tensor mat2, *, Tensor out=None)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"quantize_per_tensor(Tensor input, Tensor scale, Tensor zero_point, ScalarType dtype)\",\r\n-    \"quantize_per_tensor(Tensor input, double scale, int64_t zero_point, ScalarType dtype)\",\r\n     \"quantize_per_tensor(TensorList tensors, Tensor scales, Tensor zero_points, ScalarType dtype)\",\r\n+    \"quantize_per_tensor(Tensor input, double scale, int64_t zero_point, ScalarType dtype)\",\r\n+    \"quantize_per_tensor(Tensor input, Tensor scale, Tensor zero_point, ScalarType dtype)\",\r\n   static PythonArgParser parser({\r\n-    \"dequantize(Tensor input)\",\r\n     \"dequantize(TensorList tensors)\",\r\n+    \"dequantize(Tensor input)\",\r\n   static PythonArgParser parser({\r\n-    \"fake_quantize_per_tensor_affine(Tensor input, Tensor scale, Tensor zero_point, int64_t quant_min, int64_t quant_max)\",\r\n     \"fake_quantize_per_tensor_affine(Tensor input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max)\",\r\n+    \"fake_quantize_per_tensor_affine(Tensor input, Tensor scale, Tensor zero_point, int64_t quant_min, int64_t quant_max)\",\r\n   static PythonArgParser parser({\r\n-    \"meshgrid(TensorList tensors)\",\r\n     \"meshgrid(TensorList tensors, *, c10::string_view indexing)\",\r\n+    \"meshgrid(TensorList tensors)\",\r\n   static PythonArgParser parser({\r\n     \"result_type(Tensor tensor, Tensor other)\",\r\n-    \"result_type(Scalar scalar, Tensor tensor)\",\r\n     \"result_type(Tensor tensor, Scalar other)\",\r\n+    \"result_type(Scalar scalar, Tensor tensor)\",\r\n     \"result_type(Scalar scalar1, Scalar scalar2)\",\r\n   static PythonArgParser parser({\r\n-    \"lstm(Tensor data, Tensor batch_sizes, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional)\",\r\n     \"lstm(Tensor input, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first)\",\r\n+    \"lstm(Tensor data, Tensor batch_sizes, TensorList hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional)\",\r\n   static PythonArgParser parser({\r\n-    \"gru(Tensor data, Tensor batch_sizes, Tensor hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional)\",\r\n     \"gru(Tensor input, Tensor hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first)\",\r\n+    \"gru(Tensor data, Tensor batch_sizes, Tensor hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional)\",\r\n   static PythonArgParser parser({\r\n-    \"rnn_tanh(Tensor data, Tensor batch_sizes, Tensor hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional)\",\r\n     \"rnn_tanh(Tensor input, Tensor hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first)\",\r\n+    \"rnn_tanh(Tensor data, Tensor batch_sizes, Tensor hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional)\",\r\n   static PythonArgParser parser({\r\n-    \"rnn_relu(Tensor data, Tensor batch_sizes, Tensor hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional)\",\r\n     \"rnn_relu(Tensor input, Tensor hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional, bool batch_first)\",\r\n+    \"rnn_relu(Tensor data, Tensor batch_sizes, Tensor hx, TensorList params, bool has_biases, int64_t num_layers, double dropout, bool train, bool bidirectional)\",\r\n   static PythonArgParser parser({\r\n     \"index_fill(Tensor input, int64_t dim, Tensor index, Tensor value)\",\r\n-    \"index_fill(Tensor input, Dimname dim, Tensor index, Tensor value)\",\r\n     \"index_fill(Tensor input, int64_t dim, Tensor index, Scalar value)\",\r\n+    \"index_fill(Tensor input, Dimname dim, Tensor index, Tensor value)\",\r\n     \"index_fill(Tensor input, Dimname dim, Tensor index, Scalar value)\",\r\n   static PythonArgParser parser({\r\n-    \"scatter(Tensor input, int64_t dim, Tensor index, Tensor src, *, Tensor out=None)\",\r\n     \"scatter(Tensor input, int64_t dim, Tensor index, Tensor src, *, c10::string_view reduce, Tensor out=None)\",\r\n-    \"scatter(Tensor input, Dimname dim, Tensor index, Tensor src)\",\r\n-    \"scatter(Tensor input, int64_t dim, Tensor index, Scalar value, *, Tensor out=None)\",\r\n+    \"scatter(Tensor input, int64_t dim, Tensor index, Tensor src, *, Tensor out=None)\",\r\n     \"scatter(Tensor input, int64_t dim, Tensor index, Scalar value, *, c10::string_view reduce, Tensor out=None)\",\r\n+    \"scatter(Tensor input, int64_t dim, Tensor index, Scalar value, *, Tensor out=None)\",\r\n+    \"scatter(Tensor input, Dimname dim, Tensor index, Tensor src)\",\r\n     \"scatter(Tensor input, Dimname dim, Tensor index, Scalar value)\",\r\n   static PythonArgParser parser({\r\n     \"bitwise_and(Tensor input, Tensor other, *, Tensor out=None)\",\r\n-    \"bitwise_and(Scalar self, Tensor other)\",\r\n     \"bitwise_and(Tensor input, Scalar other, *, Tensor out=None)\",\r\n+    \"bitwise_and(Scalar self, Tensor other)\",\r\n   static PythonArgParser parser({\r\n     \"bitwise_or(Tensor input, Tensor other, *, Tensor out=None)\",\r\n-    \"bitwise_or(Scalar self, Tensor other)\",\r\n     \"bitwise_or(Tensor input, Scalar other, *, Tensor out=None)\",\r\n+    \"bitwise_or(Scalar self, Tensor other)\",\r\n   static PythonArgParser parser({\r\n     \"bitwise_xor(Tensor input, Tensor other, *, Tensor out=None)\",\r\n-    \"bitwise_xor(Scalar self, Tensor other)\",\r\n     \"bitwise_xor(Tensor input, Scalar other, *, Tensor out=None)\",\r\n+    \"bitwise_xor(Scalar self, Tensor other)\",\r\n   static PythonArgParser parser({\r\n     \"bitwise_left_shift(Tensor input, Tensor other, *, Tensor out=None)\",\r\n-    \"bitwise_left_shift(Scalar self, Tensor other)\",\r\n     \"bitwise_left_shift(Tensor input, Scalar other, *, Tensor out=None)\",\r\n+    \"bitwise_left_shift(Scalar self, Tensor other)\",\r\n   static PythonArgParser parser({\r\n     \"bitwise_right_shift(Tensor input, Tensor other, *, Tensor out=None)\",\r\n-    \"bitwise_right_shift(Scalar self, Tensor other)\",\r\n     \"bitwise_right_shift(Tensor input, Scalar other, *, Tensor out=None)\",\r\n+    \"bitwise_right_shift(Scalar self, Tensor other)\",\r\n   static PythonArgParser parser({\r\n-    \"addbmm(Scalar beta, Tensor input, Scalar alpha, Tensor batch1, Tensor batch2, *, Tensor out=None)|deprecated\",\r\n-    \"addbmm(Scalar beta, Tensor input, Tensor batch1, Tensor batch2, *, Tensor out=None)|deprecated\",\r\n     \"addbmm(Tensor input, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor out=None)\",\r\n+    \"addbmm(Scalar beta, Tensor input, Tensor batch1, Tensor batch2, *, Tensor out=None)|deprecated\",\r\n+    \"addbmm(Scalar beta, Tensor input, Scalar alpha, Tensor batch1, Tensor batch2, *, Tensor out=None)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"addcmul(Tensor input, Scalar value, Tensor tensor1, Tensor tensor2, *, Tensor out=None)|deprecated\",\r\n     \"addcmul(Tensor input, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor out=None)\",\r\n+    \"addcmul(Tensor input, Scalar value, Tensor tensor1, Tensor tensor2, *, Tensor out=None)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"addcdiv(Tensor input, Scalar value, Tensor tensor1, Tensor tensor2, *, Tensor out=None)|deprecated\",\r\n     \"addcdiv(Tensor input, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor out=None)\",\r\n+    \"addcdiv(Tensor input, Scalar value, Tensor tensor1, Tensor tensor2, *, Tensor out=None)|deprecated\",\r\n   static PythonArgParser parser({\r\n-    \"histogram(Tensor input, Tensor bins, *, Tensor? weight=None, bool density=False, TensorList[2] out=None)\",\r\n     \"histogram(Tensor input, int64_t bins=100, *, ArrayRef<double>? range=None, Tensor? weight=None, bool density=False, TensorList[2] out=None)\",\r\n+    \"histogram(Tensor input, Tensor bins, *, Tensor? weight=None, bool density=False, TensorList[2] out=None)\",\r\n   static PythonArgParser parser({\r\n     \"remainder(Tensor input, Tensor other, *, Tensor out=None)\",\r\n-    \"remainder(Scalar self, Tensor other)\",\r\n     \"remainder(Tensor input, Scalar other, *, Tensor out=None)\",\r\n+    \"remainder(Scalar self, Tensor other)\",\r\n   static PythonArgParser parser({\r\n-    \"quantile(Tensor input, Tensor q, int64_t? dim=None, bool keepdim=False, *, c10::string_view interpolation=\\\"linear\\\", Tensor out=None)\",\r\n     \"quantile(Tensor input, double q, int64_t? dim=None, bool keepdim=False, *, c10::string_view interpolation=\\\"linear\\\", Tensor out=None)\",\r\n+    \"quantile(Tensor input, Tensor q, int64_t? dim=None, bool keepdim=False, *, c10::string_view interpolation=\\\"linear\\\", Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"nanquantile(Tensor input, Tensor q, int64_t? dim=None, bool keepdim=False, *, c10::string_view interpolation=\\\"linear\\\", Tensor out=None)\",\r\n     \"nanquantile(Tensor input, double q, int64_t? dim=None, bool keepdim=False, *, c10::string_view interpolation=\\\"linear\\\", Tensor out=None)\",\r\n+    \"nanquantile(Tensor input, Tensor q, int64_t? dim=None, bool keepdim=False, *, c10::string_view interpolation=\\\"linear\\\", Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"sort(Tensor input, *, bool? stable, int64_t dim=-1, bool descending=False, TensorList[2] out=None)\",\r\n     \"sort(Tensor input, int64_t dim=-1, bool descending=False, *, TensorList[2] out=None)\",\r\n-    \"sort(Tensor input, *, bool? stable, Dimname dim, bool descending=False, TensorList[2] out=None)\",\r\n+    \"sort(Tensor input, *, bool? stable, int64_t dim=-1, bool descending=False, TensorList[2] out=None)\",\r\n     \"sort(Tensor input, Dimname dim, bool descending=False, *, TensorList[2] out=None)\",\r\n+    \"sort(Tensor input, *, bool? stable, Dimname dim, bool descending=False, TensorList[2] out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"argsort(Tensor input, *, bool stable, int64_t dim=-1, bool descending=False)\",\r\n     \"argsort(Tensor input, int64_t dim=-1, bool descending=False)\",\r\n+    \"argsort(Tensor input, *, bool stable, int64_t dim=-1, bool descending=False)\",\r\n     \"argsort(Tensor input, Dimname dim, bool descending=False)\",\r\n   static PythonArgParser parser({\r\n     \"pow(Tensor input, Tensor exponent, *, Tensor out=None)\",\r\n-    \"pow(Scalar self, Tensor exponent, *, Tensor out=None)\",\r\n     \"pow(Tensor input, Scalar exponent, *, Tensor out=None)\",\r\n+    \"pow(Scalar self, Tensor exponent, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n     \"float_power(Tensor input, Tensor exponent, *, Tensor out=None)\",\r\n-    \"float_power(Scalar self, Tensor exponent, *, Tensor out=None)\",\r\n     \"float_power(Tensor input, Scalar exponent, *, Tensor out=None)\",\r\n+    \"float_power(Scalar self, Tensor exponent, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"normal(Tensor mean, Tensor std, *, Generator? generator=None, Tensor out=None)\",\r\n-    \"normal(Tensor mean, double std=1, *, Generator? generator=None, Tensor out=None)\",\r\n-    \"normal(double mean, Tensor std, *, Generator? generator=None, Tensor out=None)\",\r\n     \"normal(double mean, double std, SymIntArrayRef size, *, Generator? generator=None, Tensor out=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False, bool? requires_grad=False)\",\r\n+    \"normal(double mean, Tensor std, *, Generator? generator=None, Tensor out=None)\",\r\n+    \"normal(Tensor mean, double std=1, *, Generator? generator=None, Tensor out=None)\",\r\n+    \"normal(Tensor mean, Tensor std, *, Generator? generator=None, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_add(TensorList self, Scalar scalar)\",\r\n-    \"_foreach_add(TensorList self, ScalarList scalars)\",\r\n-    \"_foreach_add(TensorList self, Tensor other, Scalar alpha=1)\",\r\n     \"_foreach_add(TensorList self, TensorList other, *, Scalar alpha=1)\",\r\n+    \"_foreach_add(TensorList self, Tensor other, Scalar alpha=1)\",\r\n+    \"_foreach_add(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_add(TensorList self, Scalar scalar)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_add_(TensorList self, Scalar scalar)\",\r\n-    \"_foreach_add_(TensorList self, ScalarList scalars)\",\r\n-    \"_foreach_add_(TensorList self, Tensor other, Scalar alpha=1)\",\r\n     \"_foreach_add_(TensorList self, TensorList other, *, Scalar alpha=1)\",\r\n+    \"_foreach_add_(TensorList self, Tensor other, Scalar alpha=1)\",\r\n+    \"_foreach_add_(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_add_(TensorList self, Scalar scalar)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_sub(TensorList self, Scalar scalar)\",\r\n-    \"_foreach_sub(TensorList self, ScalarList scalars)\",\r\n     \"_foreach_sub(TensorList self, TensorList other, *, Scalar alpha=1)\",\r\n+    \"_foreach_sub(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_sub(TensorList self, Scalar scalar)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_sub_(TensorList self, Scalar scalar)\",\r\n-    \"_foreach_sub_(TensorList self, ScalarList scalars)\",\r\n     \"_foreach_sub_(TensorList self, TensorList other, *, Scalar alpha=1)\",\r\n+    \"_foreach_sub_(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_sub_(TensorList self, Scalar scalar)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_mul(TensorList self, ScalarList scalars)\",\r\n     \"_foreach_mul(TensorList self, Tensor other)\",\r\n-    \"_foreach_mul(TensorList self, TensorList other)\",\r\n+    \"_foreach_mul(TensorList self, ScalarList scalars)\",\r\n     \"_foreach_mul(TensorList self, Scalar scalar)\",\r\n+    \"_foreach_mul(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_mul_(TensorList self, ScalarList scalars)\",\r\n     \"_foreach_mul_(TensorList self, Tensor other)\",\r\n-    \"_foreach_mul_(TensorList self, TensorList other)\",\r\n+    \"_foreach_mul_(TensorList self, ScalarList scalars)\",\r\n     \"_foreach_mul_(TensorList self, Scalar scalar)\",\r\n+    \"_foreach_mul_(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_div(TensorList self, Scalar scalar)\",\r\n     \"_foreach_div(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_div(TensorList self, Scalar scalar)\",\r\n     \"_foreach_div(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_div_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_div_(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_div_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_div_(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_clamp_max(TensorList self, Scalar scalar)\",\r\n     \"_foreach_clamp_max(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_clamp_max(TensorList self, Scalar scalar)\",\r\n     \"_foreach_clamp_max(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_clamp_max_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_clamp_max_(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_clamp_max_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_clamp_max_(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_clamp_min(TensorList self, Scalar scalar)\",\r\n     \"_foreach_clamp_min(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_clamp_min(TensorList self, Scalar scalar)\",\r\n     \"_foreach_clamp_min(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_clamp_min_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_clamp_min_(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_clamp_min_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_clamp_min_(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_maximum(TensorList self, Scalar scalar)\",\r\n     \"_foreach_maximum(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_maximum(TensorList self, Scalar scalar)\",\r\n     \"_foreach_maximum(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_maximum_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_maximum_(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_maximum_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_maximum_(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_minimum(TensorList self, Scalar scalar)\",\r\n     \"_foreach_minimum(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_minimum(TensorList self, Scalar scalar)\",\r\n     \"_foreach_minimum(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_minimum_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_minimum_(TensorList self, ScalarList scalars)\",\r\n+    \"_foreach_minimum_(TensorList self, Scalar scalar)\",\r\n     \"_foreach_minimum_(TensorList self, TensorList other)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_addcdiv(TensorList self, TensorList tensor1, TensorList tensor2, ScalarList scalars)\",\r\n     \"_foreach_addcdiv(TensorList self, TensorList tensor1, TensorList tensor2, Tensor scalars)\",\r\n+    \"_foreach_addcdiv(TensorList self, TensorList tensor1, TensorList tensor2, ScalarList scalars)\",\r\n     \"_foreach_addcdiv(TensorList self, TensorList tensor1, TensorList tensor2, Scalar value=1)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_addcdiv_(TensorList self, TensorList tensor1, TensorList tensor2, ScalarList scalars)\",\r\n     \"_foreach_addcdiv_(TensorList self, TensorList tensor1, TensorList tensor2, Tensor scalars)\",\r\n+    \"_foreach_addcdiv_(TensorList self, TensorList tensor1, TensorList tensor2, ScalarList scalars)\",\r\n     \"_foreach_addcdiv_(TensorList self, TensorList tensor1, TensorList tensor2, Scalar value=1)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_addcmul(TensorList self, TensorList tensor1, TensorList tensor2, ScalarList scalars)\",\r\n     \"_foreach_addcmul(TensorList self, TensorList tensor1, TensorList tensor2, Tensor scalars)\",\r\n+    \"_foreach_addcmul(TensorList self, TensorList tensor1, TensorList tensor2, ScalarList scalars)\",\r\n     \"_foreach_addcmul(TensorList self, TensorList tensor1, TensorList tensor2, Scalar value=1)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_addcmul_(TensorList self, TensorList tensor1, TensorList tensor2, ScalarList scalars)\",\r\n     \"_foreach_addcmul_(TensorList self, TensorList tensor1, TensorList tensor2, Tensor scalars)\",\r\n+    \"_foreach_addcmul_(TensorList self, TensorList tensor1, TensorList tensor2, ScalarList scalars)\",\r\n     \"_foreach_addcmul_(TensorList self, TensorList tensor1, TensorList tensor2, Scalar value=1)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_lerp(TensorList self, TensorList tensors1, Scalar weight)\",\r\n     \"_foreach_lerp(TensorList self, TensorList tensors1, TensorList weights)\",\r\n+    \"_foreach_lerp(TensorList self, TensorList tensors1, Scalar weight)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_lerp_(TensorList self, TensorList tensors1, Scalar weight)\",\r\n     \"_foreach_lerp_(TensorList self, TensorList tensors1, TensorList weights)\",\r\n+    \"_foreach_lerp_(TensorList self, TensorList tensors1, Scalar weight)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_pow(Scalar self, TensorList exponent)\",\r\n-    \"_foreach_pow(TensorList self, Scalar exponent)\",\r\n     \"_foreach_pow(TensorList self, ScalarList exponent)\",\r\n+    \"_foreach_pow(TensorList self, Scalar exponent)\",\r\n+    \"_foreach_pow(Scalar self, TensorList exponent)\",\r\n     \"_foreach_pow(TensorList self, TensorList exponent)\",\r\n   static PythonArgParser parser({\r\n-    \"_foreach_pow_(TensorList self, Scalar exponent)\",\r\n     \"_foreach_pow_(TensorList self, ScalarList exponent)\",\r\n+    \"_foreach_pow_(TensorList self, Scalar exponent)\",\r\n     \"_foreach_pow_(TensorList self, TensorList exponent)\",\r\n   static PythonArgParser parser({\r\n-    \"_test_autograd_multiple_dispatch(Tensor input)\",\r\n     \"_test_autograd_multiple_dispatch(Tensor input, bool b)\",\r\n+    \"_test_autograd_multiple_dispatch(Tensor input)\",\r\n   static PythonArgParser parser({\r\n-    \"squeeze_copy(Tensor input, *, Tensor out=None)\",\r\n     \"squeeze_copy(Tensor input, int64_t dim, *, Tensor out=None)\",\r\n+    \"squeeze_copy(Tensor input, *, Tensor out=None)\",\r\n     \"squeeze_copy(Tensor input, IntArrayRef dim, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"view_copy(Tensor input, ScalarType dtype, *, Tensor out=None)\",\r\n     \"view_copy(Tensor input, SymIntArrayRef size, *, Tensor out=None)\",\r\n+    \"view_copy(Tensor input, ScalarType dtype, *, Tensor out=None)\",\r\n   static PythonArgParser parser({\r\n-    \"_fused_adam_(TensorList self, TensorList grads, TensorList exp_avgs, TensorList exp_avg_sqs, TensorList max_exp_avg_sqs, TensorList state_steps, *, Tensor lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None)\",\r\n     \"_fused_adam_(TensorList self, TensorList grads, TensorList exp_avgs, TensorList exp_avg_sqs, TensorList max_exp_avg_sqs, TensorList state_steps, *, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None)\",\r\n+    \"_fused_adam_(TensorList self, TensorList grads, TensorList exp_avgs, TensorList exp_avg_sqs, TensorList max_exp_avg_sqs, TensorList state_steps, *, Tensor lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None)\",\r\n   static PythonArgParser parser({\r\n-    \"_fused_adamw_(TensorList self, TensorList grads, TensorList exp_avgs, TensorList exp_avg_sqs, TensorList max_exp_avg_sqs, TensorList state_steps, *, Tensor lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None)\",\r\n     \"_fused_adamw_(TensorList self, TensorList grads, TensorList exp_avgs, TensorList exp_avg_sqs, TensorList max_exp_avg_sqs, TensorList state_steps, *, double lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None)\",\r\n+    \"_fused_adamw_(TensorList self, TensorList grads, TensorList exp_avgs, TensorList exp_avg_sqs, TensorList max_exp_avg_sqs, TensorList state_steps, *, Tensor lr, double beta1, double beta2, double weight_decay, double eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None)\",\r\n```\r\n\r\n<details>\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111214\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 48,
        "deletions": 3,
        "changed_files": 3,
        "created_at": "2023-10-13T17:26:33Z",
        "closed_at": "2023-10-13T17:26:42Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111213\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T17:20:57Z",
        "closed_at": "2023-10-13T21:36:37Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111212\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-13T16:56:03Z",
        "closed_at": "2023-10-14T00:10:08Z",
        "merged_at": null,
        "body": "The CUDA architecture flags from TORCH_CUDA_ARCH_LIST will be skipped if the TORCH_EXTENSION_NAME includes the substring \"arch\". A C++ Extension should be allowed to have any name. I just manually skip the TORCH_EXTENSION_NAME flag when checking if one of the flags is \"arch\". There is probably a better fix, but I'll leave this to experts.\r\n\r\n\r\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 46,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2023-10-13T14:15:32Z",
        "closed_at": "2023-10-13T18:36:20Z",
        "merged_at": null,
        "body": "Fixes https://github.com/pytorch/pytorch/issues/111199\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/111203\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @ezyang ",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 21,
        "changed_files": 2,
        "created_at": "2023-10-13T14:08:13Z",
        "closed_at": "2023-10-13T18:46:34Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111207\n\nThis dictionary is not used anywhere. The _make_dupe_guard function does\nnot exist anymore\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2023-10-13T13:54:14Z",
        "closed_at": "2023-10-13T21:17:48Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111205\n\nFixes https://github.com/pytorch/pytorch/issues/111119\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 268,
        "deletions": 146,
        "changed_files": 6,
        "created_at": "2023-10-13T13:38:16Z",
        "closed_at": "2023-10-13T22:20:04Z",
        "merged_at": null,
        "body": "Summary: Pull Request resolved: https://github.com/pytorch/executorch/pull/845\n\nTest Plan: CI\n\nDifferential Revision: D50191531\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @avikchaudhuri @gmagogsfm @tugsbayasgalan",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 208,
        "deletions": 177,
        "changed_files": 12,
        "created_at": "2023-10-13T11:35:39Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #108420\n* #110524\n* __->__ #111196\n* #110523\n* #110522\n\nThis prepares the PR where we implement sets in terms of dicts.\n\nWhile implementing this I had to make hashable keys somewhat consistent\nall throughout. I did that by having an auxiliary class (similar to the\none that was inside SetVariable). This variable is opaque on purpose, so\nthat it fails hard if it unadvertedly leaks back into user code.\n\nWe also found and fixed a number of latent bugs, like where dict with\nbuiltin types was going through to UserDefinedObjectVariable, even\nthough we had a test that was testing that this worked (the test was not\ntesting whether we were tracing anything or not).\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 152,
        "deletions": 129,
        "changed_files": 6,
        "created_at": "2023-10-13T09:56:57Z",
        "closed_at": null,
        "merged_at": null,
        "body": "First time contributor helping out with issue #106571\r\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T08:55:59Z",
        "closed_at": "2023-10-17T03:36:35Z",
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-13T08:32:16Z",
        "closed_at": "2023-10-13T09:57:47Z",
        "merged_at": null,
        "body": "As a part of the migrating linux arm64 runners to the autoscaling group, fixed description of the run_on input for the linux-binary-test ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-13T03:55:20Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Currently this gives this warning:\r\n[W Resize.cpp:35] Warning: An output with one or more elements was resized since it had shape [10, 10], which does not match the required output shape [1, 12, 10, 10]. This behavior is\r\ndeprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (function _resize_output_check)\r\n\r\nFixes #ISSUE_NUMBER\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-10-13T03:52:52Z",
        "closed_at": "2023-10-13T22:20:12Z",
        "merged_at": null,
        "body": "Enable [iter-method-return-iterable (PYI045)](https://docs.astral.sh/ruff/rules/iter-method-return-iterable/#iter-method-return-iterable-pyi045)\r\n\r\nLink: #110950",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 9,
        "changed_files": 7,
        "created_at": "2023-10-13T03:28:49Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Enable [unsupported-method-call-on-all (PYI056)](https://docs.astral.sh/ruff/rules/unsupported-method-call-on-all/#unsupported-method-call-on-all-pyi056)\r\n\r\nLink: #110950",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-13T03:22:31Z",
        "closed_at": "2023-10-13T20:07:54Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111181\r\n\r\nSummary: Auto label all torch/_export changes with ciflow/inductor to trigger AOTInductor tests.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 46,
        "deletions": 16,
        "changed_files": 2,
        "created_at": "2023-10-13T02:51:48Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Possible hacky fix to: https://github.com/pytorch/pytorch/issues/110738\r\n\r\nI doubt you would approve @lezcano, but sympy does not handle bool -> int casting as python does.\r\n\r\nI don't think this is complete, but it is at least a start so I'll leave it up here in case anyone has a better idea for how to continue. \r\n\r\n\r\ncc @lezcano @peterbell10  ",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 159,
        "deletions": 168,
        "changed_files": 4,
        "created_at": "2023-10-13T02:41:01Z",
        "closed_at": "2023-10-15T11:50:05Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111177\n* #111176\n* #111166\n* #111160\n\r\nWe see use cases where embedding sharding is also needed in TP API so we enabled it in the API since DTensor already support colwise embedding sharding.\r\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2023-10-13T01:33:37Z",
        "closed_at": "2023-10-15T00:39:31Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111177\n* __->__ #111176\n* #111166\n* #111160\n\r\nAs part of TP UX improvements, we want to keep our API simple (not easy) so that users get the flexibility to do what they want and avoid a too generic API which tries to solve everything and get things too complicated. We are updating the doc accordingly.\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2023-10-13T00:57:57Z",
        "closed_at": "2023-10-13T01:13:39Z",
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-10-13T00:43:15Z",
        "closed_at": "2023-10-13T16:28:09Z",
        "merged_at": null,
        "body": "Summary:\nwithout the `all` in the fix\n```\nnode.kwargs.get(\"beta\", 1.0) == 1.0\nnode.kwargs.get(\"alpha\", 1.0) == 1.0\nand len(input_shape) == 2\nand len(weight_shape) == 2\nand all(x % 2 == 0 for x in input_shape + weight_shape)\nand shape <= MAX_FUSE_TENSOR_SIZE_GROUP_LINEAR # <----- HERE\nfor shape in input_shape + weight_shape\n```\nthis statement defaults to a generator object which means it will always be true. One of the issues is that the shapes could be an odd number which forces gmm to load element-by-element rather than vectorized load. In VDDv3 torchbench example(posted in test plan), you can see there is a 37ms GMM call which swamps any gain from fusion. Overall this change makes the GMM fusion 24% faster\n\nDifferential Revision: D48696572\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 399,
        "deletions": 31,
        "changed_files": 4,
        "created_at": "2023-10-13T00:29:36Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111172\n\nSummary:\nPreviously we actually did not really support this, this PR added the support.\n\nNext\n* clean up insert observer logic\n* add allow_transitive_sharing boolean flag to allow people to turn this op for certain edges\n\nTest Plan:\npython test/test_quantization.py TestQuantizePT2E.test_shared_qspec_transitivity\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D50250789](https://our.internmc.facebook.com/intern/diff/D50250789)",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 13,
        "changed_files": 3,
        "created_at": "2023-10-13T00:14:47Z",
        "closed_at": "2023-10-13T15:49:12Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111171\n\n\n\ncc @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 19,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-12T23:28:15Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Differential Revision: D50246956\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 13,
        "changed_files": 2,
        "created_at": "2023-10-12T22:46:13Z",
        "closed_at": "2023-10-14T21:41:40Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111167\n\nSummary: When a fallback kernel is called without specifying any kwargs, we still need to fill in default values for those kwargs when generating cpp call.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 152,
        "deletions": 6,
        "changed_files": 6,
        "created_at": "2023-10-12T22:27:58Z",
        "closed_at": "2023-10-14T15:37:59Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111176\n* __->__ #111166\n* #111160\n\r\nIn some use cases, we found that users might want to annote the input/output DTensor layout for the parent module rather than the submodule whose parameters are to be distributed so that we want to have these two class for users to annote input/output DTensor layouts so that we register pre-FWD/FWD hook for the TP-lized module.\r\n\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 532,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-12T22:01:17Z",
        "closed_at": "2023-10-13T20:36:28Z",
        "merged_at": null,
        "body": "Summary: Add libraries to apply tensor parallel transformation to an exported program.\n\nTest Plan: buck test mode/opt  -c fbcode.enable_gpu_sections=true //caffe2/test/distributed/_tensor/experimental:tensor_parallel_sharding\n\nDifferential Revision: D50214796\n\n\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 7,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-12T21:20:17Z",
        "closed_at": "2023-10-13T03:52:33Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111164\r\n\r\nThis PR will cause logging.exception() to also dump the exception and stacktrace. Copied from https://github.com/python/cpython/blob/74723e11109a320e628898817ab449b3dad9ee96/Lib/logging/__init__.py#L707-L711\r\n\r\nrepro:\r\n\r\n<details>\r\n\r\n```python\r\nimport torch\r\nimport torch._inductor.config\r\n\r\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = \"runtime_error\"\r\n\r\ndef fn(x, y):\r\n    return (x @ y).relu()\r\n\r\nx, y = [torch.rand((16, 16), device='cuda') for _ in range (2)]\r\ntorch.compile(fn)(x, y)\r\n```\r\nrun with TORCHDYNAMO_REPRO_AFTER=aot TORCHDYNAMO_REPRO_LEVEL=4\r\n\r\n</details>\r\n\r\nbefore:\r\n```\r\n...\r\n[2023-10-12 14:18:52,902] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph.\r\n```\r\n\r\nnow:\r\n```\r\n...\r\n[2023-10-12 14:18:52,902] torch._dynamo.debug_utils: [ERROR] While minifying the program in accuracy minification mode, ran into a runtime exception which is likely an unrelated issue. Skipping this graph.\r\nTraceback (most recent call last):\r\n  File \"/data/users/dberard/scripts/relu_accuracy_issue.py\", line 10, in <module>\r\n    torch.compile(fn)(x, y)\r\n...\r\n```",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2519,
        "deletions": 0,
        "changed_files": 17,
        "created_at": "2023-10-12T21:02:23Z",
        "closed_at": "2023-10-13T09:35:41Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111163\n\r\n\r\nThis PR addresses the persistent issue of merge conflicts in the benchmarks/dynamo/ci_expected_accuracy/ directory, specifically those arising from frequently updated CSV files. Based on @malfet  suggestion, the solution implemented adds three spaces between each line in the CSV files. This approach has proven effective in preventing merge conflicts, as evidenced in [D50239634](https://www.internalfb.com/intern/diff/D50239634/). Regardless of these changes the extra new lines should still allow the csvs to be ingested as normal.\r\n\r\nIf you have access to the diff:\r\nNormally, modifying a line that is later altered in the stack results in a merge conflict during restacking. With this new spacing strategy, lines that are not modified further down the stack will not trigger merge conflicts, achieving our intended outcome.\r\n\r\ncc @albanD @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 50,
        "deletions": 11,
        "changed_files": 3,
        "created_at": "2023-10-12T20:55:49Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111162\r\n* #111091\r\n* #110900\r\n* #110898\r\n\r\nThis PR fixes the dtype conversion issue when calling nn.Module.to after\r\nswapping parameters with DTensors, the problem was nn.Module._apply by\r\ndefault uses a `.data` setting way to swap out storages directly instead\r\nof just replacing the nn.Parameters. For wrapper tensor subclasses, it\r\nwould not work as wrapper tensor subclass is different from normal\r\ntensor as its real storage points separately, i.e.\r\n\r\n* DTensor wrapper subclass instance `storage` does not point to real storage\r\n* The real storage of DTensor pointed to its local shard. But if we do `.data=` it seems that the change will not propagate into the DTensor's local shard\r\n\r\nThis probably also fix https://github.com/pytorch/pytorch/issues/102812",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 11,
        "changed_files": 2,
        "created_at": "2023-10-12T20:46:41Z",
        "closed_at": "2023-10-13T00:28:42Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111161\n\nDifferential Revision: [D50240459](https://our.internmc.facebook.com/intern/diff/D50240459/)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 379,
        "deletions": 51,
        "changed_files": 6,
        "created_at": "2023-10-12T20:45:19Z",
        "closed_at": "2023-10-14T15:26:43Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111176\n* #111166\n* __->__ #111160\n\r\n\r\nOne thing we find it challenging for users is that we don't want to expose the concept of prepare_input and prepare_out to users since there are so many func names for users to select from which is quite confusing. On the other hand, the colwise and rowwise parallel always need input(out) and output(in) to be certain layout so we can somehow simplify the logic here and make it more usable.\r\n\r\nSo we added three public attributes to the parallelStyle here and the code logic is like:\r\n\r\n```python\r\nclass ParallelStyle(ABC):\r\n    \"\"\"\r\n    The parallel style user wants the module or submodule to be parallelized.\r\n    We can add more in future, but this seems sufficient for immediate needs. Users can extend this class to build their own parallel style with customized input/output preparations.\r\n  \"\"\"\r\n    input_layouts: Union[placement, Tuple[placement]]\r\n    output_layouts: Union[placement, Tuple[placement]]\r\n    use_local: bool\r\n \r\n \r\nclass RowwiseParallel(ParallelStyle):\r\n    \"\"\"\r\n    Partitioning the row of a module. We assume the input to be a sharded DTensor and output to be a replicate Tensor.\r\n    \"\"\"\r\n    def __init__(self):\r\n        super().__init__(input_layouts=Shard(-1), output_layouts=Replicate(), use_local=True)\r\n\r\n\r\nClass ColwiseParallel(ParallelStyle):\r\n    \"\"\"\r\n    Partitioning the column of a module. We assume the input to be a Replicated DTensor and output to be a sharded DTensor.\r\n    \"\"\"\r\n    def __init__(self):\r\n        super().__init__(input_layouts=Replicate(), output_layouts=Shard(-1), use_local=True)\r\n\r\n# For the case of Sequence parallel, users just set different input_shard, Shard(0) or Shard(1) instead of Replicate()\r\n\r\nClass PrepareModuleInput(ParallelStyle):\r\n    \"\"\"\r\n    Only used to specify the input distribute spec for a module.\r\n    \"\"\"\r\n    def __init__(self):\r\n        super().__init__(input_layouts=Shard(0), output_layouts=Replicate(), use_local=False)\r\n\r\nClass PrepareModuleOutput(ParallelStyle):\r\n    \"\"\"\r\n    Only used to specify the output distribute spec for a module.\r\n    \"\"\"\r\n    def __init__(self):\r\n        super().__init__(input_layouts=Replicate(), output_layouts=Shard(0), use_local=True)\r\n\r\nparallelize_plan = {\r\n    \"embedding\": ColwiseParallel(output_shard=Replicate()),\r\n    \"attn\": PrepareModuleInput(),\r\n    \"attn.w1\": ColwiseParallel(),\r\n    \"attn.w2\": ColwiseParallel(),\r\n    \"attn.w3\": ColwiseParallel(),\r\n    \"attn.wo\": RowwiseParallel(),\r\n}\r\n\r\nparallelize_module(\r\n    module=block, # this can be a submodule or module\r\n    device_mesh=mesh['tp'],\r\n    parallelize_plan=parallelize_plan,\r\n)\r\n```\r\n\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 264,
        "deletions": 0,
        "changed_files": 4,
        "created_at": "2023-10-12T19:29:10Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111156\n\nSummary:\n\nThis PR adds in support for Wanda pruning into pytorch.\nMany thanks to @Eric-mingjie who provided\n[inital design and POC](https://github.com/Eric-mingjie/pytorch/tree/patch-1).\n\n[Wanda](https://arxiv.org/abs/2306.11695) is a pruning method that takes into\nacccount both the weight and activation statistics when pruning.\n\nIn order to keep track of activation norm statistics, this sparsifier\nutilizes the `torch.ao.quantization` API and a custom observer,\n`PerChannelNormObserver`.\n\nA user can then calibrate their model by doing\n```\nfor x in dataset:\n    model(x)\n```\n\nWe only use the observer to colllect input norm statistics, which we\naccess when calling `update_mask`\n\nFinally, we remove observers and mask parameterization by calling\n`squash_mask()`.\n\nNote that at this time we only support using the unpruned activations\nfor calibration.\n\nWandaSparsifier supports both unstructured and semi-structured (2:4)\npruning modes.\n\nTest Plan:\n```\npytest test/ao/sparsity/test_wanda_sparsifier.py\n```\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 14,
        "changed_files": 8,
        "created_at": "2023-10-12T19:28:58Z",
        "closed_at": "2023-10-13T16:52:58Z",
        "merged_at": null,
        "body": "This PR fixes typo in comments and messages in files under `c10` directory.\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 12,
        "changed_files": 9,
        "created_at": "2023-10-12T19:25:57Z",
        "closed_at": "2023-10-13T16:43:54Z",
        "merged_at": null,
        "body": "This PR fixes typo in comments and messages in files under `torchgen` directory.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-12T18:32:50Z",
        "closed_at": "2023-10-14T02:44:25Z",
        "merged_at": null,
        "body": "Summary:\nSometimes the backend compiler can encounter a transient failure (in\nour case, a remote build service infrequently hits a hiccup).  We'd rather run\neager than fail the training job.\n\nTest Plan:\nInject an exception in the RE path and run:\n```\nbuck2 run @//mode/{opt,inplace} //caffe2/test/inductor:smoke\n```\n\nDifferential Revision: D50234516\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 12,
        "changed_files": 4,
        "created_at": "2023-10-12T18:22:24Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111152\n* #111147\n* #111092\n\n\n\ncc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan\n\nDifferential Revision: [D50332159](https://our.internmc.facebook.com/intern/diff/D50332159)",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2023-10-12T18:04:08Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #108869\r\n\r\nImplements the first solution proposed in the issue. Pull Request resolved: https://github.com/pytorch/pytorch/pull/108915 Approved by: https://github.com/wanchaol, https://github.com/wz337\r\n\r\n",
        "comments": 0
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-12T17:55:31Z",
        "closed_at": null,
        "merged_at": null,
        "body": "If I add a param and then wrap with FSDP + load state dict, when strict=False don't hard error here.\r\n\r\nDifferential Revision: [D49170812](https://our.internmc.facebook.com/intern/diff/D49170812/)\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109116\r\nApproved by: https://github.com/fegin\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 21,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-12T17:32:55Z",
        "closed_at": "2023-10-16T21:47:17Z",
        "merged_at": null,
        "body": "Summary: Add linear quantize for vulkan to custom ops so it can be used from a model.\n\nTest Plan:\nbuck2 run --target-platforms ovr_config//platform/macos:arm64-fbsource -c pt.vulkan_full_precision=1\n//xplat/caffe2/fb/custom_ops/vulkan_quantized:pt_vulkan_quantized_test_binAppleMac\\#macosx-arm64\n[       OK ] VulkanAPITest.convert_qconv2d_context (135 ms)\n[ RUN      ] VulkanAPITest.linear_2d\n[       OK ] VulkanAPITest.linear_2d (4 ms)\n[----------] 2 tests from VulkanAPITest (139 ms total)\n[----------] Global test environment tear-down\n[==========] 2 tests from 1 test suite ran. (139 ms total)\n[  PASSED  ] 2 tests.\n##############################################################\nbuck2 build --target-platforms ovr_config//platform/macos:arm64-fbsource\n//xplat/caffe2:pt_vulkan_quantized_api_test_binAppleMac\\#macosx-arm64 -c pt.vulkan_full_precision=1 --show-output\"\nbuck-out//v2/gen/fbsource/xplat/caffe2/pt_vulkan_quantized_api_test_binAppleMac\n[       OK ] VulkanAPITest.conv2d_pw_quantized_prepack_random_params_int8_int32 (11 ms)\n[ RUN      ] VulkanAPITest.linear_2d_flat\n[       OK ] VulkanAPITest.linear_2d_flat (4 ms)\n[ RUN      ] VulkanAPITest.linear_2d_small\n[       OK ] VulkanAPITest.linear_2d_small (1 ms)\n[ RUN      ] VulkanAPITest.linear_2d_large\n[       OK ] VulkanAPITest.linear_2d_large (1 ms)\n[ RUN      ] VulkanAPITest.linear_3d_flat\n[       OK ] VulkanAPITest.linear_3d_flat (2 ms)\n[ RUN      ] VulkanAPITest.linear_3d_small\n[       OK ] VulkanAPITest.linear_3d_small (2 ms)\n[ RUN      ] VulkanAPITest.linear_3d_large\n[       OK ] VulkanAPITest.linear_3d_large (1 ms)\n[ RUN      ] VulkanAPITest.linear_4d_flat\n[       OK ] VulkanAPITest.linear_4d_flat (1 ms)\n[ RUN      ] VulkanAPITest.linear_4d_small\n[       OK ] VulkanAPITest.linear_4d_small (1 ms)\n[ RUN      ] VulkanAPITest.linear_4d_large\n[       OK ] VulkanAPITest.linear_4d_large (1 ms)\n[ RUN      ] VulkanAPITest.linear_custom\n[       OK ] VulkanAPITest.linear_custom (0 ms)\n[----------] 76 tests from VulkanAPITest (1811 ms total)\n[----------] Global test environment tear-down\n[==========] 76 tests from 1 test suite ran. (1811 ms total)\n[  PASSED  ] 76 tests.\nYOU HAVE 8 DISABLED TESTS\n##############################################################\nbuck2 run --target-platforms ovr_configplatform/macos:arm64-fbsourcexplat/caffe2:pt_vulkan_api_test_binAppleMac\\#macosx-arm64 -c pt.vulkan_full_precision=1\n[----------] Global test environment tear-down\n[==========] 346 tests from 1 test suite ran. (5648 ms total)\n[  PASSED  ] 345 tests.\n[  SKIPPED ] 1 test, listed below:\n[  SKIPPED ] VulkanAPITest.querypool_flushed_shader_log\nYOU HAVE 5 DISABLED TESTS\n\nReviewed By: manuelcandales\n\nDifferential Revision: D49609985\n\n\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 33,
        "deletions": 27,
        "changed_files": 4,
        "created_at": "2023-10-12T17:21:29Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111152\n* __->__ #111147\n* #111092\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng\n\nDifferential Revision: [](https://our.internmc.facebook.com/intern/diff/)",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 88,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-12T17:16:49Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary: We detect independent relu operators and do the fusion in the pre grad.\n\nTest Plan:\n### unit test\n```\nbuck2 test mode/dev-nosan //caffe2/test/inductor:group_batch_fusion\n```\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/16888498608558485\n\n### Inlinve cvr\nf479655232\n```\nbuck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch_group\n```\nbefore vs after transformation\nhttps://www.internalfb.com/intern/diffing/?paste_number=851907099\n\n```\nbuck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split_batch_group -c\n```\n\nP852036786\n\nDifferential Revision: D50207610\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 546,
        "deletions": 65,
        "changed_files": 10,
        "created_at": "2023-10-12T16:58:17Z",
        "closed_at": "2023-10-14T13:59:22Z",
        "merged_at": null,
        "body": "Follow up to: https://github.com/pytorch/pytorch/pull/111074\r\n\r\nCacheEntry are stored on f's `__code__` object. We simply change the way of counting the cache size limit\r\n\r\ncc @voznesenskym @ezyang @Chillee ",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 38,
        "deletions": 38,
        "changed_files": 4,
        "created_at": "2023-10-12T16:55:19Z",
        "closed_at": "2023-10-13T00:48:16Z",
        "merged_at": null,
        "body": "# Summary\r\nAll our filter functions should not mutate the passed in params, this both makes the intent more clear and allows for the compiler to possible produce more optimal code.\r\n\r\n### Note\r\nI used East-const style cause I think it is more clear:\r\nhttps://mariusbancila.ro/blog/2018/11/23/join-the-east-const-revolution/\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-12T16:49:08Z",
        "closed_at": "2023-10-13T18:56:34Z",
        "merged_at": null,
        "body": "<!--\ncopilot:summary\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at 34a15a6</samp>\n\nUpdated the cmake command in `docs/cpp/source/installing.rst` to use python3 and fix a documentation error.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-12T15:46:38Z",
        "closed_at": "2023-10-13T14:07:15Z",
        "merged_at": null,
        "body": "Summary:\nThis was error was run into when running ExportPassBase on an exported model with lifted constant tensors:\n```\n  File \"/data/users/angelayi/pytorch/torch/_subclasses/fake_tensor.py\", line 1444, in dispatch\n    len(kwargs) == 0 and len(args) == 1 and type(args[0]) is torch.Tensor\nAssertionError: (FakeTensor(..., size=(s0,)),) {}\n\nWhile executing %lift_fresh_copy_1 : [num_users=1] = call_function[target=torch.ops.aten.lift_fresh_copy.default](args = (%_lifted_tensor_constant99,), kwargs = {})\nOriginal traceback:\n  File \"\" in forward\n    mean = torch.tensor([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n```\n\nIn ExportPassBase, we retrace using the fake tensors in the placeholder nodes, but when running into this lift_fresh_copy operators, it's unable to be called with the fake tensors.\n\nTest Plan: CI\n\nReviewed By: chakriu\n\nDifferential Revision: D50211827\n\n\n\ncc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-12T15:41:48Z",
        "closed_at": null,
        "merged_at": null,
        "body": "test/dynamo/test_activation_checkpointing.py fails without this\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @ydwu4 ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 147,
        "deletions": 153,
        "changed_files": 12,
        "created_at": "2023-10-12T13:49:33Z",
        "closed_at": "2023-10-17T04:52:58Z",
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 456,
        "deletions": 192,
        "changed_files": 7,
        "created_at": "2023-10-12T13:37:07Z",
        "closed_at": "2023-10-13T02:04:36Z",
        "merged_at": null,
        "body": "Summary: reland D49876258\n\nTest Plan: CI\n\nDifferential Revision: D50224384\n\n\n\n\ncc @avikchaudhuri @gmagogsfm @tugsbayasgalan",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-12T11:19:14Z",
        "closed_at": "2023-10-12T17:14:14Z",
        "merged_at": null,
        "body": "Fixes #111066 #111065 #111064\r\n\r\nCurrently use_cutlass_template is returning True on ROCm but the feature is not supported. Fix to return false on ROCm. I considering adding this change to `try_import_cutlass` instead but the comments hinted that this function would be removed at some point.\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-12T10:49:08Z",
        "closed_at": "2023-10-13T22:06:00Z",
        "merged_at": null,
        "body": "in many cases, torch.diagonal will pass (dim1=-2, dim2=-1), onnx export will always fail in these cases\r\nthis pr try to fix the bug",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-12T10:01:21Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Changes:\r\n- Enables bfloat16 support in MFMA dot on MI200 (https://github.com/ROCmSoftwarePlatform/triton/commit/23979098c881536a52ba3edf8164e12296ecfe7b)\r\n- Add support for int8 to bfloat16 conversion (https://github.com/ROCmSoftwarePlatform/triton/commit/2d3e38e182e7b9b21aadb90bc2ab0d6514b3c760) fixing a bug in bf16 triton gemm workloads.\r\n- Enable scanOp lowering by adding shfl_up support https://github.com/ROCmSoftwarePlatform/triton/pull/324\r\n- MFMA16 support - support for the mfma_16x16xX instructions - these help perf on smaller sized GEMMs - https://github.com/ROCmSoftwarePlatform/triton/commit/7e34c244c284a84191a1a7bb0cd484c6345de650\r\n- configurable wavefront-per-eu - this helps us increase our occupancy in certain use cases such as Flash Attention - https://github.com/ROCmSoftwarePlatform/triton/commit/e801638b40dac4fd511f973d9899f033ae94dbec\r\n- Many bug fixes and optimisations\r\n\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-10-12T09:26:53Z",
        "closed_at": "2023-10-12T19:06:11Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111127\n\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 140,
        "deletions": 3,
        "changed_files": 5,
        "created_at": "2023-10-12T08:29:49Z",
        "closed_at": "2023-10-13T23:37:21Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111125\n\r\nSummary: When doing quantization int_mm -> mul or int_mm -> mul ->\r\nto(dtype) is an extremely common op pattern which is currently not\r\nhandled well by inductor. Ideally, since the output of\r\nint_mm has dtype int32 we'd prefer to only realize a smaller dtype like\r\nbf16 or float16. Currently inductor doesn't have a way to force this, in\r\nmany cases the mul gets fused with a bunch of subsequent pointwise\r\nops from the dequant creating an increase in memory overhead and a general\r\nslowdown compared to the fused version.\r\n\r\nTheoretically with better control of/smarter inductor fusion, this could be something we get for free, at which point these changes can be removed.\r\n\r\nTest Plan: python test/inductor/test_pattern_matcher.py -k\r\n\"int_mm_mul\"\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 288,
        "deletions": 28,
        "changed_files": 6,
        "created_at": "2023-10-12T08:28:54Z",
        "closed_at": "2023-10-12T19:29:14Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111124\n\nThis test may be of a reference for using AOTInductor with\nTorchScript.",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 409,
        "deletions": 26,
        "changed_files": 4,
        "created_at": "2023-10-12T06:24:28Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR optimizes cases like layer_norm + fp8 quant (which includes amax and fp8 quant) fusion when amax is split into multiple reduction kernels.\r\n\r\nBenchmark:\r\n```\r\npython test/inductor/test_fp8.py -k test_layernorm_fp8_quant_benchmark\r\n\r\nBefore this PR:\r\nConfig: float8_dtype=torch.float8_e5m2, shape=(4, 2048, 4096). \r\nBenchmark results: Inductor: 0.13262102689486555ms, Eager: 0.8211962616822429ms, LN only Inductor: 0.09606276150627614ms.\r\n\r\nAfter this PR:\r\nConfig: float8_dtype=torch.float8_e5m2, shape=(4, 2048, 4096). \r\nBenchmark results: Inductor: 0.08281274131274131ms, Eager: 0.8217452830188678ms, LN only Inductor: 0.09586902286902287ms.\r\n```\r\n\r\nLN + fp8 quant is even faster than LN itself. The reason could be that LN + fp8 outputs fp8 while LN outputs fp16.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111122\n\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 37,
        "deletions": 62,
        "changed_files": 3,
        "created_at": "2023-10-12T06:13:37Z",
        "closed_at": "2023-10-17T00:15:38Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111111\n* __->__ #111120\n* #111110\n* #111109\n* #111275\n* #111107\n* #111106\n\n`state_dict` is a very common variable name people use to represent a local\nstate_dict and `load_state_dict` conflicts with DCP's `load_state_dict`.\n\nThis PR changes `state_dict` to `get_state_dict`. `get_state_dict` is more close to what is this API does -- users use the API to get the current state_dict for saving or for loading (passed to DCP for loading in-place)..\n\nThis PR also changes `load_state_dict` to `set_state_dict`. `set_state_dict` is less ideal compared to `get_state_dict` but is symetric. We can still change the API name before it goes to beta.\n\nThis PR also simplies the API signatures. `model_only` is removed and `optim_only` only exists for `get_state_dict`.\n\nDifferential Revision: [D50213931](https://our.internmc.facebook.com/intern/diff/D50213931/)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-12T05:24:35Z",
        "closed_at": "2023-10-16T00:27:08Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111118\n\r\n**Summary**\r\nFix the issue: https://github.com/pytorch/pytorch/issues/109019 of negative value used in tensor index.\r\nThis implementation refers to https://github.com/pytorch/pytorch/pull/105055\r\n\r\n**Test Plan**\r\n```\r\npython -m pytest test_torchinductor.py -k test_negative_index\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 76,
        "deletions": 35,
        "changed_files": 3,
        "created_at": "2023-10-12T05:15:12Z",
        "closed_at": "2023-10-17T03:06:59Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111402\n* __->__ #111117\n\nThis splits out changes from\nhttps://github.com/pytorch/pytorch/pull/102625 to make things easier to\nreview.\n\nThis diff creates a `make_allocation()` method that extracts the logic\nfrom `make_buffer_allocation()` while allowing us to allocate non-buffer\nobjects. In particular, we will use this to allocate memory pools during\nmemory planning.\n\nThis diff also includes a small optimization -- if the desired\nallocation is contiguous, then we emit a call to `empty()` instead of\n`empty_strided()` with its superfluous stride argument.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 14
    },
    {
        "merged": false,
        "additions": 71,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-10-12T04:47:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Adds support for hardshrink forward and backwared to MPS backend.\r\n\r\nThis is a resubmission of a pull request that was cancelled because I badly botched a rebase (#110816).\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 42,
        "deletions": 32,
        "changed_files": 4,
        "created_at": "2023-10-12T04:41:03Z",
        "closed_at": "2023-10-12T05:42:23Z",
        "merged_at": null,
        "body": "https://github.com/pytorch/pytorch/pull/109859 caused Meta-internal build breakages because it removes an API\r\nhttps://github.com/pytorch/pytorch/pull/110757 depends on 109859 and is already reverted via pytorchbot revert\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 32,
        "deletions": 33,
        "changed_files": 17,
        "created_at": "2023-10-12T03:18:42Z",
        "closed_at": "2023-10-13T16:33:14Z",
        "merged_at": null,
        "body": "Enable [snake-case-type-alias (PYI042)](https://docs.astral.sh/ruff/rules/snake-case-type-alias/)\r\n\r\nLink: #110950",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-12T03:12:34Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Enable [redundant-numeric-union (PYI041)](https://docs.astral.sh/ruff/rules/redundant-numeric-union/)\r\n\r\nLink: #110950\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 185,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-12T03:02:35Z",
        "closed_at": "2023-10-17T03:09:19Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111111\n* #111120\n* #111110\n* #111109\n* #111275\n* #111107\n* #111106\n\nAs title\n\nDifferential Revision: [D50209732](https://our.internmc.facebook.com/intern/diff/D50209732/)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 147,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-10-12T03:02:27Z",
        "closed_at": "2023-10-16T23:25:45Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111111\n* #111120\n* __->__ #111110\n* #111109\n* #111275\n* #111107\n* #111106\n\nIt is not easy for user to do submodules save and load (e.g., fine tuning) because FSDP requires to get the root module. This PR enables the support of submodule save and load.\n\nDifferential Revision: [D50209727](https://our.internmc.facebook.com/intern/diff/D50209727/)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-12T03:02:19Z",
        "closed_at": "2023-10-15T04:58:30Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111111\n* #111120\n* #111110\n* __->__ #111109\n* #111275\n* #111107\n* #111106\n\nAs title\n\nDifferential Revision: [D50209723](https://our.internmc.facebook.com/intern/diff/D50209723/)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 25,
        "changed_files": 2,
        "created_at": "2023-10-12T03:02:12Z",
        "closed_at": "2023-10-13T21:03:59Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #111111\r\n* #111120\r\n* #111110\r\n* #111109\r\n* __->__ #111108\r\n* #111107\r\n* #111106\r\n\r\n1. Rename DistributedStateDictOptions to StateDictOptions.\r\n2. Remove cpu_offload as we have not yet required this option.\r\n3. Rename save_frozen_parameters to ignore_frozen_params.\r\n",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 40,
        "deletions": 21,
        "changed_files": 2,
        "created_at": "2023-10-12T03:02:04Z",
        "closed_at": "2023-10-13T18:41:04Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111111\n* #111120\n* #111110\n* #111109\n* #111108\n* __->__ #111107\n* #111106\n\nIt's quite annoying that users have to create a tuple of optimizers even if there is only one optimizer. This PR makes most users' life easier.\n\nDifferential Revision: [D50209704](https://our.internmc.facebook.com/intern/diff/D50209704/)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-12T03:01:56Z",
        "closed_at": "2023-10-13T18:03:32Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111111\n* #111120\n* #111110\n* #111109\n* #111108\n* #111107\n* __->__ #111106\n\nCalling os.sync() to ensure the tempfile can be seens across ranks.\n\nDifferential Revision: [D50209697](https://our.internmc.facebook.com/intern/diff/D50209697/)",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2023-10-12T02:56:01Z",
        "closed_at": "2023-10-13T21:20:01Z",
        "merged_at": null,
        "body": "Enable [non-self-return-type (PYI034)](https://docs.astral.sh/ruff/rules/non-self-return-type/#non-self-return-type-pyi034)\r\n\r\nLink: #110950\r\n\r\n\r\n\r\n**EDIT**: to newly added reviewers, please ignore the request, it's due to a rebase error :sweat_smile: \n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 91,
        "deletions": 13,
        "changed_files": 4,
        "created_at": "2023-10-12T02:48:13Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111104\n\n\n\ncc @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2023-10-12T02:43:43Z",
        "closed_at": "2023-10-12T13:32:06Z",
        "merged_at": null,
        "body": "Enable [unnecessary-literal-union (PYI030)](https://docs.astral.sh/ruff/rules/unnecessary-literal-union/)\r\n\r\nLink: #110950",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 110,
        "deletions": 42,
        "changed_files": 5,
        "created_at": "2023-10-12T01:19:49Z",
        "closed_at": "2023-10-13T08:46:25Z",
        "merged_at": null,
        "body": "It's a reimplementation of #111089 \r\n\r\n1. When using fake inputs make sure they are on the same device as the original inputs.\r\n2. Don't change the value of self.cpp_wrapper from True to False if can't generate a C++ wrapper, instead have a check and fail early to avoid producing Python code for C++ compiler.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-12T01:16:39Z",
        "closed_at": "2023-10-12T16:26:28Z",
        "merged_at": null,
        "body": "Enable [unused-private-type-var (PYI018)](https://docs.astral.sh/ruff/rules/unused-private-type-var/#unused-private-type-var-pyi018)\r\n\r\nLink: #110950\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-12T00:44:24Z",
        "closed_at": "2023-10-12T00:48:51Z",
        "merged_at": null,
        "body": "Temporarily mark these models as fail. Failures are due to https://github.com/pytorch/pytorch/pull/111030 which is needed for ExecuTorch's release so it can't be reverted. Will forward fix the failures.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 94,
        "deletions": 230,
        "changed_files": 1,
        "created_at": "2023-10-12T00:31:38Z",
        "closed_at": "2023-10-12T02:30:37Z",
        "merged_at": null,
        "body": "https://github.com/pytorch/pytorch/pull/107846 caused Meta-internal S369412\r\nhttps://github.com/pytorch/pytorch/pull/109695 depends on 107846 so also needs to be reverted",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-12T00:17:41Z",
        "closed_at": "2023-10-12T04:31:01Z",
        "merged_at": null,
        "body": "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/_update-commit-hash.yml).\nUpdate the pinned vision hash.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 371,
        "deletions": 333,
        "changed_files": 63,
        "created_at": "2023-10-12T00:15:05Z",
        "closed_at": "2023-10-14T00:37:09Z",
        "merged_at": null,
        "body": "Fixes #110982\r\n\r\nhttps://github.com/pytorch/pytorch/pull/62257 deprecated `torch.onnx.export(use_external_data_format: bool=...)`  argument, but it seems the introduced `EncoderBase::GetGraphProtoSize` has a bug and doesn't detect models > 2GB when onnx Constant nodes are large (and responsible for the size overflow)\r\n\r\nThis PR adds the constant node to the total size of the model, along with initializers.\r\n\r\nIn python, what we need to do is:\r\n\r\n```python\r\nimport onnx\r\n\r\ndef compute_tensor_size(tensor):\r\n    # Compute the size of the tensor based on its shape and data type\r\n    size = tensor.size * tensor.itemsize\r\n    return size\r\n\r\ndef sum_constant_and_initializer_sizes(model_path):\r\n    # Load the ONNX model\r\n    model = onnx.load(model_path)\r\n\r\n    total_size = 0\r\n    initializer_size = 0\r\n    constant_size = 0\r\n\r\n    # Compute the size of constant nodes\r\n    for node in model.graph.node:\r\n        if node.op_type == 'Constant':\r\n            constant_value = node.attribute[0].t\r\n            # Convert constant value to numpy array\r\n            constant_array = onnx.numpy_helper.to_array(constant_value)\r\n            # Compute the size of the constant tensor\r\n            tensor_size = compute_tensor_size(constant_array)\r\n            total_size += tensor_size\r\n            constant_size += tensor_size\r\n\r\n    # Compute the size of initializer nodes that are not graph inputs\r\n    for initializer in model.graph.initializer:\r\n        if initializer.name not in [input.name for input in model.graph.input]:\r\n            # Convert the shape and data type information to calculate size\r\n            # tensor = onnx.helper.tensor_value_info_to_tensor(input)\r\n            tensor = onnx.numpy_helper.to_array(initializer)\r\n            tensor_size = compute_tensor_size(tensor)\r\n            total_size += tensor_size\r\n            initializer_size += tensor_size\r\n\r\n    return total_size, constant_size, initializer_size\r\n\r\nmodel_path = '/path/to/model.onnx'\r\ntotal_size, constant_size, initializer_size = sum_constant_and_initializer_sizes(model_path)\r\n\r\nprint(\"Total size of constant nodes in bytes:\", constant_size)\r\nprint(\"Total size of initializer nodes (excluding graph inputs) in bytes:\", initializer_size)\r\nprint(\"Total size of constant and initializer nodes (excluding graph inputs) in bytes:\", total_size)\r\n```\n\ncc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-11T23:45:37Z",
        "closed_at": "2023-10-12T20:27:13Z",
        "merged_at": null,
        "body": "`_shard_tensor()` calls into `dist.all_gather_object()` and this is causing optimizer state dict loading to be super slow. Workaround: call `FSDP._shard_utils._create_chunk_sharded_tensor()` to construct ShardedTensor without any communication. \r\n\r\nThanks to @fegin for suggesting the fix!\r\nThanks @mvpatel2000 for reporting the issue and providing profiling details to help us isolate the problematic source code quickly. ",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 188,
        "deletions": 113,
        "changed_files": 1,
        "created_at": "2023-10-11T23:43:29Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #110313\n* #110477\n* #110178\n* #108376\n* __->__ #111095\n* #110183\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 68,
        "deletions": 34,
        "changed_files": 2,
        "created_at": "2023-10-11T23:37:09Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111094\n* #111016\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 188,
        "changed_files": 12,
        "created_at": "2023-10-11T23:34:53Z",
        "closed_at": "2023-10-18T01:00:55Z",
        "merged_at": null,
        "body": "Removes the existing integration code & build of nvfuser in TorchScript.\r\n\r\nNote that I intentionally left the part where we wipe out `third_party/nvfuser` repo. I'll do that in a separate PR.\r\n\n\ncc @mcarilli @ptrblck @leslie-fang-intel @jgong5",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-10-11T23:10:44Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111152\n* #111147\n* __->__ #111092\n\r\nAs titled.\r\n\r\nTest Plan:\r\nexisting tests.\r\n\r\n\r\ncc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan\n\nDifferential Revision: [D50232731](https://our.internmc.facebook.com/intern/diff/D50232731)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-11T23:09:24Z",
        "closed_at": "2023-10-12T03:37:25Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111091\n* #110900\n* #110898\n\nas titled",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 88,
        "deletions": 37,
        "changed_files": 4,
        "created_at": "2023-10-11T22:44:31Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary:\nThe changes:\n1. When using fake inputs make sure they are on the same device as the original inputs.\n1. Don't change the value of `self.cpp_wrapper` from `True` to `False` if can't generate a C++ wrapper, instead have a check and fail early to avoid producing Python code for C++ compiler.\n\nDifferential Revision: D50154720\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 0,
        "changed_files": 0,
        "created_at": "2023-10-11T22:32:24Z",
        "closed_at": "2023-10-12T15:19:34Z",
        "merged_at": null,
        "body": "Summary: All employees are now onboarded onto the GK redirect_to_workflow which makes /skycastle/run into /sandcastle/workflow -- let's update various places in the code that point at the old URL and staticdocs references.\n\nTest Plan: sandcastle\n\nReviewed By: Sushisugre\n\nDifferential Revision: D50094631\n\n\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-11T22:24:55Z",
        "closed_at": "2023-10-12T02:14:44Z",
        "merged_at": null,
        "body": "tensor like should check the instance for a torch function impl, not the type",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 87,
        "deletions": 11,
        "changed_files": 9,
        "created_at": "2023-10-11T20:42:48Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This is the culminated result of https://github.com/pytorch/pytorch/pull/110954#issuecomment-1758520411.\r\n\r\nWe are making the code slightly more complicated to gain some perf in minimizing calls to `.copy_()` and `.to()`.\r\n\r\n### Code\r\n```\r\nimport torch\r\nwith torch.cuda.device(0):\r\n    steps = [torch.zeros((), device=\"cpu\", dtype=torch.float32) for i in range(1000)]\r\n\r\n    with torch.profiler.profile(\r\n        activities=[\r\n            torch.profiler.ProfilerActivity.CPU,\r\n            torch.profiler.ProfilerActivity.CUDA,\r\n        ]\r\n    ) as p:\r\n        # New code:\r\n        # step_device = steps[0].device\r\n        # one = torch.tensor(1.0, device=step_device) if str(step_device) == \"cpu\" else 1\r\n        # torch._foreach_add_(steps, one, 1.0)\r\n\r\n        # Old code:\r\n        torch._foreach_add_(steps, 1)\r\n\r\n    print(p.key_averages().table(sort_by=\"cpu_time_total\"))\r\n```\r\n\r\n### Profiles\r\n**with old code**\r\n```\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n      aten::_foreach_add_        35.31%      52.089ms        99.99%     147.495ms     147.495ms             1  \r\n               aten::add_        25.05%      36.949ms        64.68%      95.406ms      95.406us          1000  \r\n                 aten::to         3.97%       5.852ms        39.63%      58.457ms      58.457us          1000  \r\n           aten::_to_copy        10.11%      14.917ms        35.66%      52.605ms      52.605us          1000  \r\n              aten::copy_        21.65%      31.939ms        21.65%      31.939ms      31.939us          1000  \r\n      aten::empty_strided         3.90%       5.749ms         3.90%       5.749ms       5.749us          1000  \r\n    cudaDeviceSynchronize         0.01%      18.000us         0.01%      18.000us      18.000us             1  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 147.513ms\r\n```\r\n\r\n**with new code**\r\n```\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n      aten::_foreach_add_        55.06%      49.963ms        99.86%      90.625ms      90.625ms             1  \r\n               aten::add_        44.81%      40.662ms        44.81%      40.662ms      40.662us          1000  \r\n            aten::detach_         0.01%       8.000us         0.05%      45.000us      45.000us             1  \r\n                  detach_         0.04%      37.000us         0.04%      37.000us      37.000us             1  \r\n              aten::empty         0.03%      30.000us         0.03%      30.000us      30.000us             1  \r\n                 aten::to         0.03%      23.000us         0.03%      23.000us      23.000us             1  \r\n    cudaDeviceSynchronize         0.02%      22.000us         0.02%      22.000us      22.000us             1  \r\n         aten::lift_fresh         0.01%       6.000us         0.01%       6.000us       6.000us             1  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 90.751ms\r\n```\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111084\n* #111079\n\r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-11T20:35:57Z",
        "closed_at": "2023-10-11T23:33:16Z",
        "merged_at": "2023-10-11T23:33:16Z",
        "body": "This file needs to be added to the list like others.  The publish command `BUILD_LITE_INTERPRETER=1 android/gradlew -p android publish` finishes successfully with this and files are available on Nexus:\r\n\r\n![Screenshot 2023-10-11 at 11 56 53](https://github.com/pytorch/pytorch/assets/475357/849d4aa7-79f6-47fa-a471-d452d7c1bdf6)\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 20,
        "changed_files": 3,
        "created_at": "2023-10-11T20:22:40Z",
        "closed_at": "2023-10-18T04:35:42Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111080\n\nSummary: Return the compiled library path as a string instead of wrap it as a callable.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler\n\nDifferential Revision: [D50246941](https://our.internmc.facebook.com/intern/diff/D50246941)",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 145,
        "deletions": 59,
        "changed_files": 8,
        "created_at": "2023-10-11T20:20:09Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Adding a Tensor overload will allow us to:\r\n- optimize in more cases than before\r\n- increase coverage for scalarTensor instead of just scalars in our foreach APIs\r\n\r\nThe main complication in this PR was that add.Tensor has a scalar overload, so I've now built out support for that.\r\n\r\ncc @crcrpar \r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111084\n* __->__ #111079\n\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 150,
        "deletions": 24,
        "changed_files": 8,
        "created_at": "2023-10-11T20:20:03Z",
        "closed_at": "2023-10-13T03:27:48Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111078\r\n* #109123\r\n\r\nThis PR:\r\n\r\n* Introduces a new layout: torch.jagged\r\n* Expands torch.nested.nested_tensor(tensor_list) to accept a layout kwarg with the default set to torch.strided\r\n\r\nTODO: non-copying constructor",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-10-11T19:36:47Z",
        "closed_at": "2023-10-12T08:54:39Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111076\n\nFormats the string using the existing getNCCLversion\n\nDifferential Revision: [D50193558](https://our.internmc.facebook.com/intern/diff/D50193558/)",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 462,
        "deletions": 45,
        "changed_files": 9,
        "created_at": "2023-10-11T19:07:43Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes: https://github.com/pytorch/pytorch/issues/110682\r\n\r\nThe guards are installed based on config that is valid at the call to `torch.compile`, rather than at any subsequent call / triggered compilation. Subsequent compilations will restore the config if there is a config mismatch with the saved config.\r\n\r\nTODO:\r\n- [X] add tests\r\n\r\nFollow up PRs:\r\n- [x] add revised cache size computation (follow up PR: https://github.com/pytorch/pytorch/pull/111145, based on: https://github.com/pytorch/pytorch/pull/107496) \r\n- [ ] handle run-only mode?\r\n- [ ] config restoration itself is not thread-safe (tracked: https://github.com/pytorch/pytorch/issues/111150)\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @ezyang @Chillee",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 1421,
        "deletions": 158,
        "changed_files": 17,
        "created_at": "2023-10-11T19:06:12Z",
        "closed_at": "2023-10-11T19:07:11Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111073\n\nSummary:\n\nThis PR adds epilogue fusion code generation support for the new experimental\n[Inductor Cutlass backend]([https://github.com/pytorch/pytorch/pull/108015]).\n\nDetails:\n\nA fusion happens on the GEMM template level by taking a Cutlass 3.x GEMM Universal Matmul Kernel template\nand adding a custom template functor based on Cutlass new \u201cEpilogue Visitor Trees\u201d (EVT) on top, which represents and\nperforms the computation of the fused Pointwise / Elementwise computation nodes.\n\nThis is the approach dictated by [NVIDIA/cutlass example 49](https://github.com/NVIDIA/cutlass/blob/main/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu),\nwhich is currently the only documentation and example of Cutlass Epilogue Visitor Trees.\n\nThis EVT functor in turn is a hierarchical template expression which represents an abstract syntax tree of the fused computation to perform.\nA second codegen task is to create a hierarchical initializer expression, which provides potentially necessary arguments\nto each of the functor subexpressions.\n\nStep 1 functionality:\n\n * End to end code generation is possible using the above approach.\n * Supports simple elementwise expression fusion of chains of elementwise operations (with scalar constants )\n   after a matmul.\n * Elementwise operation support includes addition, subtraction, multiplication, division, minimum, maximum etc.\n * Examples / Unit tests include ReLU and ReLU6 fusion.\n * Support for fp16 and fp16 with fp32 accumulation data types.\n * Generates SM90 ( Hopper ) based CUDA Kernels ( as Cutlass up to 3.2.0 only supported EVT for SM90 )\n\nThe following is not yet supported, and is left for future work:\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 702,
        "deletions": 29,
        "changed_files": 17,
        "created_at": "2023-10-11T19:03:06Z",
        "closed_at": "2023-10-12T16:59:30Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #109739\n* #110307\n* __->__ #111072\n* #111061\n\r\nThis reverts commit 314a502eb04c6382e2cc9af0573533efba54109d.\r\n\r\nChanges since original PR:\r\nReland 1\r\n *  rename torch.distributed.hooks to torch.distributed._hooks\r\n\r\nReland 2\r\n * make _hooks importable even if !distributed.is_available()\r\n * handle cuda driver exit intermittent failure caused by new cuda api usage in callback caller (see prev PR in stack)\r\n\r\n(original PR https://github.com/pytorch/pytorch/pull/108815 desc copied below)\r\n\r\nExpose a set of observability hooks into C10D such that our users can\r\ndetect collectives failure both faster and more easily.\r\n\r\nThe design is similar to NCCL desync debug that it minimized the\r\noverhead by doing most of the work out of the main thread.\r\n\r\nThis PR introduces a new module torch.distributed.hooks that exposes the following set of methods:\r\n\r\n    register_collective_start_hook\r\n    register_collective_end_hook\r\n    register_process_group_hook\r\n\r\nThe process group hook exposes PG creation on the member ranks and call them inline from the\r\nthe PG creation code. This is fine since this happens during initialization and a limited number of times.\r\n\r\nThe collective start/end hooks are fired from a single background thread. It reads\r\nevents from a C++ queue and dispatches over.\r\n\r\nQueue notification is oddly done using a pipe, this is needed so python can abort the thread on shutdown\r\nand have it as background thread. This is not possible with more reasonable choices like a condvar.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-11T19:01:40Z",
        "closed_at": "2023-10-11T20:28:19Z",
        "merged_at": null,
        "body": "This file needs to be added to the list like others.  The publish command `BUILD_LITE_INTERPRETER=1 android/gradlew -p android publish` finishes successfully with this and files are available on Nexus:\r\n\r\n![Screenshot 2023-10-11 at 11 56 53](https://github.com/pytorch/pytorch/assets/475357/849d4aa7-79f6-47fa-a471-d452d7c1bdf6)\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 688,
        "deletions": 29,
        "changed_files": 17,
        "created_at": "2023-10-11T18:57:23Z",
        "closed_at": "2023-10-11T19:03:32Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111069\n* #111061\n\nThis reverts commit ff0358b0384d6a3a5b8ceeae625c93221612ba8e.\n\n(original PR https://github.com/pytorch/pytorch/pull/108815 desc copied below)\n\nExpose a set of observability hooks into C10D such that our users can\ndetect collectives failure both faster and more easily.\n\nThe design is similar to NCCL desync debug that it minimized the\noverhead by doing most of the work out of the main thread.\n\nThis PR introduces a new module torch.distributed.hooks that exposes the following set of methods:\n\n    register_collective_start_hook\n    register_collective_end_hook\n    register_process_group_hook\n\nThe process group hook exposes PG creation on the member ranks and call them inline from the\nthe PG creation code. This is fine since this happens during initialization and a limited number of times.\n\nThe collective start/end hooks are fired from a single background thread. It reads\nevents from a C++ queue and dispatches over.\n\nQueue notification is oddly done using a pipe, this is needed so python can abort the thread on shutdown\nand have it as background thread. This is not possible with more reasonable choices like a condvar.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 25,
        "changed_files": 8,
        "created_at": "2023-10-11T18:37:37Z",
        "closed_at": "2023-10-11T23:09:42Z",
        "merged_at": null,
        "body": "This PR fixes typo the the of comments and exception messages in files under `torch/_functorch` directory.\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 56,
        "deletions": 52,
        "changed_files": 1,
        "created_at": "2023-10-11T18:23:43Z",
        "closed_at": "2023-10-12T13:47:12Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #109739\n* #110307\n* #111072\n* __->__ #111061\n\nThe pattern here is that main may exit and kill cuda driver before\nc10d watchdog related threads have cleanly exited.  If this happens,\nc10d threads may still make CUDA api calls and raise an exception about\nthe cuda driver being dead.\n\nIn the past we've patched a few helper functions that call into cuda\nto specifically handle this driver exiting message.  Instead, we know\nthat this problem applies only to codepaths in our background threads,\nso we should catch at that scope and not worry about fine-grained\ncatching at the helper granularity. (and if a helper is used from the main\nthread, we should NOT catch this exception- it's the application's fault)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 70,
        "deletions": 4,
        "changed_files": 4,
        "created_at": "2023-10-11T18:17:24Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111055\n\r\nToday, we say an attribute crosses ctx boundaries when it is written to the ctx in Autograd.Function forward and accessed in backwards.\r\n\r\nThis mostly works. However, if that attribute has a proxy, (tensor, symint) - it will fail during attribute lifting as the proxy.tracer is from the forward speculation. We have prior art of bypassing lifting, as the proxy is already known the tracer, via the save_for_backwards calls in autograd.Function. \r\n\r\nThis PR extends a similar hit through the proxy to let attribute lifting know that this proxy is already known to us and is safely tracked in side_effects. \r\n\r\ncc @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 73,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-11T17:56:47Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111049\n* #110583\n\nSummary:\n\nThis PR adds in torch.compile support for semi-structured sparsity,\nusing the subclass tracing @bdhirsh added.\n\nBased on wether we are using cuSPARSELt or CUTLASS, we return a\ndifferent representation of the inner tensors.\n\nTest Plan:\n```\npython test/test_sparse_semi_structured.py -k compile\n```\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:",
        "comments": 19
    },
    {
        "merged": false,
        "additions": 44,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-11T17:54:09Z",
        "closed_at": null,
        "merged_at": null,
        "body": "The shim still requires the ActivityTYpe.h header to get the enum Activity type.\r\n\r\nSo cut-n-paste just enough of the header in to do this.\r\n\r\n\r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 307,
        "deletions": 50,
        "changed_files": 3,
        "created_at": "2023-10-11T17:32:23Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111046\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-11T17:20:56Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Cherry picked https://github.com/ROCmSoftwarePlatform/pytorch/commit/2d9f3ec757d0c71d613c59e88c082c74a86f828a\r\n\n\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 200,
        "deletions": 23,
        "changed_files": 7,
        "created_at": "2023-10-11T17:02:50Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR is by cherry picking the below two commits. \r\n\r\nCentos stream9 PyTorch image support - https://github.com/ROCmSoftwarePlatform/pytorch/commit/5ab8d09d51acd74ccd483e8d3b128f178dd4bca4\r\nUpdated to latest conda for CentOS stream 9 - https://github.com/ROCmSoftwarePlatform/pytorch/commit/2cb18f8d102b12b90a1635e78f8c3eaa7ccc8fcd\r\n\r\ncc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 54,
        "deletions": 14,
        "changed_files": 10,
        "created_at": "2023-10-11T16:34:31Z",
        "closed_at": "2023-10-12T17:40:20Z",
        "merged_at": null,
        "body": "Will fix package after publishing https://github.com/pytorch/pytorch/issues/100974\r\nPoetry install requires all wheels on pypi to have same metadata. Hence including linux dependencies in all non-linux wheels\r\n",
        "comments": 4
    },
    {
        "merged": true,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-11T16:33:14Z",
        "closed_at": "2023-10-11T16:51:07Z",
        "merged_at": "2023-10-11T16:51:07Z",
        "body": "This pins test-infra checkout branch to `release/2.1` and fixes the missing script issue after https://github.com/pytorch/test-infra/pull/4626",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-11T15:53:01Z",
        "closed_at": "2023-10-16T15:18:27Z",
        "merged_at": null,
        "body": "This is kind of hard to test, but I can try to add a test case if requested.\r\n\r\nI noticed locally that we now end up logging to the ProxyTensorMode and FakeTensorMode `not_implemented` logs in very simple compile examples: https://github.com/pytorch/pytorch/blob/main/torch/fx/experimental/proxy_tensor.py#L269\r\n\r\nIt was because `_mirror_autograd_meta_to()` indirectly queries sizes, and since modes have higher priority than subclasses, `aten::sym_sizes()` was getting dispatched to our modes before going to `FunctionalTensor.__torch_dispatch__`.\r\n\r\nThis works out fine (they return NotImplemented and we eventually get to `FunctionalTensor`) but I figured we want to avoid cluttering up the logs. So I wrapped the calls with `FunctionalTensorMode`.\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111364\n* __->__ #111040\n\r\n",
        "comments": 1
    },
    {
        "merged": true,
        "additions": 10,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-10-11T15:47:55Z",
        "closed_at": "2023-10-12T17:39:42Z",
        "merged_at": "2023-10-12T17:39:42Z",
        "body": "Try to use linux.arm64.2xlarge runners.\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107672 Approved by: https://github.com/atalman\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-11T15:25:38Z",
        "closed_at": null,
        "merged_at": null,
        "body": null,
        "comments": 2
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-11T15:24:58Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes typing so that linter does not complain when using CustomPolicy. Pull Request resolved: https://github.com/pytorch/pytorch/pull/110545 Approved by: https://github.com/awgu, https://github.com/Skylion007\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-10-11T15:17:34Z",
        "closed_at": "2023-10-12T03:28:34Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111035\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 14,
        "changed_files": 2,
        "created_at": "2023-10-11T13:39:51Z",
        "closed_at": "2023-10-11T20:48:31Z",
        "merged_at": null,
        "body": "Summary:\nhttps://docs.google.com/document/d/1QJJEGnj2nHGPODlw38BEG3KLLCOTfdOVjPrNQbz_LM8/edit#bookmark=id.lp80wfshq130\nChanges:\n* `torch.export` will return a functional ATen graph but not lowered to core aten decompositions (CompositeImplicitAutograd decomps still run)\n* `exported_program.run_decompositions(decomposition_table)` will optionally take a decomposition table, and run decompositions on the exported program, returning a new exported program. By default we will run the Core ATen decomposition table.\n\nCalling convention for Executorch stays the same:\n```\npre_autograd_graph = capture_pre_autograd_graph(f, args, ...)\naten_graph_no_decomps = torch.export.export(pre_autograd_graph, args, ...)\n# Within to_edge we decompose to core aten and then convert to edge\nedge_graph = exir.to_edge(aten_graph_no_decomps)\n```\n\nTest Plan: CI\n\nDifferential Revision: D50172210\n\n\n\n\ncc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 186,
        "deletions": 32,
        "changed_files": 10,
        "created_at": "2023-10-11T09:12:55Z",
        "closed_at": "2023-10-12T23:05:30Z",
        "merged_at": null,
        "body": "Adding modules imported here and the following functions to the `__all__`:\r\n* [clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\r\n* [clip_grad_value_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html)\r\n* [remove_weight_norm](https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_weight_norm.html)\r\n* [parameters_to_vector](https://pytorch.org/docs/stable/generated/torch.nn.utils.parameters_to_vector.html)\r\n* [vector_to_parameters](https://pytorch.org/docs/stable/generated/torch.nn.utils.vector_to_parameters.html)\r\n* [remove_spectral_norm](https://pytorch.org/docs/stable/generated/torch.nn.utils.remove_spectral_norm.html)\r\n* [skip_init](https://pytorch.org/docs/stable/generated/torch.nn.utils.skip_init.html)",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 148,
        "deletions": 6,
        "changed_files": 1,
        "created_at": "2023-10-11T06:12:05Z",
        "closed_at": "2023-10-16T21:22:49Z",
        "merged_at": null,
        "body": "Using vectorized loads/stores makes the `layer_norm_grad_input_kernel` generally faster. This PR accelerates medium and larger problem sizes.\r\n\r\n```python\r\ndef run_model_on_device(fs, X, gO, device_string, numeric_type):\r\n    ln = torch.nn.LayerNorm((fs,), device=device_string, dtype=numeric_type)\r\n    ln.reset_parameters()\r\n    X.grad = None\r\n    ln.zero_grad(set_to_none=True)\r\n    out = ln(X)\r\n    out.backward(gO)\r\n    return (ln.weight.grad, ln.bias.grad)\r\n\r\n\r\ndef run_correctness_test(eps_weight, eps_bias):\r\n    dtype = torch.float\r\n    for val in l_inputs:\r\n        bs = val[0][0]\r\n        fs = val[0][1]\r\n\r\n        mean_adjustment = torch.randn(fs, device=\"cpu\", dtype=torch.float)\r\n        X = mean_adjustment * torch.randn(\r\n            bs, fs, device=\"cpu\", dtype=torch.float, requires_grad=True\r\n        )\r\n\r\n        X = X.detach().requires_grad_()\r\n        gO = torch.rand_like(X)\r\n        X_gpu = X.to(\"cuda\")\r\n        X_gpu = X_gpu.detach().requires_grad_()\r\n        gO_gpu = gO.to(\"cuda\")\r\n        gO_gpu = gO_gpu.detach().requires_grad_()\r\n\r\n        grad_cpu_ref = run_model_on_device(fs, X, gO, \"cpu\", dtype)\r\n        grad_gpu = run_model_on_device(fs, X_gpu, gO_gpu, \"cuda\", dtype)\r\n        weight_grad_gpu_target = grad_gpu[0].detach().to(\"cpu\")\r\n        bias_grad_gpu_target = grad_gpu[1].detach().to(\"cpu\")\r\n\r\n        weight_delta = torch.abs(grad_cpu_ref[0] - weight_grad_gpu_target)\r\n        weight_mismatches = (weight_delta >= eps_weight).nonzero()\r\n        weight_mismatch_pct = len(weight_mismatches) / len(weight_delta) * 100\r\n\r\n        bias_delta = torch.abs(grad_cpu_ref[1] - bias_grad_gpu_target)\r\n        bias_mismatches = (bias_delta >= eps_bias).nonzero()\r\n        bias_mismatch_pct = len(bias_mismatches) / len(bias_delta) * 100\r\n\r\n        if weight_mismatch_pct > 0 or bias_mismatch_pct > 0:\r\n            print(\r\n                \"Size ({} x {}) mismatch percentage: weight {:3.2f} bias {:3.2f}\".format(\r\n                    fs, bs, weight_mismatch_pct, bias_mismatch_pct\r\n                )\r\n            )\r\n\r\n\r\n# Run the correctness tests\r\nrun_correctness_test(0.01, 0.01)\r\ntorch.cuda.synchronize()\r\n\r\n# Allocate a tensor equal to L2 cache size on A100 GPUs\r\nl2_cache_flusher = torch.empty(int(80 * (1024**2)), dtype=torch.float, device=\"cuda\")\r\n\r\n# Run the performance tests. We need to run this at global scope because otherwise\r\n# the `ln` and `gO` objects are likely removed by the JIT compiler\r\nresults = []\r\nfor dtype in (torch.float, torch.half):\r\n    for val in l_inputs:\r\n        bs = val[0][0]\r\n        fs = val[0][1]\r\n        iterations = val[1]\r\n\r\n        ln = torch.nn.LayerNorm((fs,), device=\"cuda\", dtype=dtype)\r\n        X = torch.randn(bs, fs, device=\"cuda\", dtype=dtype, requires_grad=True)\r\n        gO = torch.rand_like(X)\r\n\r\n        # Try to measure FWD and BWD pass in the same loop\r\n        l_ev_start_fwd = [torch.cuda.Event(enable_timing=True)] * iterations\r\n        l_ev_stop_fwd = [torch.cuda.Event(enable_timing=True)] * iterations\r\n        l_ev_stop_bwd = [torch.cuda.Event(enable_timing=True)] * iterations\r\n\r\n        l_fwd_times = []\r\n        l_bwd_times = []\r\n        torch.cuda.synchronize()\r\n        for i in range(iterations):\r\n            l2_cache_flusher.zero_()\r\n            torch.cuda._sleep(1_000_000)\r\n\r\n            X.grad = None\r\n            ln.zero_grad(set_to_none=True)\r\n\r\n            l_ev_start_fwd[i].record()\r\n            out = ln(X)\r\n            l_ev_stop_fwd[i].record()\r\n            out.backward(gO)\r\n            l_ev_stop_bwd[i].record()\r\n        torch.cuda.synchronize()\r\n\r\n        l_fwd_times = []\r\n        l_bwd_times = []\r\n        for i in range(iterations):\r\n            l_fwd_times.append(l_ev_start_fwd[i].elapsed_time(l_ev_stop_fwd[i]))\r\n            l_bwd_times.append(l_ev_stop_fwd[i].elapsed_time(l_ev_stop_bwd[i]))\r\n\r\n        print(\r\n            \"({}, {}, {}, fwd_ms, bwd_ms)|{:.3f}|{:.3f}\".format(\r\n                dtype,\r\n                bs,\r\n                fs,\r\n                sum(l_fwd_times) / iterations * 1000,\r\n                sum(l_bwd_times) / iterations * 1000,\r\n            )\r\n        )\r\n```\r\n\r\nResults in the attached picture:\r\n\r\n<img width=\"314\" alt=\"Screenshot 2023-10-16 at 11 08 25 AM\" src=\"https://github.com/pytorch/pytorch/assets/23515689/ce571fc5-c84e-47eb-95f6-9faa44042cc1\">\r\n\r\nI also isolated the previous implementation and the vectorized one into a native CUDA program and the speedup is confirmed. **Average speedup = 21.73%**\r\n\r\n```\r\nSize (2048, 2048); Mismatches: dX = 0 out of 4194304. Max missmatch idx = 0.                                                                                                                                                                                           [16/1529]\r\nreference = 0.0560 (ms); optimized = 0.0435 (ms); bw_opt = 1437.54 GB/s; speedup = 28.78%\r\nSize (4096, 512); Mismatches: dX = 0 out of 2097152. Max missmatch idx = 0.\r\nreference = 0.0220 (ms); optimized = 0.0174 (ms); bw_opt = 1797.26 GB/s; speedup = 26.44%\r\nSize (1024, 512); Mismatches: dX = 0 out of 524288. Max missmatch idx = 0.\r\nreference = 0.0101 (ms); optimized = 0.0082 (ms); bw_opt = 953.49 GB/s; speedup = 22.97%\r\nSize (1024, 256); Mismatches: dX = 1 out of 262144. Max missmatch idx = 22411.\r\nreference = 0.0082 (ms); optimized = 0.0075 (ms); bw_opt = 521.14 GB/s; speedup = 9.21%\r\nSize (1024, 1024); Mismatches: dX = 0 out of 1048576. Max missmatch idx = 0.\r\nreference = 0.0137 (ms); optimized = 0.0108 (ms); bw_opt = 1447.42 GB/s; speedup = 26.93%\r\nSize (2048, 512); Mismatches: dX = 0 out of 1048576. Max missmatch idx = 0.\r\nreference = 0.0141 (ms); optimized = 0.0116 (ms); bw_opt = 1349.79 GB/s; speedup = 21.81%\r\nSize (2048, 256); Mismatches: dX = 0 out of 524288. Max missmatch idx = 0.\r\nreference = 0.0108 (ms); optimized = 0.0102 (ms); bw_opt = 768.90 GB/s; speedup = 6.09%\r\nSize (1024, 128); Mismatches: dX = 1 out of 131072. Max missmatch idx = 9165.\r\nreference = 0.0070 (ms); optimized = 0.0068 (ms); bw_opt = 288.56 GB/s; speedup = 2.81%\r\nSize (1024, 2048); Mismatches: dX = 0 out of 2097152. Max missmatch idx = 0.\r\nreference = 0.0223 (ms); optimized = 0.0164 (ms); bw_opt = 1905.58 GB/s; speedup = 35.90%\r\nSize (1024, 768); Mismatches: dX = 3 out of 786432. Max missmatch idx = 507105.\r\nreference = 0.0113 (ms); optimized = 0.0101 (ms); bw_opt = 1160.00 GB/s; speedup = 11.79%\r\nSize (2048, 128); Mismatches: dX = 0 out of 262144. Max missmatch idx = 0.\r\nreference = 0.0097 (ms); optimized = 0.0089 (ms); bw_opt = 440.97 GB/s; speedup = 9.12%\r\nSize (2048, 1024); Mismatches: dX = 0 out of 2097152. Max missmatch idx = 0.\r\nreference = 0.0204 (ms); optimized = 0.0166 (ms); bw_opt = 1881.43 GB/s; speedup = 22.81%\r\nSize (4096, 256); Mismatches: dX = 1 out of 1048576. Max missmatch idx = 601965.\r\nreference = 0.0156 (ms); optimized = 0.0154 (ms); bw_opt = 1016.47 GB/s; speedup = 1.24%\r\nSize (4096, 1024); Mismatches: dX = 0 out of 4194304. Max missmatch idx = 0.\r\nreference = 0.0411 (ms); optimized = 0.0417 (ms); bw_opt = 1499.55 GB/s; speedup = -1.43%\r\nSize (4096, 4096); Mismatches: dX = 0 out of 16777216. Max missmatch idx = 0.\r\nreference = 0.2323 (ms); optimized = 0.2077 (ms); bw_opt = 1203.75 GB/s; speedup = 11.83%\r\nSize (1024, 4096); Mismatches: dX = 0 out of 4194304. Max missmatch idx = 0.\r\nreference = 0.0659 (ms); optimized = 0.0570 (ms); bw_opt = 1096.51 GB/s; speedup = 15.60%\r\nSize (1024, 3072); Mismatches: dX = 0 out of 3145728. Max missmatch idx = 0.\r\nreference = 0.0425 (ms); optimized = 0.0299 (ms); bw_opt = 1568.10 GB/s; speedup = 42.11%\r\nSize (1024, 2464); Mismatches: dX = 8 out of 2523136. Max missmatch idx = 2087476.\r\nreference = 0.0292 (ms); optimized = 0.0230 (ms); bw_opt = 1636.18 GB/s; speedup = 27.07%\r\nSize (1024, 800); Mismatches: dX = 1 out of 819200. Max missmatch idx = 652342.\r\nreference = 0.0114 (ms); optimized = 0.0104 (ms); bw_opt = 1175.05 GB/s; speedup = 9.63%\r\nSize (1024, 6144); Mismatches: dX = 0 out of 6291456. Max missmatch idx = 0.\r\nreference = 0.0973 (ms); optimized = 0.0844 (ms); bw_opt = 1110.87 GB/s; speedup = 15.28%\r\nSize (1024, 4904); Mismatches: dX = 6 out of 5021696. Max missmatch idx = 4670210.\r\nreference = 0.0814 (ms); optimized = 0.0721 (ms); bw_opt = 1037.99 GB/s; speedup = 12.90%\r\nSize (4096, 2048); Mismatches: dX = 0 out of 8388608. Max missmatch idx = 0.\r\nreference = 0.0990 (ms); optimized = 0.0770 (ms); bw_opt = 1623.58 GB/s; speedup = 28.54%\r\nSize (1024, 1860); Mismatches: dX = 0 out of 1904640. Max missmatch idx = 0.\r\nreference = 0.0219 (ms); optimized = 0.0174 (ms); bw_opt = 1631.12 GB/s; speedup = 25.75%\r\nSize (1024, 20160); Mismatches: dX = 23 out of 20643840. Max missmatch idx = 20274656.\r\nreference = 0.3054 (ms); optimized = 0.2600 (ms); bw_opt = 1183.08 GB/s; speedup = 17.45%\r\nSize (3072, 256); Mismatches: dX = 0 out of 786432. Max missmatch idx = 0.\r\nreference = 0.0129 (ms); optimized = 0.0127 (ms); bw_opt = 925.71 GB/s; speedup = 1.69%\r\nSize (4096, 128); Mismatches: dX = 3 out of 524288. Max missmatch idx = 451331.\r\nreference = 0.0128 (ms); optimized = 0.0129 (ms); bw_opt = 608.06 GB/s; speedup = -0.74%\r\nSize (512, 128); Mismatches: dX = 0 out of 65536. Max missmatch idx = 0.\r\nreference = 0.0062 (ms); optimized = 0.0061 (ms); bw_opt = 161.25 GB/s; speedup = 2.35%\r\nSize (2048, 64); Mismatches: dX = 0 out of 131072. Max missmatch idx = 0.\r\nreference = 0.0084 (ms); optimized = 0.0086 (ms); bw_opt = 228.70 GB/s; speedup = -2.49%\r\nSize (3072, 2048); Mismatches: dX = 0 out of 6291456. Max missmatch idx = 0.\r\nreference = 0.0770 (ms); optimized = 0.0614 (ms); bw_opt = 1527.43 GB/s; speedup = 25.44%\r\nSize (3200, 104); Mismatches: dX = 0 out of 332800. Max missmatch idx = 0.\r\nreference = 0.0105 (ms); optimized = 0.0113 (ms); bw_opt = 440.93 GB/s; speedup = -6.96%\r\nSize (1152, 384); Mismatches: dX = 0 out of 442368. Max missmatch idx = 0.\r\nreference = 0.0102 (ms); optimized = 0.0084 (ms); bw_opt = 786.48 GB/s; speedup = 21.59%\r\nSize (131072, 64); Mismatches: dX = 12 out of 8388608. Max missmatch idx = 7659094.\r\nreference = 0.2054 (ms); optimized = 0.2873 (ms); bw_opt = 438.49 GB/s; speedup = -28.51%\r\nSize (64, 131072); Mismatches: dX = 0 out of 8388608. Max missmatch idx = 0.\r\nreference = 0.8372 (ms); optimized = 0.3295 (ms); bw_opt = 379.37 GB/s; speedup = 154.09%\r\nSize (131072, 128); Mismatches: dX = 18 out of 16777216. Max missmatch idx = 16158071.\r\nreference = 0.2296 (ms); optimized = 0.3116 (ms); bw_opt = 805.47 GB/s; speedup = -26.31%\r\nSize (128, 131072); Mismatches: dX = 0 out of 16777216. Max missmatch idx = 0.\r\nreference = 0.9297 (ms); optimized = 0.3785 (ms); bw_opt = 660.52 GB/s; speedup = 145.64%\r\nSize (131072, 256); Mismatches: dX = 47 out of 33554432. Max missmatch idx = 33062426.\r\nreference = 0.3003 (ms); optimized = 0.4231 (ms); bw_opt = 1184.07 GB/s; speedup = -29.02%\r\nSize (256, 131072); Mismatches: dX = 0 out of 33554432. Max missmatch idx = 0.\r\nreference = 1.0449 (ms); optimized = 0.4828 (ms); bw_opt = 1035.63 GB/s; speedup = 116.43%\r\nAverage speedup = 21.73%\r\n```\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 34,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-10-11T02:44:15Z",
        "closed_at": "2023-10-12T15:13:57Z",
        "merged_at": null,
        "body": "# Motivation\r\nsupport xpu channel last for inductor layout optimization path.\r\nCurrently, `_conv_determine_backend_memory_format` always returns torch.contiguous_format for XPU conv.\r\n\r\n# Solution\r\nAdd xpu channel last detection stragey in `determine_backend_memory_format`\r\n\r\n\n\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "comments": 24
    },
    {
        "merged": false,
        "additions": 455,
        "deletions": 192,
        "changed_files": 7,
        "created_at": "2023-10-11T02:15:22Z",
        "closed_at": "2023-10-12T03:39:11Z",
        "merged_at": null,
        "body": "Summary:\nPreviously we design the GraphSignature format as a bunch of inputs and outputs node names. After a discussion in the design meeting we decide to change the format to make signature more self-contained. Now the signature format look like the following:\n```\n[\nInputSpec(\n   kind=InputKind.USER_INPUT,\n   arg=TensorArgument(name=\"arg0_1\"),\n   target=None,\n),\n...\n]\n```\n\nTest Plan: CI\n\nReviewed By: angelayi\n\nDifferential Revision: D49876258\n\n\n\n\ncc @avikchaudhuri @gmagogsfm @tugsbayasgalan",
        "comments": 20
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-11T02:11:49Z",
        "closed_at": "2023-10-11T19:41:42Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #111016\r\n\r\nDynamo is swallowing a user exception when suppress_errors is set to True. There's an issue filed for that: https://github.com/pytorch/pytorch/issues/108798. In the meantime we still like the functionality in this test which works without the default setting (dont suppress errors) to not regress.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 173,
        "deletions": 62,
        "changed_files": 3,
        "created_at": "2023-10-11T02:03:36Z",
        "closed_at": "2023-10-11T17:53:31Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111015\n\nFixes #110711\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-11T01:24:05Z",
        "closed_at": "2023-10-11T06:40:25Z",
        "merged_at": null,
        "body": "My typo mistake after #110976",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 0,
        "changed_files": 3,
        "created_at": "2023-10-11T00:50:28Z",
        "closed_at": "2023-10-11T15:41:38Z",
        "merged_at": null,
        "body": "To fix distributed compilation with clang-15\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/110974\r\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 63,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-10-11T00:43:21Z",
        "closed_at": "2023-10-11T20:59:55Z",
        "merged_at": null,
        "body": "Fixes https://github.com/pytorch/pytorch/issues/110666\r\n\r\nSlight update to original PR here: https://github.com/pytorch/pytorch/pull/111005\r\n\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111040\n* __->__ #111011\n\r\n",
        "comments": 16
    },
    {
        "merged": false,
        "additions": 44,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2023-10-11T00:42:50Z",
        "closed_at": "2023-10-11T15:31:19Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* (to be filled)\n\n",
        "comments": 2
    },
    {
        "merged": true,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-11T00:41:48Z",
        "closed_at": "2023-10-11T16:54:23Z",
        "merged_at": "2023-10-11T16:54:23Z",
        "body": "So we can release the binary.",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 55,
        "deletions": 24,
        "changed_files": 9,
        "created_at": "2023-10-11T00:39:15Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes https://github.com/pytorch/pytorch/issues/110940\r\n\r\ncc @janeyx99 @crcrpar \r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 39,
        "deletions": 41,
        "changed_files": 2,
        "created_at": "2023-10-11T00:30:55Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# Summary\r\nThis is a prep PR for updating the FAV2 kernels in core\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 14,
        "changed_files": 8,
        "created_at": "2023-10-11T00:26:42Z",
        "closed_at": "2023-10-14T14:15:51Z",
        "merged_at": null,
        "body": null,
        "comments": 18
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 6,
        "changed_files": 2,
        "created_at": "2023-10-11T00:09:18Z",
        "closed_at": "2023-10-12T07:32:22Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111005\n* #110932\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 872,
        "deletions": 827,
        "changed_files": 3,
        "created_at": "2023-10-10T23:46:53Z",
        "closed_at": "2023-10-12T01:16:11Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111004\n\nSummary:\natt\n\nTest Plan:\npython test/test_quantization.py TestXNNPACKQuantizer\npython test/test_quantization.py TestXNNPACKQuantizerModels\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-10T23:16:58Z",
        "closed_at": "2023-10-11T05:16:07Z",
        "merged_at": null,
        "body": "Otherwise following error is thrown when attempted to compile with WERROR enabled:\r\n```\r\nIn file included from /home/nshulga/git/pytorch/pytorch/torch/csrc/distributed/c10d/socket.cpp:30:\r\n/home/nshulga/git/pytorch/pytorch/third_party/fmt/include/fmt/chrono.h:340:24: warning: redundant redeclaration of \u2018constexpr\u2019 static data member \u2018fmt::v10::detail::codecvt_result<CodeUnit>::max_size\u2019 [-Wdeprecated]\r\n  340 | constexpr const size_t codecvt_result<CodeUnit>::max_size;\r\n      |                        ^~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/nshulga/git/pytorch/pytorch/third_party/fmt/include/fmt/chrono.h:335:33: note: previous declaration of \u2018fmt::v10::detail::codecvt_result<CodeUnit>::max_size\u2019\r\n  335 |   static constexpr const size_t max_size = 32;\r\n      |                                 ^~~~~~~~\r\n```\r\nor following if using clang as host compiler\r\n```\r\nIn file included from /Users/nshulga/git/pytorch/pytorch/torch/csrc/distributed/c10d/socket.cpp:30:\r\n/Users/nshulga/git/pytorch/pytorch/third_party/fmt/include/fmt/chrono.h:340:50: warning: out-of-line definition of constexpr static data member is redundant in C++17 and is deprecated [-Wdeprecated]\r\nconstexpr const size_t codecvt_result<CodeUnit>::max_size;\r\n```\r\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 48,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-10-10T23:15:59Z",
        "closed_at": "2023-10-12T02:11:51Z",
        "merged_at": null,
        "body": "Currently, we only support intranode TP when compositin TP with other parallelism. This PR adds additional check to validate the TP mesh dim in TP initialization when parent mesh exists. \r\n\r\ncc. @fegin, @fduwjj ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-10T23:10:42Z",
        "closed_at": "2023-10-11T19:43:53Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #111000\n\nSummary:\nThis is a hacky flag that we had before in fx flow, and we don't want this in the new flow\n\nTest Plan:\npython test/test_quantization.py TestQuantizePT2E\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 20,
        "deletions": 46,
        "changed_files": 6,
        "created_at": "2023-10-10T22:45:43Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110999\n\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-10T22:33:32Z",
        "closed_at": "2023-10-11T19:52:21Z",
        "merged_at": null,
        "body": "Fixes #101213\r\n\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 6,
        "deletions": 3,
        "changed_files": 1,
        "created_at": "2023-10-10T22:31:43Z",
        "closed_at": null,
        "merged_at": null,
        "body": "To reduce signal time\r\n\n\ncc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 224,
        "deletions": 45,
        "changed_files": 9,
        "created_at": "2023-10-10T22:25:36Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #109889\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-10T22:16:05Z",
        "closed_at": "2023-10-17T15:17:35Z",
        "merged_at": null,
        "body": "Fixes #ISSUE_NUMBER\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 148,
        "deletions": 3,
        "changed_files": 4,
        "created_at": "2023-10-10T22:06:05Z",
        "closed_at": "2023-10-13T20:08:47Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #110575\n* __->__ #110990\n\n`sys.modules` is currently treated as a constant dictionary and any reference to\nit will result in guards on the full contents of `sys.modules`. This instead\nadds a specialized variable tracker which tries to guard only on the modules\nreferenced by the code. e.g.\n\n```\nsys.modules[\"operator\"].add(x, x)\n```\n\nwill generate the guard\n```\n___dict_contains('operator', G['sys'].modules)\n```\n\nIt does this with special support for `__contains__` `__getitem__` and `.get`\nwhich are probably the most commonly used with `sys.modules`. For anything else\nwe just fall back to building the dict tracker as normal.\n\nWhile accessing `sys.modules` may seem unusual, it actually comes up when\ninlining the `warnings.catch_warnings` context manager which internally accesses\n`sys.modules[\"warnings\"]`.\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 6080,
        "deletions": 0,
        "changed_files": 27,
        "created_at": "2023-10-10T21:55:22Z",
        "closed_at": "2023-10-12T06:50:04Z",
        "merged_at": null,
        "body": "Same PR as #110934, but without ghstack.\n\ncc @ptrblck @albanD",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 9,
        "changed_files": 3,
        "created_at": "2023-10-10T21:49:16Z",
        "closed_at": "2023-10-13T13:38:54Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110988\n\nFixes https://github.com/pytorch/pytorch/issues/110696\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 5,
        "changed_files": 3,
        "created_at": "2023-10-10T21:43:51Z",
        "closed_at": "2023-10-11T05:13:16Z",
        "merged_at": null,
        "body": "Better support device agnostic, add a \"cpu\" return for `current_device()` in torch.cpu so that we won't run into `AttributeError: module 'torch.cpu' has no attribute 'current_device'`.\r\n\r\n\r\n\r\ncc @ptrblck @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 10,
        "deletions": 51,
        "changed_files": 6,
        "created_at": "2023-10-10T21:14:04Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110985\n\n\n\ncc @zou3519 @Chillee @samdow @kshitij12345 @janeyx99",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-10-10T20:51:08Z",
        "closed_at": "2023-10-11T17:33:26Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110984\n\nSummary:\nNot sure why this is added in the beginning but this is not a good behavior and we should remove this and fix the callsites\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFx\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nDifferential Revision: [D50144129](https://our.internmc.facebook.com/intern/diff/D50144129)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2023-10-10T20:49:05Z",
        "closed_at": "2023-10-11T16:42:12Z",
        "merged_at": null,
        "body": "Summary: Add more details about the export_memory_timeline API, as we've landed new representations of the memory timeline data.\r\n\r\nTest Plan: CI, should be no functional change, as we only changed comments.\r\n\n\ncc @svekars @carljparker",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 6093,
        "deletions": 1,
        "changed_files": 26,
        "created_at": "2023-10-10T20:41:21Z",
        "closed_at": "2023-10-11T21:47:59Z",
        "merged_at": null,
        "body": "Resubmission without ghstack to make it easier to import https://github.com/pytorch/pytorch/pull/110934/commits\n\ncc @albanD",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 57,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2023-10-10T20:15:28Z",
        "closed_at": "2023-10-11T00:32:42Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #110979\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/93468\r\n\r\nThere's a few extra tests that are sort of unrelated, but I ended up writing them while working on the fix and decided to keep them. The big idea here is to split the `_check` so that `expect_true` works; I could have probably also improved the symbolic reasoning but I'm lazy. One small logging fix too.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 22,
        "changed_files": 1,
        "created_at": "2023-10-10T19:49:51Z",
        "closed_at": "2023-10-10T19:57:31Z",
        "merged_at": null,
        "body": "Rever [Profiler] Improve the docstring for export_memory_timeline #110949 ",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 19,
        "changed_files": 2,
        "created_at": "2023-10-10T19:47:18Z",
        "closed_at": "2023-10-11T14:28:12Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110977\n* #110951\n\nSummary:\nMake it easier to add `generate_opcheck_tests` by adding defaults for\nthe failures_dict location, the additional decorators, and the test\nutils.\n\nTest Plan:\nExisting tests\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 131,
        "deletions": 12,
        "changed_files": 9,
        "created_at": "2023-10-10T19:41:15Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This adds 2 jobs to build PyTorch Android with and without lite interpreter:\r\n\r\n* Keep the list of currently supported ABI armeabi-v7a, arm64-v8a, x86, x86_64\r\n* Pass all the test on emulator\r\n* Run an the test app on emulator and my Android phone `arm64-v8a` without any issue\r\n![Screenshot_20231010-114453](https://github.com/pytorch/pytorch/assets/475357/57e12188-1675-44d2-a259-9f9577578590)\r\n* Run on AWS https://us-west-2.console.aws.amazon.com/devicefarm/home#/mobile/projects/b531574a-fb82-40ae-b687-8f0b81341ae0/runs/5fce6818-628a-4099-9aab-23e91a212076",
        "comments": 12
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-10-10T19:24:05Z",
        "closed_at": "2023-10-10T20:38:46Z",
        "merged_at": null,
        "body": "Summary: The bullet list items need proper indentation. Need this forward fix for the docstring.\n\nTest Plan: CI\n\nDifferential Revision: D50136379\n\nPulled By: aaronenyeshi\n\n\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 13,
        "deletions": 9,
        "changed_files": 2,
        "created_at": "2023-10-10T19:19:08Z",
        "closed_at": "2023-10-11T13:30:13Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110972\n\nSummary: To prevent perf regression like the one caused by #110510\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 31,
        "deletions": 13,
        "changed_files": 6,
        "created_at": "2023-10-10T18:46:59Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary:\n## Motivation:\nIn hook, the check_variable_result check the grad size to make sure that the hook function doesn't change the grad size.\nSince C++ Nested tensor doesn't support size, it got blocked when using hook.\n\n## This diff:\nReplace the sym_sizes call with is_same_size which supports nested tensor.\nTo use is_same_size, also change the signatures from TensorBase to Tensor.\n\nTest Plan: add a test case that the hook changes the tensor size and it throws.\n\nDifferential Revision: D50106928\n\n\n",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 17,
        "deletions": 14,
        "changed_files": 1,
        "created_at": "2023-10-10T18:46:27Z",
        "closed_at": null,
        "merged_at": null,
        "body": "\u2026_mean and torch.cov (#109326)\r\n\r\nFixes #109186.\r\n\r\nThis PR updates the docs for\r\n- `torch.var`\r\n- `torch.var_mean`\r\n- `torch.std`\r\n- `torch.std_mean`\r\n- `torch.cov`\r\n\r\nto reflect the actual implementation behavior when `correction >= N`. The math for `torch.cov` should probably be double checked before merging.\r\n\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109326\r\nApproved by: https://github.com/albanD\r\n\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 234,
        "deletions": 16,
        "changed_files": 7,
        "created_at": "2023-10-10T18:35:16Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes https://github.com/pytorch/pytorch/pull/110953/files#r1352868935\r\n\r\nDepends on: https://github.com/pytorch/pytorch/pull/110953\r\n\r\nWhy not use these for `repeat(item, count)`:\r\n> These are not preferred as they return an opaque VariableTracker. In particular, one cannot do `enumerate(repeat(1))`. `repeat(1, 10)` benefits from the integration enjoyed by `ListVariableIterator`\r\n\r\nFollow ups:\r\n- [ ] make listiterator an IteratorVariable, define iterator integrations on base IteratorVariable where unspecialized https://github.com/pytorch/pytorch/pull/110967#discussion_r1356656469\r\n    - Please make a new issue for this \r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @ezyang ",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 139,
        "deletions": 385,
        "changed_files": 11,
        "created_at": "2023-10-10T18:27:59Z",
        "closed_at": "2023-10-11T05:16:54Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110964\n\nThis reverts commit f786fbdebdd24d3a6807e3b9fbf055836db4ad60.\n\nForward fixes\n\ncc @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-10T18:23:02Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Running `python test_nn.py -v -k test_nll_loss_large_tensor` on a machine with a small host RAM availability (e.g. ~50GB) fails with a `SIGKILL` even though the currently specified memory requirements for CPU (and GPU) are set to 48GB and are thus met.\r\n\r\nProfiling the peak memory usage via:\r\n```\r\n\\time -v python test_nn.py -v -k test_nll_loss_large_tensor\r\n```\r\nand adding `print(torch.cuda.memory_summaryu())` at the end of the test shows a higher host RAM usage of >100GB and a device memory usage of ~32GB.\r\n```\r\n\tCommand being timed: \"python test_nn.py -v -k test_nll_loss_large_tensor\"\r\n\tUser time (seconds): 81.66\r\n\tSystem time (seconds): 229.02\r\n\tPercent of CPU this job got: 671%\r\n\tElapsed (wall clock) time (h:mm:ss or m:ss): 0:46.30\r\n\tAverage shared text size (kbytes): 0\r\n\tAverage unshared data size (kbytes): 0\r\n\tAverage stack size (kbytes): 0\r\n\tAverage total size (kbytes): 0\r\n\tMaximum resident set size (kbytes): 118150096\r\n\tAverage resident set size (kbytes): 0\r\n\tMajor (requiring I/O) page faults: 0\r\n\tMinor (reclaiming a frame) page faults: 90280839\r\n\tVoluntary context switches: 1669\r\n\tInvoluntary context switches: 1214548\r\n\tSwaps: 0\r\n\tFile system inputs: 0\r\n\tFile system outputs: 0\r\n\tSocket messages sent: 0\r\n\tSocket messages received: 0\r\n\tSignals delivered: 0\r\n\tPage size (bytes): 4096\r\n\tExit status: 0\r\n```\r\n```\r\n|                  PyTorch CUDA memory summary, device ID 0                 |\r\n|---------------------------------------------------------------------------|\r\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\r\n|===========================================================================|\r\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\r\n|---------------------------------------------------------------------------|\r\n| Allocated memory      |  32769 MiB |  32769 MiB |  81923 MiB |  49154 MiB |\r\n|       from large pool |  32768 MiB |  32768 MiB |  81921 MiB |  49152 MiB |\r\n|       from small pool |      0 MiB |      0 MiB |      1 MiB |      1 MiB |\r\n|---------------------------------------------------------------------------|\r\n| Active memory         |  32769 MiB |  32769 MiB |  81923 MiB |  49154 MiB |\r\n|       from large pool |  32768 MiB |  32768 MiB |  81921 MiB |  49152 MiB |\r\n|       from small pool |      0 MiB |      0 MiB |      1 MiB |      1 MiB |\r\n|---------------------------------------------------------------------------|\r\n| Requested memory      |  32769 MiB |  32769 MiB |  81923 MiB |  49154 MiB |\r\n|       from large pool |  32768 MiB |  32768 MiB |  81921 MiB |  49152 MiB |\r\n|       from small pool |      0 MiB |      0 MiB |      1 MiB |      1 MiB |\r\n|---------------------------------------------------------------------------|\r\n| GPU reserved memory   |  32774 MiB |  32774 MiB |  81938 MiB |  49164 MiB |\r\n|       from large pool |  32772 MiB |  32772 MiB |  81930 MiB |  49158 MiB |\r\n|       from small pool |      2 MiB |      2 MiB |      8 MiB |      6 MiB |\r\n|---------------------------------------------------------------------------|\r\n...\r\n```\r\n\r\nWe haven't seen this issue before as the majority of our runners have sufficient host RAM and I just ran into it by chance.\r\n\r\nCC @atalman @malfet @crcrpar ",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-10T18:00:33Z",
        "closed_at": "2023-10-11T21:26:48Z",
        "merged_at": null,
        "body": "Summary:\r\ncurrent ShardedTensor.gather is not working as expectation when the shard is empty on any rank\r\n\r\nThe root cause is identified that when a sharded tensor has no placement on a specific rank, the metadata doesn't include that rank's placement which introduces KeyError in :                 ```shard_offset = shard_placement[shard. Metadata][1]```\r\n\r\nIt's fixed by adding an empty tensor check.\r\n\r\nTest Plan:\r\nbefore change:\r\n\r\n\r\nafter change:\r\n\r\nDifferential Revision: D50114085\r\n\r\n\r\n",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 282,
        "deletions": 17,
        "changed_files": 4,
        "created_at": "2023-10-10T17:58:53Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110960\n\nKeep a buffer of the last 16384 nccl work actions, including the stack\ntrace that launched the event.\n\nWhen torch._C._distributed_c10d._dump_nccl_trace(), it an dump these to\na pickled archive.\n\nFor each action we get:\nprocess_group_id, seq_id, collective_name, size_of_first_tensor, stack trace\n\nstate - issued, started, completed (based on cuda events and queried if\nnecessary when the dump is requested)\n\nI tested that it is possible to query event state when the streams are\notherwise stuck.\n\nDifferential Revision: [D50138956](https://our.internmc.facebook.com/intern/diff/D50138956)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 207,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-10T17:11:40Z",
        "closed_at": null,
        "merged_at": null,
        "body": "This PR adds in an implementation of WandA pruning: https://arxiv.org/abs/2306.11695\r\n\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 27,
        "deletions": 9,
        "changed_files": 4,
        "created_at": "2023-10-10T16:50:55Z",
        "closed_at": "2023-10-16T18:28:29Z",
        "merged_at": null,
        "body": "Fixes #110597\r\n\r\nSummary:\r\n\r\n* Generic code: The `torch._C.Value.node().mustBeNone()` is encapsulated into the high-level API `JitScalarType.from_value` ; `_is_none` was also extended to allow either `None` or `torch._C.Value.node.mustBeNone()`, so users don't manually call into TorchScript API when implementing operators\r\n* Specific to `new_zeros` (and ops of ` *_like`  and `new_*`): When checking `dtype`, we always must use ` _is_none`, which will call  proposed by #110935",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-10-10T16:48:06Z",
        "closed_at": "2023-10-11T15:10:28Z",
        "merged_at": null,
        "body": "By calling `at::mps::sign_outf` rather than `at::sign_out` that calls dispatcher again.\r\nAlso, do not copy output unnecessarily.\r\n\r\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at f942e74</samp>\n\n> _Metal tensors rise from the ashes_\n> _`sign` and `sgn` unleash their flashes_\n> _MPSFunctions reign supreme_\n> _In the header of the metal dream_",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 41,
        "deletions": 21,
        "changed_files": 1,
        "created_at": "2023-10-10T16:06:33Z",
        "closed_at": "2023-10-11T20:52:14Z",
        "merged_at": null,
        "body": "For the slow path scalar overload, we for-loop call aten::op(tensor_from_list, scalar). This is inefficient as it will call empty_strided and copy_ for every iter of the loop. \r\n\r\nWe can likely take advantage of the heuristics and make the tensor from the scalar only once. The main benefit of this change will be for optimizers, where foreach_add_(steps, 1) is a common codepath and the step tensors are on CPU.\r\n\r\nAnticipated qs:\r\n1. what if the scalarlist is empty? we error on this earlier in the stack, so the kernel can assume the tensorlist has content.\r\n2. what if the rest of the list doesn't match the first tensor in shape, device, dtype, layout? Broadcasting solves for shape and layout. We add a check for dtype + device to maintain BCness. \r\n3. why do I guard on `is_floating_point()`? If we create a bool tensor, the wrong behavior happens where `torch.tensor(True) + 3` will return `torch.tensor(True)` instead of `torch.tensor(4)`. IMO the first behavior makes more sense but would be BC breaking.\r\n4. why not use `wrapped_scalar_tensor()`? We could. However, it defaults to kDouble instead of kFloat due to Python scalars being 64bits wide, which is inconvenient since most training is done in lower precision. Thus, an extra `.to(dtype)` would be required at the very least, and there would still need to be some sort of check added to ensure correctness. This, using `wrapped_scalar_tensor()` did not seem more performant nor clearer.\r\n\r\n\r\nCode to test:\r\n```\r\nimport torch\r\nsteps = [torch.zeros((), device=\"cpu\") for i in range(1000)]\r\n\r\nwith torch.profiler.profile(\r\n    activities=[\r\n        torch.profiler.ProfilerActivity.CPU,\r\n        torch.profiler.ProfilerActivity.CUDA,\r\n    ]\r\n) as p:\r\n    torch._foreach_add_(steps, 1024.1024)\r\n\r\nprint(p.key_averages().table(sort_by=\"cpu_time_total\"))\r\n```\r\n\r\nProfile (for 1000 tensors in a list):\r\n\r\n```\r\ncurrent state of world on main:\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n       aten::_foreach_add        37.43%      52.961ms        99.99%     141.461ms     141.461ms             1  \r\n                aten::add        28.48%      40.300ms        62.55%      88.500ms      88.500us          1000  \r\n                 aten::to         4.12%       5.825ms        34.07%      48.200ms      48.200us          1000  \r\n           aten::_to_copy        10.67%      15.102ms        29.95%      42.375ms      42.375us          1000  \r\n              aten::copy_        14.92%      21.103ms        14.92%      21.103ms      21.103us          1000  \r\n      aten::empty_strided         4.36%       6.170ms         4.36%       6.170ms       6.170us          1000  \r\n    cudaDeviceSynchronize         0.01%      19.000us         0.01%      19.000us      19.000us             1  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 141.480ms\r\n\r\nnew state after pr:\r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\n      aten::_foreach_add_        74.60%      53.748ms        99.97%      72.022ms      72.022ms             1  \r\n               aten::add_        25.28%      18.212ms        25.28%      18.212ms      18.212us          1000  \r\n          aten::new_empty         0.03%      22.000us         0.07%      51.000us      51.000us             1  \r\n              aten::empty         0.04%      29.000us         0.04%      29.000us      29.000us             1  \r\n    cudaDeviceSynchronize         0.03%      22.000us         0.03%      22.000us      22.000us             1  \r\n              aten::fill_         0.02%      11.000us         0.02%      11.000us      11.000us             1  \r\n-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \r\nSelf CPU time total: 72.044ms\r\n```\r\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #110954\r\n\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 35,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-10-10T15:25:19Z",
        "closed_at": "2023-10-10T20:40:42Z",
        "merged_at": null,
        "body": "Fixes https://github.com/pytorch/pytorch/issues/110286\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @ezyang \r\n\r\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2023-10-10T15:19:26Z",
        "closed_at": "2023-10-10T17:59:36Z",
        "merged_at": null,
        "body": "1. Updates to patch release process\r\n2. Add Release cadence section \r\n3. Changed description for Modify release matrix to reflect current process",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 43,
        "deletions": 1,
        "changed_files": 3,
        "created_at": "2023-10-10T15:14:26Z",
        "closed_at": "2023-10-10T21:43:50Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #110977\n* __->__ #110951\n\nThis PR adds the following helper functions for generated opcheck tests:\n- dontGenerateOpCheckTests is a decorator that skips generation of the\n  opcheck tests for the generated function\n- is_inside_opcheck_mode lets us query if we are in a generated test.\n  Useful for fast debugging out-of-tree without needing to update\n  PyTorch.\n\nTest Plan:\n- new tests",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 22,
        "deletions": 9,
        "changed_files": 1,
        "created_at": "2023-10-10T14:33:53Z",
        "closed_at": "2023-10-10T17:54:03Z",
        "merged_at": null,
        "body": "Summary: Add more details about the export_memory_timeline API, as we've landed new representations of the memory timeline data.\r\n\r\nTest Plan: CI, should be no functional change, as we only changed comments.\r\n\r\n\r\n\r\n",
        "comments": 17
    },
    {
        "merged": false,
        "additions": 75,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-10T13:49:43Z",
        "closed_at": "2023-10-12T15:09:40Z",
        "merged_at": null,
        "body": "This PR fixes potential double resharding of parameters that both:\r\n\r\n1. requires no gradient and,\r\n2. were used more than once during forward pass.\r\n\r\n[`_register_post_backward_hook`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/_runtime_utils.py#L1415) handles the case correctly, this PR does the same for `_register_post_backward_reshard_only_hook`.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2023-10-10T13:19:17Z",
        "closed_at": "2023-10-10T22:28:47Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110947\n\nAvoid changing default for other backends as CPU backend (GLOO) may need\nlonger timeouts.\n\nMotivated by trying to save cluster time when encountering collective\nhangs.  Generally collectives should time out within seconds and 30\nminutes (or 10 minutes) should provide ample headroom for edge cases.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 4,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-10T12:40:45Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Fixes #110933 \n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-10T12:12:41Z",
        "closed_at": "2023-10-10T12:38:46Z",
        "merged_at": null,
        "body": "Fixes #110933 \r\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 24,
        "deletions": 24,
        "changed_files": 1,
        "created_at": "2023-10-10T12:11:25Z",
        "closed_at": "2023-10-13T22:46:50Z",
        "merged_at": null,
        "body": "Fixes #110917 ",
        "comments": 21
    },
    {
        "merged": false,
        "additions": 144,
        "deletions": 25,
        "changed_files": 5,
        "created_at": "2023-10-10T11:02:41Z",
        "closed_at": "2023-10-17T10:17:51Z",
        "merged_at": null,
        "body": "Supersedes #109004.",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 1,
        "changed_files": 2,
        "created_at": "2023-10-10T08:24:38Z",
        "closed_at": "2023-10-16T16:42:42Z",
        "merged_at": null,
        "body": "Fixes export in https://github.com/pytorch/pytorch/issues/110597\r\n\r\ncc @justinchuby \r\n",
        "comments": 11
    },
    {
        "merged": false,
        "additions": 6080,
        "deletions": 0,
        "changed_files": 27,
        "created_at": "2023-10-10T07:57:03Z",
        "closed_at": "2023-10-12T06:49:28Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110934\n\n\n\ncc @ptrblck @albanD\n\nDifferential Revision: [D50140556](https://our.internmc.facebook.com/intern/diff/D50140556)",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 5,
        "changed_files": 1,
        "created_at": "2023-10-10T07:26:30Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #111005\n* __->__ #110932\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 3,
        "changed_files": 2,
        "created_at": "2023-10-10T07:24:56Z",
        "closed_at": null,
        "merged_at": null,
        "body": "## How to reproduce:\r\n```py\r\nimport os\r\nimport tempfile\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.optim import SGD\r\nfrom torch.optim.lr_scheduler import CyclicLR\r\n\r\n\r\nmodel = nn.Linear(100, 100)\r\nopt = SGD(model.parameters(), lr=1.)\r\nscheduler = CyclicLR(opt, base_lr=0.1, max_lr=0.2, scale_fn=lambda x: 0.99)\r\n\r\ntmp = tempfile.NamedTemporaryFile(delete=False)\r\ntry:\r\n    torch.save(scheduler.state_dict(), tmp.name)\r\n    scheduler.load_state_dict(torch.load(tmp.name))\r\nfinally:\r\n    tmp.close()\r\n    os.unlink(tmp.name)\r\n```\r\nError: \r\n```\r\n_pickle.PicklingError: Can't pickle <function <lambda> at 0x000001A51DF67600>: attribute lookup <lambda> on __main__ failed\r\n```\r\n## Fix:\r\nSaving `scale_fn` to the state dict only if it is a callable object and not if it is a function or lambda.\r\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 230,
        "deletions": 46,
        "changed_files": 5,
        "created_at": "2023-10-10T07:05:21Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110930\n* #109765\n* #109301\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 5,
        "deletions": 0,
        "changed_files": 1,
        "created_at": "2023-10-10T06:05:59Z",
        "closed_at": "2023-10-10T10:34:45Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110929\n\nThis will ensure all the tensors are on FSDP compute_device.\n\nDifferential Revision: [D50059492](https://our.internmc.facebook.com/intern/diff/D50059492/)",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 32,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-10T05:58:51Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Follow up to https://github.com/pytorch/pytorch/pull/110927\r\n\r\nExample output:\r\n\r\nWith `report_guard_failure_values=True`.\r\n\r\nWith all failures enabled:\r\n```\r\n[2023-10-10 02:04:37,366] torch._dynamo.convert_frame.__recompiles: [DEBUG] ('Recompiling function inner in /home/jonch/Desktop/Programming/mlsys/pytorch/torch/_dynamo/external_utils.py:15', 'triggered by the following guard failures: [\\'___guarded_code.valid\\', \"___check_obj_id(L[\\'args\\'][1], 140039813517312), with LHS value: {\\'id\\': 140040114487792}\", \"___check_obj_id(L[\\'args\\'][2], 140039813517168), with LHS value: {\\'id\\': 140040124395392}\"]')\r\n\r\n```\r\n\r\nWith default first failure:\r\n\r\n```\r\n.[2023-10-10 01:56:07,749] torch._dynamo.convert_frame.__recompiles: [DEBUG] ('Recompiling function fn in /home/jonch/Desktop/Programming/mlsys/pytorch/test/dynamo/test_misc.py:1354', \"triggered by the following guard failure: L['a'] == 0, with LHS value: 1\")\r\n[2023-10-10 01:56:07,757] torch._dynamo.convert_frame.__recompiles: [DEBUG] ('Recompiling function fn in /home/jonch/Desktop/Programming/mlsys/pytorch/test/dynamo/test_misc.py:1354', \"triggered by the following guard failure: tensor '__as_tensor(L['i'])' size mismatch at index 0. expected 1, actual 5\")\r\n[2023-10-10 01:56:07,767] torch._dynamo.convert_frame.__recompiles: [DEBUG] ('Recompiling function fn in /home/jonch/Desktop/Programming/mlsys/pytorch/test/dynamo/test_misc.py:1354', \"triggered by the following guard failure: ___check_type_id(L['a'], 94852103176992), with LHS value: {'type': <class 'NoneType'>}\")\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @lezcano @eellison ",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 12,
        "changed_files": 3,
        "created_at": "2023-10-10T05:03:33Z",
        "closed_at": "2023-10-10T19:57:52Z",
        "merged_at": null,
        "body": "Fixes https://github.com/pytorch/pytorch/issues/110879\r\n\r\nExample output:\r\n```\r\n('Recompiling function fn in /home/jonch/Desktop/Programming/mlsys/pytorch/test/dynamo/test_misc.py:4578', 'triggered by the following guard failures: [\"___check_type_id(L[\\'obj\\'], 94834370481168)\", \"L[\\'obj\\'].x == -0.5\"]')\r\n```\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng @yanboliang @Fidget-Spinner @anijain2305 @soulitzer @lezcano ",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 8,
        "changed_files": 2,
        "created_at": "2023-10-10T04:50:26Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary:\nBacground:\n\nFor Photo processing on IQCloud service on Photo GraphNode, we want to expose CompPhoto SDK code in xplat to be available in fbcode.\n\nI have exported all necessary CompPhoto dependencies to fbcode.\n\nI had to handle conflict with global variable names for\nBool and Complex types between platform X11 and Caffe2 library. The #undef directive is used to make sure the conflict doesn't exist.\n\nI was able to compile and build the following\n\nbuck2 build //xplat/compphoto/sdk/pipeline/graphs/stella/stella_background/image:StellaBackgroundImageProcessingGraphFbcode\n\nTest Plan:\nbuck2 build //xplat/compphoto/sdk/pipeline/graphs/stella/stella_background/image:StellaBackgroundImageProcessingGraphFbcode\n\nFile changed: fbsource//xplat/caffe2/c10/core/ScalarType.h\nFile changed: fbcode//caffe2/c10/core/ScalarType.h\nBuck UI: https://www.internalfb.com/buck2/da8ed23b-0735-4325-881c-796bb01b0ce4\nNetwork: Up: 10MiB  Down: 511MiB  (reSessionID-1fda6d02-0545-4135-ba2d-57e5d55b5971)\nJobs completed: 4438. Time elapsed: 8:06.7s.\nCache hits: 1%. Commands: 2189 (cached: 25, remote: 2156, local: 8)\nBUILD SUCCEEDED\n\n\n********************\n\nbuck2 build //xplat/compphoto/sdk/pipeline/graphs/stella/stella_background/image:StellaBackgroundImageProcessingGraph\nBuck UI: https://www.internalfb.com/buck2/191b4f4a-2bc7-436b-9d3b-d322bad5c6f5\nNetwork: Up: 232KiB  Down: 416KiB  (reSessionID-3fb50098-a64d-4cf2-923f-375953beac6e)\nJobs completed: 12. Time elapsed: 19.9s.\nCache hits: 0%. Commands: 3 (cached: 0, remote: 0, local: 3)\nBUILD SUCCEEDED\n\nDifferential Revision: D50036045\n\n\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 187,
        "deletions": 18,
        "changed_files": 5,
        "created_at": "2023-10-10T04:48:36Z",
        "closed_at": "2023-10-11T18:22:27Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110925\n* #110846\n* #110831\n\r\nThis PR adds a all_gather_dtensor() method to fsdp/_fsdp_extensions.py and the actual implementation in tensor/parallel/fsdp.py. This enables FSDP to load 2D DTensor state_dict into model when calling `model.load_state_dict()`.\r\n\r\ncc. @fegin \r\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 0,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-10T04:35:58Z",
        "closed_at": "2023-10-10T09:02:12Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #110924\r\n\r\n## Context\r\n\r\nFollowing up from @peterbell10 comments on https://github.com/pytorch/pytorch/pull/110882.\r\n\r\n* `empty_like` was erroneously classified as `core`. It can be decomposed using `empty_permuted` and in fact is currently decomposed this way in the core decomposition table.\r\n* `full_like` can be similarly decomposed to `full_permuted` once https://github.com/pytorch/pytorch/pull/110234 lands. The current decomposition into `empty_like` and `fill` doesn't work because `fill` decomposes to `full_like`, resulting in a recursive loop.",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 319,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2023-10-10T04:18:29Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary: We add a new pattern to merge getitem_cat to enable further split merges\n\nTest Plan:\n### test mcf model\nPatch D49972740\n```\nbuck2 run mode/opt //scripts/jackiexu0313/pt2:local_model_with_pt2 -- --test_mode split-only -c\n```\nsplit_cat_fx_passes: True\nbatch_fusion:  False\ngroup_fusion:  False\nRunning pattern matcher pass: PatternMatcherPass\nAfter pattern matcher pass:  https://www.internalfb.com/intern/everpaste/?color=0&handle=GMXjOBMnJY_5KOEFAMde4eDevKYEbr0LAAAz\nRunning pattern matcher pass: PatternMatcherPass\nmerge_getitem_cat pass\nAfter pattern matcher pass:  https://www.internalfb.com/intern/everpaste/?color=0&handle=GAnJ_hKkhC4gQUIBAJvJy1M8EGRkbr0LAAAz\nRunning pattern matcher pass: PatternMatcherPass\nAfter pattern matcher pass:  https://www.internalfb.com/intern/everpaste/?color=0&handle=GPj0tgSYVtonz3oCAFtxH-GKZWszbr0LAAAz\nRunning pattern matcher pass: PatternMatcherPass\nAfter pattern matcher pass:  https://www.internalfb.com/intern/everpaste/?color=0&handle=GEAO2gQvoiCWzIAEAAPNkJ6Ns-1bbr0LAAAz\nRunning pattern matcher pass: PatternMatcherPass\nAfter pattern matcher pass:  https://www.internalfb.com/intern/everpaste/?color=0&handle=GBX8DxArHaki_1sAADm7Vw8N9NY7br0LAAAz\nRunning pattern matcher pass: PatternMatcherPass\nAfter pattern matcher pass:  https://www.internalfb.com/intern/everpaste/?color=0&handle=GLXE_BZbXITGQKkCALO3xqXvGtV0br0LAAAz\n compare parameters. Numerical result :True\n compare loss/predict. Numerical result :True\n compare param grad. Numerical result :True\n\nP850153017\n\n### unit test\n```\nbuck2 test mode/dev-nosan //caffe2/test/inductor:split_cat_fx_passes -- test_getitem_cat_merge\n```\nBuck UI: https://www.internalfb.com/buck2/eb7411a5-a6bd-46bc-bf66-756341e3ce10\nTest UI: https://www.internalfb.com/intern/testinfra/testrun/13792273864439068\nNetwork: Up: 48KiB  Down: 15KiB  (reSessionID-39ca57cc-5743-423e-b94f-9d0f642010f8)\nJobs completed: 8. Time elapsed: 1:44.7s.\nCache hits: 0%. Commands: 2 (cached: 0, remote: 0, local: 2)\nTests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0\n\n### before vs after transformation\nhttps://www.internalfb.com/intern/diffing/?paste_number=847958889\n\nDifferential Revision: D50100667\n\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 2,
        "changed_files": 1,
        "created_at": "2023-10-10T04:15:45Z",
        "closed_at": "2023-10-16T14:29:14Z",
        "merged_at": null,
        "body": "Fixes #110769\r\n\r\nas stated.\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 52,
        "deletions": 26,
        "changed_files": 2,
        "created_at": "2023-10-10T03:55:22Z",
        "closed_at": "2023-10-11T15:58:37Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110920\n* #110891\n* #110652\n\n",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 23,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2023-10-10T03:13:20Z",
        "closed_at": "2023-10-11T00:16:46Z",
        "merged_at": null,
        "body": "Follow up: #110817 \r\n\r\nMinor improvements as discussed in prev PR\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 135,
        "deletions": 8,
        "changed_files": 5,
        "created_at": "2023-10-10T02:59:19Z",
        "closed_at": "2023-10-14T15:54:16Z",
        "merged_at": null,
        "body": "The scalar overloads of some ops like `bitwise_xor.Scalar` were dispatched to `CompositeImplicitAutograd` by default. It is against the rule for `CompositeImplicitAutograd` that all tensor operations (except reading metadata) must be done through calls to the ATen dispatcher rather than interacting with the Tensor directly.\r\nSo, here update the dispatch of these overloads to `CompositeExplicitAutograd`.\r\nFixes #93224\r\n",
        "comments": 6
    },
    {
        "merged": false,
        "additions": 26,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2023-10-10T02:00:53Z",
        "closed_at": null,
        "merged_at": null,
        "body": "fixes spda autocast fused attention codegen on my local (i.e. `test_autocast_sdpa_dynamic_shapes`)\r\n\r\nLocal compiled without flash attention nor mem-efficient attention\r\n\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @yanboliang ",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-10T00:18:38Z",
        "closed_at": "2023-10-10T04:31:16Z",
        "merged_at": null,
        "body": "This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/_update-commit-hash.yml).\nUpdate the pinned vision hash.",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 1183,
        "deletions": 1,
        "changed_files": 5,
        "created_at": "2023-10-10T00:05:59Z",
        "closed_at": "2023-10-13T01:21:25Z",
        "merged_at": null,
        "body": "Adding the weight int4pack mm CUDA kernel. The kernel comes from the tinnygemm project which developed by Jeff Johnson.\r\n",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 91,
        "deletions": 46,
        "changed_files": 2,
        "created_at": "2023-10-09T23:48:45Z",
        "closed_at": "2023-10-09T23:52:09Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110913\n* #110912\n* #110911\n* #109132\n* #106581\n* #109601\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 157,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-09T23:48:41Z",
        "closed_at": "2023-10-09T23:52:21Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #110913\n* __->__ #110912\n* #110911\n* #109132\n* #106581\n* #109601\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 255,
        "deletions": 21,
        "changed_files": 5,
        "created_at": "2023-10-09T23:48:36Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110911\n* #109132\n* #106581\n* #109601\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 10,
        "changed_files": 4,
        "created_at": "2023-10-09T23:48:34Z",
        "closed_at": "2023-10-10T18:13:23Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110910\n* #110901\n\nThese two timeout constants were set to the same value, but in two\ndifferent files.  There is no indication that they were kept separate\nfor intentional reasons, as backends are actually processgroups.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 16,
        "deletions": 0,
        "changed_files": 2,
        "created_at": "2023-10-09T23:40:49Z",
        "closed_at": "2023-10-13T16:07:12Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #110892\n* __->__ #110909\n* #110877\n* #110876\n\nI need this to do a cheap and easy output copy in D50023678.\n\nDifferential Revision: [D50105080](https://our.internmc.facebook.com/intern/diff/D50105080/)",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 12,
        "deletions": 5,
        "changed_files": 2,
        "created_at": "2023-10-09T23:32:36Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Summary:\r\nProcess Groups Mapping info collection was introduced in D46321690.\r\n\r\nImprove the mapping info collected there:\r\n- replace pg_id (a unique ID for the PG object) with pg_names (a unique name for each pg and shared by all ranks)\r\n- add number of pg info with group_count\r\n- reduce the length of pg_config_info to avoid being truncated(max length of 4096, now doubled ) by\r\n  - migrating ranks(a map from global ranks to group ranks) with the list of global ranks of a pg, since we currently don't use group rank id\r\n  - using an empty rank list to indicate that all ranks are involved in a pg and adding a field of group_size to show how many ranks are involved\r\n\r\nTest Plan:\r\nTested in HPC\r\n```\r\nbuck2 run mode/opt //hpc/torchrec/models/ads:cmf_10x_launcher -- launcher=local data_loader=random data_loader.num_batches=100 checkpoint=model_store max_ind_range=10 launcher.num_trainers=8\r\n```\r\nExample output in ET\r\n```\r\n{\r\n\"name\": \"## process_group:init ##\", \"id\": 3, \"rf_id\": 1, \"parent\": 2, \"fw_parent\": 0, \"seq_id\": -1, \"scope\": 7, \"tid\": 1, \"fw_tid\": 0, \"op_schema\": \"\",\r\n      \"inputs\": [\"[{\\\"pg_name\\\": \\\"0\\\", \\\"backend_id\\\": 140688385794048, \\\"backend_config\\\": \\\"cuda:nccl\\\", \\\"ranks\\\": {\\\"0\\\": 0, \\\"1\\\": 1, \\\"2\\\": 2, \\\"3\\\": 3, \\\"4\\\": 4, \\\"5\\\": 5, \\\"6\\\": 6, \\\"7\\\": 7}, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"1\\\", \\\"backend_id\\\": 140688386762752, \\\"backend_config\\\": \\\"cuda:nccl\\\", \\\"ranks\\\": {\\\"0\\\": 0, \\\"1\\\": 1, \\\"2\\\": 2, \\\"3\\\": 3, \\\"4\\\": 4, \\\"5\\\": 5, \\\"6\\\": 6, \\\"7\\\": 7}, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"2\\\", \\\"backend_id\\\": 140682531798720, \\\"backend_config\\\": \\\"cpu:gloo,cuda:gloo\\\", \\\"ranks\\\": {\\\"0\\\": 0, \\\"1\\\": 1, \\\"2\\\": 2, \\\"3\\\": 3, \\\"4\\\": 4, \\\"5\\\": 5, \\\"6\\\": 6, \\\"7\\\": 7}, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"faa29c0b1e06cd7abc873bd561414911_0\\\", \\\"backend_id\\\": 140672678002688, \\\"backend_config\\\": \\\"cpu:gloo,cuda:gloo\\\", \\\"ranks\\\": {\\\"0\\\": 0, \\\"1\\\": 1, \\\"2\\\": 2, \\\"3\\\": 3, \\\"4\\\": 4, \\\"5\\\": 5, \\\"6\\\": 6, \\\"7\\\": 7}, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"3\\\", \\\"backend_id\\\": 140672678007616, \\\"backend_config\\\": \\\"cpu:gloo,cuda:gloo\\\", \\\"ranks\\\": {\\\"0\\\": 0, \\\"1\\\": 1, \\\"2\\\": 2, \\\"3\\\": 3, \\\"4\\\": 4, \\\"5\\\": 5, \\\"6\\\": 6, \\\"7\\\": 7}, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"faa29c0b1e06cd7abc873bd561414911_1\\\", \\\"backend_id\\\": 140672678012544, \\\"backend_config\\\": \\\"cpu:gloo,cuda:gloo\\\", \\\"ranks\\\": {\\\"0\\\": 0, \\\"1\\\": 1, \\\"2\\\": 2, \\\"3\\\": 3, \\\"4\\\": 4, \\\"5\\\": 5, \\\"6\\\": 6, \\\"7\\\": 7}, \\\"group_count\\\": 4}]\"], \"input_shapes\": [[]], \"input_types\": [\"String\"],\r\n      \"outputs\": [], \"output_shapes\": [], \"output_types\": []\r\n    },\r\n```\r\n\r\n\r\nBefore the change, pg_config_info of >128 rank will be truncated, e.g.\r\n```\r\n\"inputs\": [\"[{\\\"pg_id\\\": 140321146893696, \\\"backend_id\\\": 140321113854976, \\\"backend_config\\\": \\\"cuda:nccl\\\", \\\"ranks\\\": {\\\"0\\\": 0, \\\"1\\\": 1, \\\"2\\\": 2, \\\"3\\\": 3, \\\"4\\\": 4, \\\"5\\\": 5, \\\"6\\\": 6, \\\"7\\\": 7, \\\"8\\\": 8, \\\"9\\\": 9, \\\"10\\\": 10, \\\"11\\\": 11, \\\"12\\\": 12, \\\"13\\\": 13, \\\"14\\\": 14, \\\"15\\\": 15, \\\"16\\\": 16, \\\"17\\\": 17, \\\"18\\\": 18, \\\"19\\\": 19, \\\"20\\\": 20, \\\"21\\\": 21, \\\"22\\\": 22, \\\"23\\\": 23, \\\"24\\\": 24, \\\"25\\\": 25, \\\"26\\\": 26, \\\"27\\\": 27, \\\"28\\\": 28, \\\"29\\\": 29, \\\"30\\\": 30, \\\"31\\\": 31, \\\"32\\\": 32, \\\"33\\\": 33, \\\"34\\\": 34, \\\"35\\\": 35, \\\"36\\\": 36, \\\"37\\\": 37, \\\"38\\\": 38, \\\"39\\\": 39, \\\"40\\\": 40, \\\"41\\\": 41, \\\"42\\\": 42, \\\"43\\\": 43, \\\"44\\\": 44, \\\"45\\\": 45, \\\"46\\\": 46, \\\"47\\\": 47, \\\"48\\\": 48, \\\"49\\\": 49, \\\"50\\\": 50, \\\"51\\\": 51, \\\"52\\\": 52, \\\"53\\\": 53, \\\"54\\\": 54, \\\"55\\\": 55, \\\"56\\\": 56, \\\"57\\\": 57, \\\"58\\\": 58, \\\"59\\\": 59, \\\"60\\\": 60, \\\"61\\\": 61, \\\"62\\\": 62, \\\"63\\\": 63, \\\"64\\\": 64, \\\"65\\\": 65, \\\"66\\\": 66, \\\"67\\\": 67, \\\"68\\\": 68, \\\"69\\\": 69, \\\"70\\\": 70, \\\"71\\\": 71, \\\"72\\\": 72, \\\"73\\\": 73, \\\"74\\\": 74, \\\"75\\\": 75, \\\"76\\\": 76, \\\"77\\\": 77, \\\"78\\\": 78, \\\"79\\\": 79, \\\"80\\\": 80, \\\"81\\\": 81, \\\"82\\\": 82, \\\"83\\\": 83, \\\"84\\\": 84, \\\"85\\\": 85, \\\"86\\\": 86, \\\"87\\\": 87, \\\"88\\\": 88, \\\"89\\\": 89, \\\"90\\\": 90, \\\"91\\\": 91, \\\"92\\\": 92, \\\"93\\\": 93, \\\"94\\\": 94, \\\"95\\\": 95, \\\"96\\\": 96, \\\"97\\\": 97, \\\"98\\\": 98, \\\"99\\\": 99, \\\"100\\\": 100, \\\"101\\\": 101, \\\"102\\\": 102, \\\"103\\\": 103, \\\"104\\\": 104, \\\"105\\\": 105, \\\"106\\\": 106, \\\"107\\\": 107, \\\"108\\\": 108, \\\"109\\\": 109, \\\"110\\\": 110, \\\"111\\\": 111, \\\"112\\\": 112, \\\"113\\\": 113, \\\"114\\\": 114, \\\"115\\\": 115, \\\"116\\\": 116, \\\"117\\\": 117, \\\"118\\\": 118, \\\"119\\\": 119, \\\"120\\\": 120, \\\"121\\\": 121, \\\"122\\\": 122, \\\"123\\\": 123, \\\"124\\\": 124, \\\"125\\\": 125, \\\"126\\\": 126, \\\"127\\\": 127}}, {\\\"pg_id\\\": 140321074662400, \\\"backend_id\\\": 140321100033024, \\\"backend_config\\\": \\\"cuda:nccl\\\", \\\"ranks\\\": {\\\"0\\\": 0, \\\"1\\\": 1, \\\"2\\\": 2, \\\"3\\\": 3, \\\"4\\\": 4, \\\"5\\\": 5, \\\"6\\\": 6, \\\"7\\\": 7, \\\"8\\\": 8, \\\"9\\\": 9, \\\"10\\\": 10, \\\"11\\\": 11, \\\"12\\\": 12, \\\"13\\\": 13, \\\"14\\\": 14, \\\"15\\\": 15, \\\"16\\\": 16, \\\"17\\\": 17, \\\"18\\\": 18, \\\"19\\\": 19, \\\"20\\\": 20, \\\"21\\\": 21, \\\"22\\\": 22, \\\"23\\\": 23, \\\"24\\\": 24, \\\"25\\\": 25, \\\"26\\\": 26, \\\"27\\\": 27, \\\"28\\\": 28, \\\"29\\\": 29, \\\"30\\\": 30, \\\"31\\\": 31, \\\"32\\\": 32, \\\"33\\\": 33, \\\"34\\\": 34, \\\"35\\\": 35, \\\"36\\\": 36, \\\"37\\\": 37, \\\"38\\\": 38, \\\"39\\\": 39, \\\"40\\\": 40, \\\"41\\\": 41, \\\"42\\\": 42, \\\"43\\\": 43, \\\"44\\\": 44, \\\"45\\\": 45, \\\"46\\\": 46, \\\"47\\\": 47, \\\"48\\\": 48, \\\"49\\\": 49, \\\"50\\\": 50, \\\"51\\\": 51, \\\"52\\\": 52, \\\"53\\\": 53, \\\"54\\\": 54, \\\"55\\\": 55, \\\"56\\\": 56, \\\"57\\\": 57, \\\"58\\\": 58, \\\"59\\\": 59, \\\"60\\\": 60, \\\"61\\\": 61, \\\"62\\\": 62, \\\"63\\\": 63, \\\"64\\\": 64, \\\"65\\\": 65, \\\"66\\\": 66, \\\"67\\\": 67, \\\"68\\\": 68, \\\"69\\\": 69, \\\"70\\\": 70, \\\"71\\\": 71, \\\"72\\\": 72, \\\"73\\\": 73, \\\"74\\\": 74, \\\"75\\\": 75, \\\"76\\\": 76, \\\"77\\\": 77, \\\"78\\\": 78, \\\"79\\\": 79, \\\"80\\\": 80, \\\"81\\\": 81, \\\"82\\\": 82, \\\"83\\\": 83, \\\"84\\\": 84, \\\"85\\\": 85, \\\"86\\\": 86, \\\"87\\\": 87, \\\"88\\\": 88, \\\"89\\\": 89, \\\"90\\\": 90, \\\"91\\\": 91, \\\"92\\\": 92, \\\"93\\\": 93, \\\"94\\\": 94, \\\"95\\\": 95, \\\"96\\\": 96, \\\"97\\\": 97, \\\"98\\\": 98, \\\"99\\\": 99, \\\"100\\\": 100, \\\"101\\\": 101, \\\"102\\\": 102, \\\"103\\\": 103, \\\"104\\\": 104, \\\"105\\\": 105, \\\"106\\\": 106, \\\"107\\\": 107, \\\"108\\\": 108, \\\"109\\\": 109, \\\"110\\\": 110, \\\"111\\\": 111, \\\"112\\\": 112, \\\"113\\\": 113, \\\"114\\\": 114, \\\"115\\\": 115, \\\"116\\\": 116, \\\"117\\\": 117, \\\"118\\\": 118, \\\"119\\\": 119, \\\"120\\\": 120, \\\"121\\\": 121, \\\"122\\\": 122, \\\"123\\\": 123, \\\"124\\\": 124, \\\"125\\\": 125, \\\"126\\\": 126, \\\"127\\\": 127}}, {\\\"pg_id\\\": 140321154994304, \\\"backend_id\\\": 140319780290048, \\\"backend_config\\\": \\\"cpu:gloo,cuda:gloo\\\", \\\"ranks\\\": {\\\"0\\\": 0, \\\"1\\\": 1, \\\"2\\\": 2, \\\"3\\\": 3, \\\"4\\\": 4, \\\"5\\\": 5, \\\"6\\\": 6, \\\"7\\\": 7, \\\"8\\\": 8, \\\"9\\\": 9, \\\"10\\\": 10, \\\"11\\\": 11, \\\"12\\\": 12, \\\"13\\\": 13, \\\"14\\\": 14, \\\"15\\\": 15, \\\"16\\\": 16, \\\"17\\\": 17, \\\"18\\\": 18, \\\"19\\\": 19, \\\"20\\\": 20, \\\"21\\\": 21, \\\"22\\\": 22, \\\"23\\\": 23, \\\"24\\\": 24, \\\"25\\\": 25, \\\"26\\\": 26, \\\"27\\\": 27, \\\"28\\\": 28, \\\"29\\\": 29, \\\"30\\\": 30, \\\"31\\\": 31, \\\"32\\\": 32, \\\"33\\\": 33, \\\"34\\\": 34, \\\"35\\\": 35, \\\"36\\\": 36, \\\"37\\\": 37, \\\"38\\\": 38, \\\"39\\\": 39, \\\"40\\\": 40, \\\"41\\\": 41, \\\"42\\\": 42, \\\"43\\\": 43, \\\"44\\\": 44, \\\"45\\\": 45, \\\"46\\\": 46, \\\"47\\\": 47, \\\"48\\\": 48, \\\"49\\\": 49, \\\"50\\\": 50, \\\"51\\\": 51, \\\"52\\\": 52, \\\"53\\\": 53, \\\"54\\\": 54, \\\"55\\\": 55, \\\"56\\\": 56, \\\"57\\\": 57, \\\"58\\\": 58, \\\"59\\\": 59, \\\"60\\\": 60, \\\"61\\\": 61, \\\"62\\\": 62, \\\"63\\\": 63, \\\"64\\\": 64, \\\"65\\\": 65, \\\"66\\\": 66, \\\"67\\\": 67, \\\"68\\\": 68, \\\"69\\\": 69, \\\"70\\\": 70, \\\"71\\\": 71, \\\"72\\\": 72, \\\"73\\\": 73, \\\"74\\\": 74, \\\"75\\\": 75, \\\"76\\\": 76, \\\"77\\\": 77, \\\"78\\\": 78, \\\"79\\\": 79, \\\"80\\\": 80, \\\"81\\\": 81, \\\"82\\\": 82, \\\"83\\\": 83, \\\"84\\\": 84, \\\"85\\\": 85, \\\"86\\\": 86, \\\"87\\\": 87, \\\"88\\\": 88, \\\"89\\\": 89, \\\"90\\\": 90, \\\"91\\\": 91, \\\"92\\\": 92, \\\"93\\\": 93, \\\"94\\\": 94, \\\"95\\\": 95, \\\"96\\\": 96, \\\"97\\\": 97, \\\"98\\\": 98, \\\"99\\\": 99, \\\"100\\\": 100, \\\"101\\\": 101, \\\"102\\\": 102, \\\"103\\\": 103, \\\"104\\\": 104, \\\"105\\\": 105, \\\"106\\\": 106, \\\"107\\\": 107, \\\"108\\\": 108, \\\"109\\\": 109, \\\"110\\\": 110, \\\"111\\\": 111, \\\"112\\\": 112, \\\"113\\\": 113, \\\"114\\\"\"], \"input_shapes\": [[]], \"input_types\": [\"String\"],\r\n\r\n```\r\nAfter the change the length reduced\r\n```\r\n\"inputs\": [\"[{\\\"pg_name\\\": \\\"0\\\", \\\"backend_id\\\": 140551405059072, \\\"backend_config\\\": \\\"cuda:nccl\\\", \\\"ranks\\\": [], \\\"group_size\\\": 128, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"1\\\", \\\"backend_id\\\": 140551399745536, \\\"backend_config\\\": \\\"cuda:nccl\\\", \\\"ranks\\\": [], \\\"group_size\\\": 128, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"2\\\", \\\"backend_id\\\": 140578999821184, \\\"backend_config\\\": \\\"cpu:gloo,cuda:gloo\\\", \\\"ranks\\\": [], \\\"group_size\\\": 128, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"ea2f9024c70c8b9a25bc06a4723e5805_0\\\", \\\"backend_id\\\": 140559197777152, \\\"backend_config\\\": \\\"cpu:gloo,cuda:gloo\\\", \\\"ranks\\\": [], \\\"group_size\\\": 128, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"3\\\", \\\"backend_id\\\": 140549119076736, \\\"backend_config\\\": \\\"cpu:gloo,cuda:gloo\\\", \\\"ranks\\\": [], \\\"group_size\\\": 128, \\\"group_count\\\": 4}, {\\\"pg_name\\\": \\\"ea2f9024c70c8b9a25bc06a4723e5805_1\\\", \\\"backend_id\\\": 140571995143424, \\\"backend_config\\\": \\\"cpu:gloo,cuda:gloo\\\", \\\"ranks\\\": [], \\\"group_size\\\": 128, \\\"group_count\\\": 4}]\"], \"input_shapes\": [[]], \"input_types\": [\"String\"],\r\n```\r\n\r\nReviewed By: louisfeng, fduwjj\r\n\r\nDifferential Revision: D50048147\r\n\r\n\r\n\r\n",
        "comments": 9
    },
    {
        "merged": false,
        "additions": 690,
        "deletions": 29,
        "changed_files": 17,
        "created_at": "2023-10-09T23:23:44Z",
        "closed_at": "2023-10-10T20:09:52Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #110907\r\n\r\nThis reverts commit ff0358b0384d6a3a5b8ceeae625c93221612ba8e.\r\n\r\n(original PR https://github.com/pytorch/pytorch/pull/108815 desc copied below)\r\n\r\nExpose a set of observability hooks into C10D such that our users can\r\ndetect collectives failure both faster and more easily.\r\n\r\nThe design is similar to NCCL desync debug that it minimized the\r\noverhead by doing most of the work out of the main thread.\r\n\r\nThis PR introduces a new module torch.distributed.hooks that exposes the following set of methods:\r\n\r\n    register_collective_start_hook\r\n    register_collective_end_hook\r\n    register_process_group_hook\r\n\r\nThe process group hook exposes PG creation on the member ranks and call them inline from the\r\nthe PG creation code. This is fine since this happens during initialization and a limited number of times.\r\n\r\nThe collective start/end hooks are fired from a single background thread. It reads\r\nevents from a C++ queue and dispatches over.\r\n\r\nQueue notification is oddly done using a pipe, this is needed so python can abort the thread on shutdown\r\nand have it as background thread. This is not possible with more reasonable choices like a condvar.\r\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 114,
        "deletions": 14,
        "changed_files": 8,
        "created_at": "2023-10-09T22:54:33Z",
        "closed_at": "2023-10-11T00:06:30Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110906\n\r\n## Context\r\n\r\nAdd decompositions for `aten.max`, `aten.min`, and `aten.var_mean`. These operators follow a pattern of returning a tuple of outputs from two component operators:\r\n\r\n```\r\naten.max(x) -> return aten.amax(x), aten.argmax(x)\r\naten.min(x) -> return aten.amin(x), aten.argmin(x)\r\naten.var_mean(x) -> return aten.var(x), aten.mean(x)\r\n```\r\n\r\nFor `var_mean`, the `refs` implementation was doing something similar, so I changed it to call `torch.` ops instead like was done for other `refs` implementations previously. cc: @peterbell10 @lezcano \r\n\r\nNote that Inductor lowers all these directly, so they are excluded from the Inductor decomp table.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan",
        "comments": 13
    },
    {
        "merged": false,
        "additions": 9,
        "deletions": 7,
        "changed_files": 1,
        "created_at": "2023-10-09T22:29:37Z",
        "closed_at": "2023-10-11T07:45:52Z",
        "merged_at": null,
        "body": "Clarify a few things around the documentation\r\n\r\nFixes #ISSUE_NUMBER\r\n",
        "comments": 4
    },
    {
        "merged": false,
        "additions": 1,
        "deletions": 1,
        "changed_files": 1,
        "created_at": "2023-10-09T22:13:41Z",
        "closed_at": null,
        "merged_at": null,
        "body": "# Summary\r\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 2,
        "deletions": 2,
        "changed_files": 2,
        "created_at": "2023-10-09T22:11:45Z",
        "closed_at": "2023-10-10T18:13:07Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #110910\n* __->__ #110901\n\nThis timeout affects NCCL/Gloo/UCC backends (and potentially any other\nbackend depending on default options).\n\nThe 30 minute timeout is causing jobs to waste valuable cluster time.\n\nIt's not clear how low we can set the timeout.  In practice, collectives\ngenerally are expected to complete in milliseconds or seconds, but\ncertain exceptional cases may exist.  This change is an attempt\nto make a significant improvement in a low-risk way.\n\nIf you are a fringe use case and know that you expect/need a long\ntimeout, please consider explicitly overriding the default timeout\nto a value appropriate to your application.",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 8,
        "deletions": 10,
        "changed_files": 1,
        "created_at": "2023-10-09T22:08:29Z",
        "closed_at": "2023-10-11T17:03:17Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110900\n* #110898\n\nmake random ops be a set instead of list",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 124,
        "deletions": 10,
        "changed_files": 2,
        "created_at": "2023-10-09T22:03:33Z",
        "closed_at": "2023-10-11T02:55:59Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110899\n\nTest Plan:\npython test/test_quantization.py TestQuantizePT2EQAT.test_qat_preserve_source_fn_stack\n\nReviewers: jerryzh168, kimishpatel\n\nSubscribers: jerryzh168, kimishpatel, supriyar\n\nDifferential Revision: [D50101253](https://our.internmc.facebook.com/intern/diff/D50101253)",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 14,
        "deletions": 19,
        "changed_files": 2,
        "created_at": "2023-10-09T21:56:31Z",
        "closed_at": "2023-10-11T17:03:16Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #110900\n* __->__ #110898\n\nThis PR switches the replicate -> partial to do division instead of\nzeroing out other ranks, it preserve same numerics, but avoid the\nper-rank behavior difference, and friendly to torch compile",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 690,
        "deletions": 29,
        "changed_files": 18,
        "created_at": "2023-10-09T21:53:27Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #109739\n* #110307\n* __->__ #110897\n\nAdd missing docs for new APIs\n\nThis reverts commit ff0358b0384d6a3a5b8ceeae625c93221612ba8e.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 9,
        "changed_files": 3,
        "created_at": "2023-10-09T21:49:29Z",
        "closed_at": "2023-10-11T19:28:17Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #110896\r\n\r\n\r\nFunctions:\r\n* aten::isfinite\r\n* aten::log_sigmoide\r\n* aten::isreal",
        "comments": 3
    },
    {
        "merged": false,
        "additions": 70,
        "deletions": 11,
        "changed_files": 5,
        "created_at": "2023-10-09T21:40:12Z",
        "closed_at": "2023-10-11T21:37:25Z",
        "merged_at": null,
        "body": "\u2026returned as-is\r\n\r\nFixes #ISSUE_NUMBER\r\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 25,
        "deletions": 2,
        "changed_files": 3,
        "created_at": "2023-10-09T21:22:45Z",
        "closed_at": "2023-10-12T09:25:57Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110894\n\r\n`is_allowed` is a tricky bit of functionality - it sits early up in builder and is used to drive the creation of TorchVariable (more notes here, meta only https://fb.workplace.com/groups/pytorch.dev/permalink/1393563781222098/) \r\n\r\nIf we are tracing distributed in full, we want to route certain calls in distributed to NOT PASS is_allowed (this does not, confusingly, mean that they are not allowed, lol, but rather that we dont want them to become TorchVariable), others, we are fine with preserving.\r\n\r\n\r\n\r\ncc @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 15,
        "deletions": 7,
        "changed_files": 3,
        "created_at": "2023-10-09T21:11:59Z",
        "closed_at": "2023-10-11T02:59:02Z",
        "merged_at": null,
        "body": "Fixes #110832\r\n",
        "comments": 5
    },
    {
        "merged": false,
        "additions": 105,
        "deletions": 22,
        "changed_files": 2,
        "created_at": "2023-10-09T20:48:30Z",
        "closed_at": "2023-10-13T16:07:14Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110892\n* #110909\n* #110877\n* #110876\n\r\nIf a Tensor can be reused and has static shape, we can just cache it across iterations.\r\n\r\nThis is meant as a quickly shippable overhead reduction for CPU overhead-bound use cases that we can ship without relying on memory planning.\r\n\r\nDifferential Revision: [D50023678](https://our.internmc.facebook.com/intern/diff/D50023678/)\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 186,
        "deletions": 67,
        "changed_files": 8,
        "created_at": "2023-10-09T20:22:44Z",
        "closed_at": "2023-10-11T15:58:36Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #110920\n* __->__ #110891\n* #110652\n\nSummary: Introduce a utility class AOTIModelRunner to take care of running an AOTInductor compiled model. It does things like dlopen a model, initialize the model container, setup inputs and outputs, and destroy the model container.",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 1493,
        "deletions": 184,
        "changed_files": 17,
        "created_at": "2023-10-09T20:11:13Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110890\n\nSummary:\n\nThis PR adds epilogue fusion code generation support for the new experimental\n[Inductor Cutlass backend]([https://github.com/pytorch/pytorch/pull/108015]).\n\nDetails:\n\nA fusion happens on the GEMM template level by taking a Cutlass 3.x GEMM Universal Matmul Kernel template\nand adding a custom template functor based on Cutlass new \u201cEpilogue Visitor Trees\u201d (EVT) on top, which represents and\nperforms the computation of the fused Pointwise / Elementwise computation nodes.\n\nThis is the approach dictated by [NVIDIA/cutlass example 49](https://github.com/NVIDIA/cutlass/blob/main/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu),\nwhich is currently the only documentation and example of Cutlass Epilogue Visitor Trees.\n\nThis EVT functor in turn is a hierarchical template expression which represents an abstract syntax tree of the fused computation to perform.\nA second codegen task is to create a hierarchical initializer expression, which provides potentially necessary arguments\nto each of the functor subexpressions.\n\nStep 1 functionality:\n\n * End to end code generation is possible using the above approach.\n * Supports simple elementwise expression fusion of chains of elementwise operations (with scalar constants )\n   after a matmul.\n * Elementwise operation support includes addition, subtraction, multiplication, division, minimum, maximum etc.\n * Examples / Unit tests include ReLU and ReLU6 fusion.\n * Support for fp16 and fp16 with fp32 accumulation data types.\n * Generates SM90 ( Hopper ) based CUDA Kernels ( as Cutlass up to 3.2.0 only supported EVT for SM90 )\n\nThe following is not yet supported, and is left for future work:\n\n * Full operation support ( e.g. full set of all ops usually handled via V.ops handlers )\n * Cutlass EVT with SM80 support ( possible in Cutlass 3.2.1 according to release notes, but not yet documented )\n * Add support for additional (auxiliary) inputs, which changes the Template Kernels' call signature\n * Add support for additional (auxiliary) outputs ( requires support for full computation graphs )\n * Add support for reduction operations and operations which use different output layouts than the input\n * Add support for additional dtypes ( as far as Cutlass allows )\n\nThis PR updates third_party/cutlass to v3.2.1, which has some important improvements and features\nfor the inductor backend.\n\nSee also Cutlass release notes: https://github.com/NVIDIA/cutlass/releases/tag/v3.2.1\n\nNotable changes in Cutlass 3.2.1 include:\n * Cutlass codegen python code has moved into a package with the \"cutlass_library\" namespace, which allows to\n   prevent namespace clashes without resolving to monkey-patching ( which was done earlier ).\n * Support for SM80 epilogue visitor trees ( according to the Release Notes, not tried yet )\n * Small API changes to the cutlass_library API ( requires adapting the inductor backend code )\n\n Test Plan:\n  * CI\n  * pytest test/inductor/test_max_autotune.py\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @muchulee8 @aakhundov @ColinPeppler",
        "comments": 7
    },
    {
        "merged": false,
        "additions": 112,
        "deletions": 41,
        "changed_files": 1,
        "created_at": "2023-10-09T20:10:44Z",
        "closed_at": "2023-10-09T20:30:33Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110889\n* #110888\n* #110887\n\n",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 53,
        "deletions": 12,
        "changed_files": 2,
        "created_at": "2023-10-09T20:10:40Z",
        "closed_at": null,
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110888\n\n",
        "comments": 1
    },
    {
        "merged": false,
        "additions": 3,
        "deletions": 4,
        "changed_files": 1,
        "created_at": "2023-10-09T20:10:36Z",
        "closed_at": "2023-10-10T09:02:19Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #110888\n* __->__ #110887\n\n",
        "comments": 8
    },
    {
        "merged": false,
        "additions": 77,
        "deletions": 5,
        "changed_files": 4,
        "created_at": "2023-10-09T19:28:44Z",
        "closed_at": "2023-10-14T04:51:20Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110884\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @chenyang78 @aakhundov @kadeng",
        "comments": 2
    },
    {
        "merged": false,
        "additions": 11,
        "deletions": 8,
        "changed_files": 1,
        "created_at": "2023-10-09T18:49:41Z",
        "closed_at": "2023-10-09T22:27:07Z",
        "merged_at": null,
        "body": "Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #110882\n\r\n\r\n## Context\r\n\r\nFor more context, please refer to [this PyTorch forums post](https://dev-discuss.pytorch.org/t/defining-the-core-aten-opset/1464).\r\n\r\nThis PR registers some additional ATen operators as `core`, based on feedback from the forums post as well as the experiences from adding other core ATen decompositions.\r\n\r\nThe ATen operators registered as core in this diff, with the associated reasoning, are:\r\n\r\nATen op | reasoning\r\n--|--\r\naten::atan2 | This operator often maps to a hardware intrinsic.\r\naten::diagonal | There is no straightforward decomposition for this operator.\r\naten::empty_like | Decomposition for this operator would require `as_strided` to retain the strides of the input tensor, which should be avoided.\r\naten::expm1 | This operator often maps to a hardware intrinsic; Furthermore, decomposing it will negatively impact the numerical precision of the output.\r\naten::full_like | Decomposition for this operator would require `as_strided` to retain the strides of the input tensor, which should be avoided.\r\naten::log10 | This operator often maps to a hardware intrinsic; Furthermore, decomposing it will negatively impact the numerical precision of the output.\r\naten::log1p | This operator often maps to a hardware intrinsic; Furthermore, decomposing it will negatively impact the numerical precision of the output.\r\naten::log2 | This operator often maps to a hardware intrinsic; Furthermore, decomposing it will negatively impact the numerical precision of the output.\r\naten::pow.Scalar_Tensor | This is a Scalar variant of pow.Tensor_Tensor, which is a part of core.\r\naten::resize | There is no valid decomposition for this operator.\r\n",
        "comments": 3
    }
]